{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"General","text":""},{"location":"#coresets-and-coreset-trees","title":"Coresets and Coreset Trees","text":"<p>A Coreset is a weighted subset of samples from a larger dataset, selected in such a way that the selected samples maintain the statistical properties and corner cases of the full dataset such that training an algorithm on the Coreset will yield the same results as training that same algorithm on the full dataset.</p> <p>The DataHeroes library can be used to build a Coreset as well as to build and maintain a more complex Coreset Structure, known as a Coreset Tree. Once the Coreset or Coreset tree is built, various data science operations can be performed on it, such as training a model, updating labels and removing samples.</p> <p>Unlike a Coreset which is built over the entire dataset in one iteration, a Coreset Tree is comprised of multiple Coresets, each built separately for a chunk (batch) of the dataset (<code>chunk_size</code>) and then combined iteratively in a tree-like manner. </p> <p>In the illustration below, the entire dataset was split into 8 chunks, of 10K instances each (X1, \u2026, X8) and a Coreset of up to 1K samples (<code>coreset_size</code>) was built for each chunk separately (C1, C2, C4, \u2026, C12). Every pair of Coresets is then combined into a new Coreset, in a tree-like hierarchy, with the root being the minimal subset, called the root Coreset (C15).  The Coreset Tree data structure has several advantages over a single Coreset computed over the entire dataset:</p> <ul> <li>A single Coreset requires the entire dataset to fit into the device\u2019s memory and is therefore limited in the size of datasets it can handle. A Coreset Tree, does not have this limitation as it processes the dataset in chunks and can therefore handle any dataset size by splitting the data into the appropriate number of chunks.</li> <li>A Coreset Tree can be computed much faster since the Coresets\u2019 computation can be distributed across a cluster of devices or processors.</li> <li>In a Coreset Tree, additional data can be added to the original dataset without requiring re-computation of the entire Coreset Tree. A Coreset just needs to be built for the additional data and then that Coreset is added to the Coreset Tree, while updating only the necessary Coreset nodes on the path to the root Coreset. This makes it a great structure to use for model maintenance in production.</li> <li>Similarly, updating the target or features of existing instances or removing instances, does not require re\u2011computation of the entire Coreset Tree, updating the Coreset nodes on the path to the root Coreset will suffice. E.g.: If we updated the target for some instances in X6, all we need to do to keep the Coreset Tree updated, is to update the Coresets C9, C10, C14 and C15, while the rest of the Coresets remain unchanged.</li> </ul> <p>The main disadvantage of the Coreset Tree versus a single Coreset computed over the entire dataset, is that each level of the Coreset tree increases the approximation error, therefore leading to a slightly less accurate root Coreset. However, this error can be controlled and decreased easily, by increasing the <code>coreset_size</code>.</p>"},{"location":"#building-a-coreset-or-coreset-tree","title":"Building a Coreset or Coreset Tree","text":"<p>The class representing the Coreset Tree in the DataHeroes library is named CoresetTreeService. While the <code>chunk_size</code> and <code>coreset_size</code> can be passed as parameters to the class, the default behavior is for them to be automatically calculated by the class when the Coreset is built based on the quadruplet: \u2018number of\u00ad instances\u2019, \u2018number of features\u2019 (deduced from the dataset), \u2018number of classes\u2019 (if passed, otherwise deduced from the dataset) and the \u2018memory\u2019 (if passed, otherwise deduced based on the device\u2019s memory). Based on this quadruplet the class will also decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset. In the case of a single Coreset, every time additional data is added to the class using the partial_build function, the class will assess whether it is worth moving to an actual Coreset Tree, and will automatically convert the structure as necessary.</p> <p>To build the Coreset Tree, use the standard <code>build()</code> function or one of its sibling functions \u2013 <code>build_from_df()</code> or <code>build_from_file()</code>.</p> <p>See the build options tutorial here.</p>"},{"location":"#data-cleaning-use-case","title":"Data Cleaning Use Case","text":"<p>You can use a Coreset property referred to as Importance (or Sensitivity) to systematically identify potential errors and anomalies in your data. When building a Coreset, every instance in the data is assigned an Importance value, which indicates how important it is to the final machine learning model. Instances that receive a high Importance value in the Coreset computation, require attention as they usually indicate a labeling error, anomaly, out-of-distribution problem or other data-related issue. This version allows you to find the important samples in the dataset, regardless of what algorithm you use to train your model.</p> <p>To review data instances based on their importance, first build a Coreset using any <code>build()</code> function, while setting the parameter <code>optimized_for</code> to 'cleaning', then use the <code>get_cleaning_samples()</code> function to get the samples with the highest importance, for the classes of interest. </p> <p>When you find incorrectly labeled samples, use the <code>update_targets()</code> function to update their labels or remove the samples using the <code>remove_samples()</code> function. Any such change to the samples will automatically update the Coreset data structure to reflect the changes. </p> <p>Should you prefer to suppress these updates until you finish all your changes, set the force_do_nothing flag to True when calling the <code>update_targets()</code> or <code>remove_samples()</code> functions and call the <code>update_dirty()</code> function to update the Coreset data structure when you\u2019re ready. More advanced cleaning techniques can be applied by using filters with the <code>filter_out_samples()</code> function.</p> <p>When cleaning, it is important to remember that both the train and test dataset should be cleaned to maintain a high quality, non-biased test.</p> <p>See data cleaning tutorials here.</p>"},{"location":"#training-and-hyperparameter-tuning-use-case","title":"Training and Hyperparameter Tuning Use Case","text":"<p>Using our much smaller Coreset structure, you can train or tune your model orders of magnitude faster and consume significantly less compute resources and energy, compared to using the full dataset. </p> <p>Use the DataHeroes\u2019 library <code>fit()</code> function to fit a model on the Coreset and the <code>predict()</code> function to run predictions on the model. Alternatively, if you prefer to use other libraries for training, use the <code>get_coreset()</code> function to retrieve a numpy array or pandas dataframe version of the Coreset which can be used with other libraries. </p> <p>To check the quality of your Coreset, you can fit a model and compare its predictions to predictions from a model built using your full dataset. To decrease the approximation error in the case of a Coreset Tree, requesting level 1 or 2 instead of the default level 0 when calling <code>get_coreset()</code> will return the requested level from the Coreset Tree.</p> <p>To hyperparameter tune your model, use the library\u2019s <code>grid_search()</code> function, which works in a similar manner to Scikit-learn\u2019s GridSearchCV class, only dramatically faster. Note that calling <code>get_coreset()</code> and passing its output to GridSearchCV would yield sub-optimal results, as GridSearchCV would split the returned Coreset into folds. When a Coreset is split, it is no longer a Coreset and won\u2019t maintain the statistical properties of your dataset. The library\u2019s <code>grid_search()</code> function overcomes this obstacle by obtaining the individual nodes of a certain level of the Coreset tree and using them as folds. By default, <code>grid_search()</code> uses cross validation as it\u2019s validation method, but one can select hold-out as an alternative validation method. It is also possible to use the <code>cross_validate()</code> or <code>holdout_validate()</code> functions directly.</p> <p>The current version provides Coresets optimized for training with the following algorithms: all decision tree classification-based problems (including XGBoost, LightGBM, CatBoost, Scikit-learn and others), logistic regression, linear regression, K-Means, PCA and SVD. To build the Coreset or Coreset Tree use any build() function, while setting the parameter <code>optimized_for</code> to <code>training</code>.   </p> <p>See building and training tutorials here.</p>"},{"location":"#model-maintenance-use-case","title":"Model Maintenance Use Case","text":"<p>You can use the Coreset Tree structure to update models in production when new data comes in, by updating the Coreset Tree with the new data and training the model on the Coreset, without having to re-train the model on the full dataset.</p> <p>After building the Coreset Tree using one of the <code>build()</code> functions, you can add additional data to the Coreset Tree using the <code>partial_build()</code> function or one of its sibling functions <code>partial_build_from_df()</code> and <code>partial_build_from_file()</code>, and retrain the model as explained above.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Create a free account on https://dataheroes.ai/getting-started/.</li> <li>Install the library on your device by running: <code>pip install dataheroes</code>.</li> <li>Activate your account by executing the following code once (from each device you\u2019re using): <pre><code>from dataheroes.utils import activate_account\nactivate_account(\"john.doe@gmail.com\")\n</code></pre></li> <li>Check out our documentation and examples available here.</li> </ol>"},{"location":"#other-library-dependencies","title":"Other library dependencies","text":"<p>The DataHeroes library has dependency on other libraries. Please note that when installing the DataHeroes library, older versions of other libraries you may be using, may get automatically updated. The table below shows the minimum version required from each library the dataheroes library depends on.</p> Library Minimum Version numpy 1.19.0 scipy 1.7.0 scikit-learn 0.24.0 pandas 1.0.0 joblib 0.15.0 threadpoolctl 2.1.0 networkx 2.5 pydot 1.4.1 matplotlib 3.3.0 opentelemetry-sdk 1.14.0 opentelemetry-api 1.14.0 opentelemetry-exporter-otlp 1.14.0 psutil 5.8.0 licensing 0.31 tables 3.6.1 decorator 5.1.1"},{"location":"CODE_BUILDER/","title":"Code Builder","text":"<p> Code Builder Code Builder Dataset type Tabular Computer Vision - Image Classification Computer Vision - Object Detection Computer Vision - Semantic Segmentation NLP Use case ML algorithm Library used to train models In which form is your dataset available? File pandas DataFrame X and y numpy arrays File type CSV TSV NPY Do you have separate file(s) or directory(ies) for the target and features? No Yes How is your data organized? Single File Multiple Files Single Directory Multiple Directories Do you have separate DataFrame for the target and features? No Yes Do you have a Single DataFrame / Multiple DataFrame? Single DataFrame Multiple DataFrames Do you have a Single numpy array? Single numpy array Multiple numpy arrays <p>Warning</p> <pre>\n                    <code>\n                        from sklearn.preprocessing import LabelEncoder\n                        from scipy.sparse import issparse\n                        from scipy import sparse\n                        def sensitivity_logit(X, y, w=None, *, sketch_size=None, use_y=True, class_weight: Dict[Any, float] = None):\n                            \"\"\"\n                            Logit sampling from 2018 Paper On Coresets for Logistic Regression.\n                            \"\"\"\n                            X = np.concatenate([X, np.ones([X.shape[0], 1])], axis=1)\n\n                            if w is not None:\n                                w = _check_sample_weight(w, X, dtype=X.dtype)\n                                X, y = w_dot_X(X, y=y, w=w)\n                            else:\n                                w = _check_sample_weight(w, X, dtype=X.dtype)\n\n                        </code>\n                </pre> <p></p>"},{"location":"README-DEPLOY/","title":"README DEPLOY","text":""},{"location":"README-DEPLOY/#documentation-deployment","title":"Documentation deployment","text":"<p>During execution GitHub credentials will be asked (you need enter token as password). We keep documentation in branch docs-versions  <pre><code>pip install -r requirements_mkdocs.txt\n# from branch with version 0.1.0 (for a moment that is docs_0_1_0_fixed)\nmike deploy -b docs-versions -p 0.1.0\n# from branch with last version 0.4.0 (for a moment that is docs_0_4_0)\n# before deploy merge_ingerited_members.py should be executed (merge inherited members)\nmike deploy -b docs-versions -p -u 0.4.0 latest\nmike set-default -b docs-versions -p 0.4.0\nif we need delete certain version\nmike delete -b docs-versions -p 0.1.0\n</code></pre></p>"},{"location":"RELEASE_NOTES/","title":"Release Notes","text":"<p>Last updated on June 20, 2023</p>"},{"location":"RELEASE_NOTES/#new-features-and-improvements-in-version-050","title":"New Features and Improvements in version 0.5.0","text":"<ul> <li>Coresets can now be built using categorical features. The library will automatically one-hot encode them during the build process (as Coresets can only be built with numeric features).</li> <li><code>get_coreset</code> can now return the data according to three data <code>preprocessing_stage</code>: <code>PreprocessingStage.ORIGINAL</code> \u2013 The dataset as it is handed to the Coreset\u2019s build function. <code>PreprocessingStage.USER</code> \u2013 The dataset after any user defined data preprocessing (default). <code>PreprocessingStage.AUTO</code> \u2013 The dataset after any automatic data preprocessing done by the library, such as one-hot encoding and converting Boolean fields to numeric. The features (X), can also be returned as a sparse matrix or an array, controlled by the <code>sparse_output</code> parameter (applicable only for <code>PreprocessingStage.AUTO</code>).</li> <li><code>fit</code> can now return the data according to two <code>preprocessing_stage</code>. <code>PreprocessingStage.AUTO</code> is the default when Scikit-learn is used to train the model. <code>PreprocessingStage.USER</code> is the default when XGBoost, LightGBM or CatBoost are used to train the model.</li> <li>Adding a new <code>auto_preprocessing</code> function, allowing the user to preprocess the (test) data, as it was automatically done by the library during the Coreset\u2019s build function.</li> <li>Fixing the problem where the library\u2019s Docstrings did not show up in some IDEs.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-and-improvements-in-version-040","title":"New Features and Improvements in version 0.4.0","text":"<ul> <li>Adding support for Python 3.11.</li> <li>Allowing to create a CoresetTreeService, which can be used for both training and cleaning (<code>optimized_for=['cleaning', 'training']</code>).</li> <li>The CoresetTreeService can now handle datasets that do not fit into the device\u2019s memory also for cleaning purposes (for training purposes this was supported from the initial release).</li> <li>The <code>get_important_samples</code> function was renamed to <code>get_cleaning_samples</code> to improve the clarity of its purpose.</li> <li>Adding hyperparameter tuning capabilities to the library with the introduction of the <code>grid_search()</code> function, which works in a similar manner to Scikit-learn\u2019s GridSearchCV class, only dramatically faster, as it utilizes the Coreset tree. Introducing also the <code>cross_validate()</code> and <code>holdout_validate()</code> functions, which can be used directly or as the validation method as part of the <code>grid_search()</code> function.</li> <li>Further improving the error messages for some of the data processing problems users encountered.</li> </ul>"},{"location":"RELEASE_NOTES/#bug-fixes-in-version-031","title":"Bug Fixes in version 0.3.1","text":"<ul> <li>build and build_partial now read the data in chunks in the size of chunk_size when the file format allows it (CSV, TSV), to reduce the memory footprint when building the Coreset tree.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-and-improvements-in-version-030","title":"New Features and Improvements in version 0.3.0","text":"<ul> <li>An additional CoresetTreeService for all decision tree classification-based problems has been added to the library. This service can be used to create classification-based Coresets for all libraries including: XGBoost, LightGBM, CatBoost, Scikit-learn and others.</li> <li>Improving the results get_coreset returns in case the Coreset tree is not perfectly balanced.</li> <li>Improving the data handling capabilities, when processing the input data provided to the different build functions, such as supporting pandas.BooleanDtype and pandas.Series and returning clearer error messages for some of the data processing problems encountered.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-and-improvements-in-version-020","title":"New Features and Improvements in version 0.2.0","text":"<ul> <li>Additional CoresetTreeServices for linear regression, K-Means, PCA and SVD have been added to the library.</li> <li>Significantly reducing the memory footprint by up to 50% especially during the various build and partial_build functions.</li> <li>Data is read in chunks in the size of <code>chunk_size</code> when the file format allows it (CSV, TSV), to reduce the memory footprint when building the Coreset tree.</li> <li>Significantly improving the get_coreset time on large datasets.</li> <li>Significantly improving the save time and changing the default save format to pickle.</li> <li>Significantly improving the importance calculation when the number of data instances per class is lower than the number of features.</li> <li>Allowing to save the entire dataset and not just the selected samples, by setting the <code>save_all</code> parameter during the initialization of the class. When <code>optimized_for</code>=<code>'cleaning'</code> <code>save_all</code> is True by default and when <code>optimized_for</code>=<code>'training'</code> it is False by default.</li> <li>Allowing to define certain columns in the dataset as properties (<code>props</code>). Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the <code>select_from_function</code> of get_important_samples.</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"Example Dataset Type Description Data Cleaning: Coreset vs Random Image Classification Comparing cleaning the CIFAR-10 dataset using the dataheroes library vs. cleaning it randomly. Data Cleaning + Labeling Utility Image Classification Providing a simple utility, allowing to display the pictures and correct their labels using the dataheroes library, demonstrated on CIFAR-10. Data Cleaning Object Detection Demonstrating cleaning the COCO object detection dataset using the dataheroes library. Data Cleaning Image Classification Demonstrating cleaning the ImageNet dataset using the dataheroes library. Build Options Tabular Demonstrating different possibilities to construct the Coreset tree data structure. All Library Functions Tabular Demonstrating the usage of all library functions available for the Coreset tree data structure. Configuration File - An example configuration file."},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>services<ul> <li>coreset_tree<ul> <li>CoresetTreeServiceDTC</li> <li>CoresetTreeServiceKMeans</li> <li>CoresetTreeServiceLG</li> <li>CoresetTreeServiceLR</li> <li>CoresetTreeServicePCA</li> <li>CoresetTreeServiceSVD</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/services/coreset_tree/dtc/","title":"CoresetTreeServiceDTC","text":""},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC","title":"CoresetTreeServiceDTC","text":"<pre><code>CoresetTreeServiceDTC(*, data_manager=None, data_params=None, n_instances=None, max_memory_gb=None, n_classes=None, optimized_for, chunk_size=None, coreset_size=None, coreset_params=None, sample_all=None, working_directory=None, cache_dir=None, node_train_function=None, node_train_function_params=None, node_metadata_func=None, save_all=None)\n</code></pre> <p>         Bases: <code>CoresetTreeServiceClassifierMixin</code>, <code>CoresetTreeServiceSupervisedMixin</code>, <code>CoresetTreeService</code></p> <p>Subclass of CoresetTreeService for Decision Tree Classification-based problems. A service class for creating a coreset tree and working with it. optimized_for is a required parameter defining the main usage of the service: 'training', 'cleaning' or both, optimized_for=['training', 'cleaning']. The service will decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset, based on the quadruplet: n_instances, n_classes, max_memory_gb and the 'number of features' (deduced from the dataset). The chunk_size and coreset_size will be deduced based on the above quadruplet too. In case chunk_size and coreset_size are provided, they will override all above mentioned parameters (less recommended).</p> <p>If you intend passing class_weight to your classifier, it is recommended to pass it as a parameter to the class in coreset_params (see example below), so the coreset can be built while taking into account the class_weight. You can continue passing class_weight to your classifier, while retrieving the coreset using the get_coreset method with the parameter inverse_class_weight set to True (default). If you wish to stop passing class_weight to the classifier, retrieve the coreset using the get_coreset method with the parameter inverse_class_weight set to False.</p> <p>The default model class which will be selected for this class instance will be XGBClassifier, on condition the xgboost library is installed. Otherwise, LGBMClassifier will be chosen if the lightgbm library is installed. Else, in the presence of the Catboost library, the selected class will be the CatBoostClassifier. Lastly, if none of the mentioned three libraries are installed, sklearn's GradientBoostingClassifier will be chosen as the final fallback.</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. The class used to interact with the provided data and store it locally. By default, only the sampled data is stored in HDF5 files format.</p> <code>None</code> <code>data_params</code> <code>Union[DataParams, dict]</code> <p>DataParams, optional. Preprocessing information. For Example: data_params = { 'target': {'name': 'Cover_Type'}, 'index': {'name': 'index_column'} }</p> <p>By default, all categorical features are automatically detected and one-hot encoded by the library. To disable this functionality set <code>detect_categorical=False</code>. Note, coresets can only be built with numeric features. Forcing specific features, which include only numeric values, to be categorical, can be done in two possible ways. On a feature-by-feature base or using the categorical_features list, (passing the feature names or the feature index in the dataset starting from 0), as shown in the following example:</p> <p>data_params = { features: [ {name: occupation, categorical: True}, {name: company_name, categorical: True}, {name: gender}, {name: age}, {name: education}, categorical_features: ['gender', 'education'] }</p> <p><code>ohe_min_frequency</code> - Similarly to Skicit-learn's OneHotEncoder min_frequency parameter, specifies the minimum frequency below which a category will be considered infrequent.</p> <p><code>ohe_max_categories</code> - Similarly to Skicit-learn's OneHotEncoder max_categories parameter, specifies an upper limit to the number of output features for each input feature when considering infrequent categories.</p> <p>By default, all datetime features are automatically detected and turned into properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples. To disable this functionality set datetime_to_properties=False.</p> <code>None</code> <code>n_instances</code> <code>int</code> <p>int. The total number of instances that are going to be processed (can be an estimation). This parameter is required and the only one from the above mentioned quadruplet, which isn't deduced from the data.</p> <code>None</code> <code>n_classes</code> <code>int</code> <p>int. The total number of classes (labels). When not provided, will be deduced from the provided data. When multiple files are provided n_classes will be deduced based on the first file only.</p> <code>None</code> <code>max_memory_gb</code> <code>int</code> <p>int, optional. The maximum memory in GB that should be used. When not provided, the server's total memory is used. In any case only 80% of the provided memory or the server's total memory is considered.</p> <code>None</code> <code>optimized_for</code> <code>Union[list, str]</code> <p>str or list Either 'training', 'cleaning' or or both ['training', 'cleaning']. The main usage of the service.</p> required <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances to be used when creating a coreset node in the tree. When defined, it will override the parameters of optimized_for, n_instances, n_classes and max_memory_gb. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>coreset_size</code> <code>Union[int, dict]</code> <p>int, optional. Represents the coreset size of each node in the coreset tree. The coreset is constructed by sampling data instances from the dataset based on their calculated importance. Since each instance may be sampled more than once, in practice, the actual size of the coreset is mostly smaller than coreset_size.</p> <code>None</code> <code>sample_all</code> <code>Iterable</code> <p>iterable, optional. Relevant for classification tasks only. A list of classes for which all instances should be taken, instead of applying sampling.</p> <code>None</code> <code>coreset_params</code> <code>Union[CoresetParams, dict]</code> <p>CoresetParams or dict, optional. Coreset algorithm specific parameters.</p> <p>For example: coreset_params = {     \"class_weight\": {\"a\": 0.5, \"b\": 0.5} }</p> <code>None</code> <code>node_train_function</code> <code>Callable[[np.ndarray, np.ndarray, np.ndarray], Any]</code> <p>Callable, optional. method for training model at tree node level.</p> <code>None</code> <code>node_train_function_params</code> <code>dict</code> <p>dict, optional. kwargs to be used when calling node_train_function.</p> <code>None</code> <code>node_metadata_func</code> <code>Callable[[Tuple[np.ndarray], np.ndarray, Union[list, None]], Union[list, dict, None]]</code> <p>callable, optional. A method for storing user meta data on each node.</p> <code>None</code> <code>working_directory</code> <code>Union[str, os.PathLike]</code> <p>str, path, optional. Local directory where intermediate data is stored.</p> <code>None</code> <code>cache_dir</code> <code>Union[str, os.PathLike]</code> <p>str, path, optional. For internal use when loading a saved service.</p> <code>None</code> <code>save_all</code> <code>bool</code> <p>bool, optional. When set to True, the entire dataset would be saved and not only selected samples. When optimized_for='cleaning' the default is False. When optimized_for='training' the default is True. When both are set the default is True.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.fit","title":"fit","text":"<pre><code>fit(level=0, model=None, preprocessing_stage=None, **model_params)\n</code></pre> <p>Fit a model on the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>preprocessing_stage</code> <code>str</code> <p>string, optional, default <code>PreprocessingStage.USER</code> when XGBoost, LightGBM or CatBoost are used, <code>PreprocessingStage.AUTO</code> when Scikit-learn is used The different stages reflect the data preprocessing workflow. - PreprocessingStage.USER (default) - Return the data after any user defined data preprocessing (if defined). - PreprocessingStage.AUTO - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>None</code> <code>model_params</code> <p>Model hyperparameters kwargs. Input when instantiating default model class.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Fitted estimator.</p>"},{"location":"reference/services/coreset_tree/kmeans/","title":"CoresetTreeServiceKMeans","text":""},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans","title":"CoresetTreeServiceKMeans","text":"<pre><code>CoresetTreeServiceKMeans(*, data_manager=None, data_params=None, n_instances=None, max_memory_gb=None, optimized_for, chunk_size=None, k=8, coreset_size=None, coreset_params=None, working_directory=None, cache_dir=None, node_train_function=None, node_train_function_params=None, node_metadata_func=None, save_all=None)\n</code></pre> <p>         Bases: <code>CoresetTreeServiceUnsupervisedMixin</code>, <code>CoresetTreeService</code></p> <p>Subclass of CoresetTreeService for KMeans. A service class for creating a coreset tree and working with it. optimized_for is a required parameter defining the main usage of the service: 'training', 'cleaning' or both, optimized_for=['training', 'cleaning']. The service will decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset, based on the triplet: n_instances, max_memory_gb and the 'number of features' (deduced from the dataset). The chunk_size and coreset_size will be deduced based on the above triplet too. In case chunk_size and coreset_size are provided, they will override all above mentioned parameters (less recommended).</p> <p>When fitting KMeans on the Coreset, it is highly recommended to use the built-in fit function of the CoresetTreeServiceKMeans class. Sklearn uses by default k-means++ as its initialization method. While sklearn's KMeans implementation supports the receipt of sample_weight, the kmeans_plusplus implementation does not. When building the Coreset, samples are selected and weights are assigned to them, therefore, not using these weights will significantly degrade the quality of the results. The fit implementation of the CoresetTreeServiceKMeans solves this problem, by extending kmeans_plusplus to receive sample_weight.</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. The class used to interact with the provided data and store it locally. By default, only the sampled data is stored in HDF5 files format.</p> <code>None</code> <code>data_params</code> <code>Union[DataParams, dict]</code> <p>DataParams, optional. Preprocessing information. For Example: data_params = { 'target': {'name': 'Cover_Type'}, 'index': {'name': 'index_column'} }</p> <p>By default, all categorical features are automatically detected and one-hot encoded by the library. To disable this functionality set <code>detect_categorical=False</code>. Note, coresets can only be built with numeric features. Forcing specific features, which include only numeric values, to be categorical, can be done in two possible ways. On a feature-by-feature base or using the categorical_features list, (passing the feature names or the feature index in the dataset starting from 0), as shown in the following example:</p> <p>data_params = { features: [ {name: occupation, categorical: True}, {name: company_name, categorical: True}, {name: gender}, {name: age}, {name: education}, categorical_features: ['gender', 'education'] }</p> <p><code>ohe_min_frequency</code> - Similarly to Skicit-learn's OneHotEncoder min_frequency parameter, specifies the minimum frequency below which a category will be considered infrequent.</p> <p><code>ohe_max_categories</code> - Similarly to Skicit-learn's OneHotEncoder max_categories parameter, specifies an upper limit to the number of output features for each input feature when considering infrequent categories.</p> <p>By default, all datetime features are automatically detected and turned into properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples. To disable this functionality set datetime_to_properties=False.</p> <code>None</code> <code>n_instances</code> <code>int</code> <p>int. The total number of instances that are going to be processed (can be an estimation). This parameter is required and the only one from the above mentioned quadruplet, which isn't deduced from the data.</p> <code>None</code> <code>max_memory_gb</code> <code>int</code> <p>int, optional. The maximum memory in GB that should be used. When not provided, the server's total memory is used. In any case only 80% of the provided memory or the server's total memory is considered.</p> <code>None</code> <code>optimized_for</code> <code>Union[list, str]</code> <p>str or list Either 'training', 'cleaning' or or both ['training', 'cleaning']. The main usage of the service.</p> required <code>k</code> <p>int, default=8. Only relevant when tree is optimized_for cleaning. The number of clusters to form as well as the number of centroids to generate.</p> <code>8</code> <code>chunk_size</code> <code>Union[dict, int]</code> <p>int, optional. The number of instances to be used when creating a coreset node in the tree. When defined, it will override the parameters of optimized_for, n_instances, n_classes and max_memory_gb. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>coreset_size</code> <code>Union[int, dict]</code> <p>int, optional. Represents the coreset size of each node in the coreset tree. The coreset is constructed by sampling data instances from the dataset based on their calculated importance. Since each instance may be sampled more than once, in practice, the actual size of the coreset is mostly smaller than coreset_size.</p> <code>None</code> <code>coreset_params</code> <code>Union[CoresetParams, dict]</code> <p>CoresetParams or dict, optional. Coreset algorithm specific parameters.</p> <code>None</code> <code>node_train_function</code> <code>Callable[[np.ndarray, np.ndarray, np.ndarray], Any]</code> <p>Callable, optional. method for training model at tree node level.</p> <code>None</code> <code>node_train_function_params</code> <code>dict</code> <p>dict, optional. kwargs to be used when calling node_train_function.</p> <code>None</code> <code>node_metadata_func</code> <code>Callable[[Tuple[np.ndarray], np.ndarray, Union[list, None]], Union[list, dict, None]]</code> <p>callable, optional. A method for storing user meta data on each node.</p> <code>None</code> <code>working_directory</code> <code>Union[str, os.PathLike]</code> <p>str, path, optional. Local directory where intermediate data is stored.</p> <code>None</code> <code>cache_dir</code> <code>Union[str, os.PathLike]</code> <p>str, path, optional. For internal use when loading a saved service.</p> <code>None</code> <code>save_all</code> <code>bool</code> <p>bool, optional. When set to True, the entire dataset would be saved and not only selected samples. When optimized_for='cleaning' the default is False. When optimized_for='training' the default is True. When both are set the default is True.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/lg/","title":"CoresetTreeServiceLG","text":""},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG","title":"CoresetTreeServiceLG","text":"<pre><code>CoresetTreeServiceLG(*, data_manager=None, data_params=None, n_instances=None, max_memory_gb=None, n_classes=None, optimized_for, chunk_size=None, coreset_size=None, coreset_params=None, sample_all=None, working_directory=None, cache_dir=None, node_train_function=None, node_train_function_params=None, node_metadata_func=None, save_all=None)\n</code></pre> <p>         Bases: <code>CoresetTreeServiceClassifierMixin</code>, <code>CoresetTreeServiceSupervisedMixin</code>, <code>CoresetTreeService</code></p> <p>Subclass of CoresetTreeService for Logistic Regression. A service class for creating a coreset tree and working with it. optimized_for is a required parameter defining the main usage of the service: 'training', 'cleaning' or both, optimized_for=['training', 'cleaning']. The service will decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset, based on the quadruplet: n_instances, n_classes, max_memory_gb and the 'number of features' (deduced from the dataset). The chunk_size and coreset_size will be deduced based on the above quadruplet too. In case chunk_size and coreset_size are provided, they will override all above mentioned parameters (less recommended).</p> <p>If you intend passing class_weight to your classifier, it is recommended to pass it as a parameter to the class in coreset_params (see example below), so the coreset can be built while taking into account the class_weight. You can continue passing class_weight to your classifier, while retrieving the coreset using the get_coreset method with the parameter inverse_class_weight set to True (default). If you wish to stop passing class_weight to the classifier, retrieve the coreset using the get_coreset method with the parameter inverse_class_weight set to False.</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. The class used to interact with the provided data and store it locally. By default, only the sampled data is stored in HDF5 files format.</p> <code>None</code> <code>data_params</code> <code>Union[DataParams, dict]</code> <p>DataParams, optional. Preprocessing information. For Example: data_params = { 'target': {'name': 'Cover_Type'}, 'index': {'name': 'index_column'} }</p> <p>By default, all categorical features are automatically detected and one-hot encoded by the library. To disable this functionality set <code>detect_categorical=False</code>. Note, coresets can only be built with numeric features. Forcing specific features, which include only numeric values, to be categorical, can be done in two possible ways. On a feature-by-feature base or using the categorical_features list, (passing the feature names or the feature index in the dataset starting from 0), as shown in the following example:</p> <p>data_params = { features: [ {name: occupation, categorical: True}, {name: company_name, categorical: True}, {name: gender}, {name: age}, {name: education}, categorical_features: ['gender', 'education'] }</p> <p><code>ohe_min_frequency</code> - Similarly to Skicit-learn's OneHotEncoder min_frequency parameter, specifies the minimum frequency below which a category will be considered infrequent.</p> <p><code>ohe_max_categories</code> - Similarly to Skicit-learn's OneHotEncoder max_categories parameter, specifies an upper limit to the number of output features for each input feature when considering infrequent categories.</p> <p>By default, all datetime features are automatically detected and turned into properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples. To disable this functionality set datetime_to_properties=False.</p> <code>None</code> <code>n_instances</code> <code>int</code> <p>int. The total number of instances that are going to be processed (can be an estimation). This parameter is required and the only one from the above mentioned quadruplet, which isn't deduced from the data.</p> <code>None</code> <code>n_classes</code> <code>int</code> <p>int. The total number of classes (labels). When not provided, will be deduced from the provided data. When multiple files are provided n_classes will be deduced based on the first file only.</p> <code>None</code> <code>max_memory_gb</code> <code>int</code> <p>int, optional. The maximum memory in GB that should be used. When not provided, the server's total memory is used. In any case only 80% of the provided memory or the server's total memory is considered.</p> <code>None</code> <code>optimized_for</code> <code>Union[list, str]</code> <p>str or list Either 'training', 'cleaning' or or both ['training', 'cleaning']. The main usage of the service.</p> required <code>chunk_size</code> <code>Union[dict, int]</code> <p>int, optional. The number of instances to be used when creating a coreset node in the tree. When defined, it will override the parameters of optimized_for, n_instances, n_classes and max_memory_gb. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>coreset_size</code> <code>Union[int, dict]</code> <p>int, optional. Represents the coreset size of each node in the coreset tree. The coreset is constructed by sampling data instances from the dataset based on their calculated importance. Since each instance may be sampled more than once, in practice, the actual size of the coreset is mostly smaller than coreset_size.</p> <code>None</code> <code>sample_all</code> <code>Union[Iterable, dict]</code> <p>iterable, optional. Relevant for classification tasks only. A list of classes for which all instances should be taken, instead of applying sampling.</p> <code>None</code> <code>coreset_params</code> <code>Union[CoresetParams, dict]</code> <p>CoresetParams or dict, optional. Coreset algorithm specific parameters.</p> <p>For example: coreset_params = {     \"class_weight\": {\"a\": 0.5, \"b\": 0.5} }</p> <code>None</code> <code>node_train_function</code> <code>Callable[[np.ndarray, np.ndarray, np.ndarray], Any]</code> <p>Callable, optional. method for training model at tree node level.</p> <code>None</code> <code>node_train_function_params</code> <code>dict</code> <p>dict, optional. kwargs to be used when calling node_train_function.</p> <code>None</code> <code>node_metadata_func</code> <code>Callable[[Tuple[np.ndarray], np.ndarray, Union[list, None]], Union[list, dict, None]]</code> <p>callable, optional. A method for storing user meta data on each node.</p> <code>None</code> <code>working_directory</code> <code>Union[str, os.PathLike]</code> <p>str, path, optional. Local directory where intermediate data is stored.</p> <code>None</code> <code>cache_dir</code> <code>Union[str, os.PathLike]</code> <p>str, path, optional. For internal use when loading a saved service.</p> <code>None</code> <code>save_all</code> <code>bool</code> <p>bool, optional. When set to True, the entire dataset would be saved and not only selected samples. When optimized_for='cleaning' the default is False. When optimized_for='training' the default is True. When both are set the default is True.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/lr/","title":"CoresetTreeServiceLR","text":""},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR","title":"CoresetTreeServiceLR","text":"<pre><code>CoresetTreeServiceLR(*, data_manager=None, data_params=None, n_instances=None, max_memory_gb=None, optimized_for, chunk_size=None, coreset_size=None, coreset_params=None, working_directory=None, cache_dir=None, node_train_function=None, node_train_function_params=None, node_metadata_func=None, save_all=None)\n</code></pre> <p>         Bases: <code>CoresetTreeServiceSupervisedMixin</code>, <code>CoresetTreeService</code></p> <p>Subclass of CoresetTreeService for Linear Regression. A service class for creating a coreset tree and working with it. optimized_for is a required parameter defining the main usage of the service: 'training', 'cleaning' or both, optimized_for=['training', 'cleaning']. The service will decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset, based on the triplet: n_instances, max_memory_gb and the 'number of features' (deduced from the dataset). The chunk_size and coreset_size will be deduced based on the above triplet too. In case chunk_size and coreset_size are provided, they will override all above mentioned parameters (less recommended).</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. The class used to interact with the provided data and store it locally. By default, only the sampled data is stored in HDF5 files format.</p> <code>None</code> <code>data_params</code> <code>Union[DataParams, dict]</code> <p>DataParams, optional. Preprocessing information. For Example: data_params = { 'target': {'name': 'Cover_Type'}, 'index': {'name': 'index_column'} }</p> <p>By default, all categorical features are automatically detected and one-hot encoded by the library. To disable this functionality set <code>detect_categorical=False</code>. Note, coresets can only be built with numeric features. Forcing specific features, which include only numeric values, to be categorical, can be done in two possible ways. On a feature-by-feature base or using the categorical_features list, (passing the feature names or the feature index in the dataset starting from 0), as shown in the following example:</p> <p>data_params = { features: [ {name: occupation, categorical: True}, {name: company_name, categorical: True}, {name: gender}, {name: age}, {name: education}, categorical_features: ['gender', 'education'] }</p> <p><code>ohe_min_frequency</code> - Similarly to Skicit-learn's OneHotEncoder min_frequency parameter, specifies the minimum frequency below which a category will be considered infrequent.</p> <p><code>ohe_max_categories</code> - Similarly to Skicit-learn's OneHotEncoder max_categories parameter, specifies an upper limit to the number of output features for each input feature when considering infrequent categories.</p> <p>By default, all datetime features are automatically detected and turned into properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples. To disable this functionality set datetime_to_properties=False.</p> <code>None</code> <code>n_instances</code> <code>int</code> <p>int. The total number of instances that are going to be processed (can be an estimation). This parameter is required and the only one from the above mentioned quadruplet, which isn't deduced from the data.</p> <code>None</code> <code>max_memory_gb</code> <code>int</code> <p>int, optional. The maximum memory in GB that should be used. When not provided, the server's total memory is used. In any case only 80% of the provided memory or the server's total memory is considered.</p> <code>None</code> <code>optimized_for</code> <code>Union[list, str]</code> <p>str or list Either 'training', 'cleaning' or or both ['training', 'cleaning']. The main usage of the service.</p> required <code>chunk_size</code> <code>Union[dict, int]</code> <p>int, optional. The number of instances to be used when creating a coreset node in the tree. When defined, it will override the parameters of optimized_for, n_instances, n_classes and max_memory_gb. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>coreset_size</code> <code>Union[int, dict]</code> <p>int, optional. Represents the coreset size of each node in the coreset tree. The coreset is constructed by sampling data instances from the dataset based on their calculated importance. Since each instance may be sampled more than once, in practice, the actual size of the coreset is mostly smaller than coreset_size.</p> <code>None</code> <code>coreset_params</code> <code>Union[CoresetParams, dict]</code> <p>CoresetParams or dict, optional. Coreset algorithm specific parameters.</p> <code>None</code> <code>node_train_function</code> <code>Callable[[np.ndarray, np.ndarray, np.ndarray], Any]</code> <p>Callable, optional. method for training model at tree node level.</p> <code>None</code> <code>node_train_function_params</code> <code>dict</code> <p>dict, optional. kwargs to be used when calling node_train_function.</p> <code>None</code> <code>node_metadata_func</code> <code>Callable[[Tuple[np.ndarray], np.ndarray, Union[list, None]], Union[list, dict, None]]</code> <p>callable, optional. A method for storing user meta data on each node.</p> <code>None</code> <code>working_directory</code> <code>Union[str, os.PathLike]</code> <p>str, path, optional. Local directory where intermediate data is stored.</p> <code>None</code> <code>cache_dir</code> <code>Union[str, os.PathLike]</code> <p>str, path, optional. For internal use when loading a saved service.</p> <code>None</code> <code>save_all</code> <code>bool</code> <p>bool, optional. When set to True, the entire dataset would be saved and not only selected samples. When optimized_for='cleaning' the default is False. When optimized_for='training' the default is True. When both are set the default is True.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/pca/","title":"CoresetTreeServicePCA","text":""},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA","title":"CoresetTreeServicePCA","text":"<pre><code>CoresetTreeServicePCA(*, data_manager=None, data_params=None, n_instances=None, max_memory_gb=None, optimized_for, chunk_size=None, coreset_size=None, coreset_params=None, working_directory=None, cache_dir=None, node_train_function=None, node_train_function_params=None, node_metadata_func=None, save_all=None)\n</code></pre> <p>         Bases: <code>CoresetTreeServiceUnsupervisedMixin</code>, <code>CoresetTreeService</code></p> <p>Subclass of CoresetTreeService for PCA. A service class for creating a coreset tree and working with it. optimized_for is a required parameter defining the main usage of the service: 'training', 'cleaning' or both, optimized_for=['training', 'cleaning']. The service will decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset, based on the triplet: n_instances, n_classes, max_memory_gb and the 'number of features' (deduced from the dataset). The chunk_size and coreset_size will be deduced based on the above triplet too. In case chunk_size and coreset_size are provided, they will override all above mentioned parameters (less recommended).</p> <p>When building the Coreset, samples are selected and weights are assigned to them, therefore it is important to use functions that support the receipt of sample_weight. Sklearn's PCA implementation does not support the receipt of sample_weight, therefore, it is highly recommended to use the built-in fit or fit_transform functions of the CoresetTreeServicePCA class as they were extended to receive sample_weight.</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. The class used to interact with the provided data and store it locally. By default, only the sampled data is stored in HDF5 files format.</p> <code>None</code> <code>data_params</code> <code>Union[DataParams, dict]</code> <p>DataParams, optional. Preprocessing information. For Example: data_params = { 'target': {'name': 'Cover_Type'}, 'index': {'name': 'index_column'} }</p> <p>By default, all categorical features are automatically detected and one-hot encoded by the library. To disable this functionality set <code>detect_categorical=False</code>. Note, coresets can only be built with numeric features. Forcing specific features, which include only numeric values, to be categorical, can be done in two possible ways. On a feature-by-feature base or using the categorical_features list, (passing the feature names or the feature index in the dataset starting from 0), as shown in the following example:</p> <p>data_params = { features: [ {name: occupation, categorical: True}, {name: company_name, categorical: True}, {name: gender}, {name: age}, {name: education}, categorical_features: ['gender', 'education'] }</p> <p><code>ohe_min_frequency</code> - Similarly to Skicit-learn's OneHotEncoder min_frequency parameter, specifies the minimum frequency below which a category will be considered infrequent.</p> <p><code>ohe_max_categories</code> - Similarly to Skicit-learn's OneHotEncoder max_categories parameter, specifies an upper limit to the number of output features for each input feature when considering infrequent categories.</p> <p>By default, all datetime features are automatically detected and turned into properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples. To disable this functionality set datetime_to_properties=False.</p> <code>None</code> <code>n_instances</code> <code>int</code> <p>int. The total number of instances that are going to be processed (can be an estimation). This parameter is required and the only one from the above mentioned quadruplet, which isn't deduced from the data.</p> <code>None</code> <code>max_memory_gb</code> <code>int</code> <p>int, optional. The maximum memory in GB that should be used. When not provided, the server's total memory is used. In any case only 80% of the provided memory or the server's total memory is considered.</p> <code>None</code> <code>optimized_for</code> <code>Union[list, str]</code> <p>str or list Either 'training', 'cleaning' or or both ['training', 'cleaning']. The main usage of the service.</p> required <code>chunk_size</code> <code>Union[dict, int]</code> <p>int, optional. The number of instances to be used when creating a coreset node in the tree. When defined, it will override the parameters of optimized_for, n_instances, n_classes and max_memory_gb. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>coreset_size</code> <code>Union[int, dict]</code> <p>int, optional. Represents the coreset size of each node in the coreset tree. The coreset is constructed by sampling data instances from the dataset based on their calculated importance. Since each instance may be sampled more than once, in practice, the actual size of the coreset is mostly smaller than coreset_size.</p> <code>None</code> <code>coreset_params</code> <code>Union[CoresetParams, dict]</code> <p>CoresetParams or dict, optional. Coreset algorithm specific parameters.</p> <code>None</code> <code>node_train_function</code> <code>Callable[[np.ndarray, np.ndarray, np.ndarray], Any]</code> <p>Callable, optional. method for training model at tree node level.</p> <code>None</code> <code>node_train_function_params</code> <code>dict</code> <p>dict, optional. kwargs to be used when calling node_train_function.</p> <code>None</code> <code>node_metadata_func</code> <code>Callable[[Tuple[np.ndarray], np.ndarray, Union[list, None]], Union[list, dict, None]]</code> <p>callable, optional. A method for storing user meta data on each node.</p> <code>None</code> <code>working_directory</code> <code>Union[str, os.PathLike]</code> <p>str, path, optional. Local directory where intermediate data is stored.</p> <code>None</code> <code>cache_dir</code> <code>Union[str, os.PathLike]</code> <p>str, path, optional. For internal use when loading a saved service.</p> <code>None</code> <code>save_all</code> <code>bool</code> <p>bool, optional. When set to True, the entire dataset would be saved and not only selected samples. When optimized_for='cleaning' the default is False. When optimized_for='training' the default is True. When both are set the default is True.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/svd/","title":"CoresetTreeServiceSVD","text":""},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD","title":"CoresetTreeServiceSVD","text":"<p>         Bases: <code>CoresetTreeServiceUnsupervisedMixin</code>, <code>CoresetTreeService</code></p> <p>Subclass of CoresetTreeService for SVD. A service class for creating a coreset tree and working with it. optimized_for is a required parameter defining the main usage of the service: 'training', 'cleaning' or both, optimized_for=['training', 'cleaning']. The service will decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset, based on the triplet: n_instances, n_classes, max_memory_gb and the 'number of features' (deduced from the dataset). The chunk_size and coreset_size will be deduced based on the above triplet too. In case chunk_size and coreset_size are provided, they will override all above mentioned parameters (less recommended).</p> <p>When building the Coreset, samples are selected and weights are assigned to them, therefore it is important to use functions that support the receipt of sample_weight. Sklearn's SVD implementation does not support the receipt of sample_weight, therefore, it is highly recommended to use the built-in fit function of the CoresetTreeServiceSVD class as it was extended to receive sample_weight.</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <p>DataManagerBase subclass, optional. The class used to interact with the provided data and store it locally. By default, only the sampled data is stored in HDF5 files format.</p> required <code>data_params</code> <p>DataParams, optional. Preprocessing information. For Example: data_params = { 'target': {'name': 'Cover_Type'}, 'index': {'name': 'index_column'} }</p> <p>By default, all categorical features are automatically detected and one-hot encoded by the library. To disable this functionality set <code>detect_categorical=False</code>. Note, coresets can only be built with numeric features. Forcing specific features, which include only numeric values, to be categorical, can be done in two possible ways. On a feature-by-feature base or using the categorical_features list, (passing the feature names or the feature index in the dataset starting from 0), as shown in the following example:</p> <p>data_params = { features: [ {name: occupation, categorical: True}, {name: company_name, categorical: True}, {name: gender}, {name: age}, {name: education}, categorical_features: ['gender', 'education'] }</p> <p><code>ohe_min_frequency</code> - Similarly to Skicit-learn's OneHotEncoder min_frequency parameter, specifies the minimum frequency below which a category will be considered infrequent.</p> <p><code>ohe_max_categories</code> - Similarly to Skicit-learn's OneHotEncoder max_categories parameter, specifies an upper limit to the number of output features for each input feature when considering infrequent categories.</p> <p>By default, all datetime features are automatically detected and turned into properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples. To disable this functionality set datetime_to_properties=False.</p> required <code>n_instances</code> <p>int. The total number of instances that are going to be processed (can be an estimation). This parameter is required and the only one from the above mentioned quadruplet, which isn't deduced from the data.</p> required <code>max_memory_gb</code> <p>int, optional. The maximum memory in GB that should be used. When not provided, the server's total memory is used. In any case only 80% of the provided memory or the server's total memory is considered.</p> required <code>optimized_for</code> <p>str or list Either 'training', 'cleaning' or or both ['training', 'cleaning']. The main usage of the service.</p> required <code>chunk_size</code> <p>int, optional. The number of instances to be used when creating a coreset node in the tree. When defined, it will override the parameters of optimized_for, n_instances, n_classes and max_memory_gb. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> required <code>coreset_size</code> <p>int, optional. Represents the coreset size of each node in the coreset tree. The coreset is constructed by sampling data instances from the dataset based on their calculated importance. Since each instance may be sampled more than once, in practice, the actual size of the coreset is mostly smaller than coreset_size.</p> required <code>coreset_params</code> <p>CoresetParams or dict, optional. Coreset algorithm specific parameters.</p> required <code>node_train_function</code> <p>Callable, optional. method for training model at tree node level.</p> required <code>node_train_function_params</code> <p>dict, optional. kwargs to be used when calling node_train_function.</p> required <code>node_metadata_func</code> <p>callable, optional. A method for storing user meta data on each node.</p> required <code>working_directory</code> <p>str, path, optional. Local directory where intermediate data is stored.</p> required <code>cache_dir</code> <p>str, path, optional. For internal use when loading a saved service.</p> required <code>save_all</code> <p>bool, optional. When set to True, the entire dataset would be saved and not only selected samples. When optimized_for='cleaning' the default is False. When optimized_for='training' the default is True. When both are set the default is True.</p> required"},{"location":"coverage/","title":"Coverage","text":""}]}