{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"General","text":""},{"location":"#coresets-and-coreset-trees","title":"Coresets and Coreset Trees","text":"<p>A Coreset is a weighted subset of samples from a larger dataset, selected in such a way that the selected samples maintain the statistical properties and corner cases of the full dataset such that training an algorithm on the Coreset will yield the same results as training that same algorithm on the full dataset.</p> <p>The DataHeroes library can be used to build a Coreset as well as to build and maintain a more complex Coreset Structure, known as a Coreset Tree. Once the Coreset or Coreset tree is built, various data science operations can be performed on it, such as training a model, updating labels and removing samples.</p> <p>Unlike a Coreset which is built over the entire dataset in one iteration, a Coreset Tree is comprised of multiple Coresets, each built separately for a chunk (batch) of the dataset (<code>chunk_size</code>) and then combined iteratively in a tree-like manner. </p> <p>In the illustration below, the entire dataset was split into 8 chunks, of 10K instances each (X1, \u2026, X8) and a Coreset of up to 1K samples (<code>coreset_size</code>) was built for each chunk separately (C1, C2, C4, \u2026, C12). Every pair of Coresets is then combined into a new Coreset, in a tree-like hierarchy, with the root being the minimal subset, called the root Coreset (C15).  The Coreset Tree data structure has several advantages over a single Coreset computed over the entire dataset:</p> <ul> <li>A single Coreset requires the entire dataset to fit into the device\u2019s memory and is therefore limited in the size of datasets it can handle. A Coreset Tree, does not have this limitation as it processes the dataset in chunks and can therefore handle any dataset size by splitting the data into the appropriate number of chunks.</li> <li>A Coreset Tree can be computed much faster since the Coresets\u2019 computation can be distributed across a cluster of devices or processors.</li> <li>In a Coreset Tree, additional data can be added to the original dataset without requiring re-computation of the entire Coreset Tree. A Coreset just needs to be built for the additional data and then that Coreset is added to the Coreset Tree, while updating only the necessary Coreset nodes on the path to the root Coreset. This makes it a great structure to use for model retraining in production.</li> <li>Similarly, updating the target or features of existing instances or removing instances, does not require re\u2011computation of the entire Coreset Tree, updating the Coreset nodes on the path to the root Coreset will suffice. E.g.: If we updated the target for some instances in X6, all we need to do to keep the Coreset Tree updated, is to update the Coresets C9, C10, C14 and C15, while the rest of the Coresets remain unchanged.</li> </ul> <p>The main disadvantage of the Coreset Tree versus a single Coreset computed over the entire dataset, is that each level of the Coreset tree increases the approximation error, therefore leading to a slightly less accurate root Coreset. However, this error can be controlled and decreased easily, by increasing the <code>coreset_size</code>.</p>"},{"location":"#building-a-coreset-or-coreset-tree","title":"Building a Coreset or Coreset Tree","text":"<p>The class representing the Coreset Tree in the DataHeroes library is named CoresetTreeService. While the <code>chunk_size</code> and <code>coreset_size</code> can be passed as parameters to the class, the default behavior is for them to be automatically calculated by the class when the Coreset is built based on the quadruplet: \u2018number of\u00ad instances\u2019, \u2018number of features\u2019 (deduced from the dataset), \u2018number of classes\u2019 (if passed, otherwise deduced from the dataset) and the \u2018memory\u2019 (if passed, otherwise deduced based on the device\u2019s memory). Based on this quadruplet the class will also decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset. In the case of a single Coreset, every time additional data is added to the class using the partial_build function, the class will assess whether it is worth moving to an actual Coreset Tree, and will automatically convert the structure as necessary.</p> <p>To build the Coreset Tree, use the standard <code>build()</code> function or one of its sibling functions \u2013 <code>build_from_df()</code> or <code>build_from_file()</code>.</p> <p>See the build options tutorial here.</p>"},{"location":"#data-cleaning-use-case","title":"Data Cleaning Use Case","text":"<p>You can use a Coreset property referred to as Importance (or Sensitivity) to systematically identify potential errors and anomalies in your data. When building a Coreset, every instance in the data is assigned an Importance value, which indicates how important it is to the final machine learning model. Instances that receive a high Importance value in the Coreset computation, require attention as they usually indicate a labeling error, anomaly, out-of-distribution problem or other data-related issue. This version allows you to find the important samples in the dataset, regardless of what algorithm you use to train your model.</p> <p>To review data instances based on their importance, first build a Coreset using any <code>build()</code> function, while setting the parameter <code>optimized_for</code> to 'cleaning', then use the <code>get_cleaning_samples()</code> function to get the samples with the highest importance, for the classes of interest. </p> <p>When you find incorrectly labeled samples, use the <code>update_targets()</code> function to update their labels or remove the samples using the <code>remove_samples()</code> function. Any such change to the samples will automatically update the Coreset data structure to reflect the changes. </p> <p>Should you prefer to suppress these updates until you finish all your changes, set the force_do_nothing flag to True when calling the <code>update_targets()</code> or <code>remove_samples()</code> functions and call the <code>update_dirty()</code> function to update the Coreset data structure when you\u2019re ready. More advanced cleaning techniques can be applied by using filters with the <code>filter_out_samples()</code> function.</p> <p>When cleaning, it is important to remember that both the train and test dataset should be cleaned to maintain a high quality, non-biased test.</p> <p>See data cleaning tutorials here.</p>"},{"location":"#training-and-hyperparameter-tuning-use-case","title":"Training and Hyperparameter Tuning Use Case","text":"<p>Using our much smaller Coreset structure, you can train or tune your model orders of magnitude faster and consume significantly less compute resources and energy, compared to using the full dataset. </p> <p>Use the DataHeroes\u2019 library <code>fit()</code> function to fit a model on the Coreset and the <code>predict()</code> function to run predictions on the model. Alternatively, if you prefer to use other libraries for training, use the <code>get_coreset()</code> function to retrieve a numpy array or pandas dataframe version of the Coreset which can be used with other libraries. </p> <p>To check the quality of your Coreset, you can fit a model and compare its predictions to predictions from a model built using your full dataset. To decrease the approximation error in the case of a Coreset Tree, requesting level 1 or 2 instead of the default level 0 when calling <code>get_coreset()</code> will return the requested level from the Coreset Tree.</p> <p>To hyperparameter tune your model, use the library\u2019s <code>grid_search()</code> function, which works in a similar manner to Scikit-learn\u2019s GridSearchCV class, only dramatically faster. Note that calling <code>get_coreset()</code> and passing its output to GridSearchCV would yield sub-optimal results, as GridSearchCV would split the returned Coreset into folds. When a Coreset is split, it is no longer a Coreset and won\u2019t maintain the statistical properties of your dataset. The library\u2019s <code>grid_search()</code> function overcomes this obstacle by obtaining the individual nodes of a certain level of the Coreset tree and using them as folds. By default, <code>grid_search()</code> uses cross validation as it\u2019s validation method, but one can select hold-out as an alternative validation method. It is also possible to use the <code>cross_validate()</code> or <code>holdout_validate()</code> functions directly.</p> <p>The current version provides Coresets optimized for training with the following algorithms: all decision tree classification-based problems (including XGBoost, LightGBM, CatBoost, Scikit-learn and others), logistic regression, linear regression, K-Means, PCA and SVD. To build the Coreset or Coreset Tree use any build() function, while setting the parameter <code>optimized_for</code> to <code>training</code>.   </p> <p>See building and training tutorials here.</p>"},{"location":"#model-retraining-use-case","title":"Model retraining Use Case","text":"<p>You can use the Coreset Tree structure to update models in production when new data comes in, by updating the Coreset Tree with the new data and training the model on the Coreset, without having to re-train the model on the full dataset.</p> <p>After building the Coreset Tree using one of the <code>build()</code> functions, you can add additional data to the Coreset Tree using the <code>partial_build()</code> function or one of its sibling functions <code>partial_build_from_df()</code> and <code>partial_build_from_file()</code>, and retrain the model as explained above.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Create a free account on https://dataheroes.ai/getting-started/.</li> <li>Install the library on your device by running: <code>pip install dataheroes</code>.</li> <li>Activate your account by executing the following code once (from each device you\u2019re using): <pre><code>from dataheroes.utils import activate_account\nactivate_account(\"john.doe@gmail.com\")\n</code></pre></li> <li>Check out our documentation and examples available here.</li> </ol>"},{"location":"#other-library-dependencies","title":"Other library dependencies","text":"<p>The DataHeroes library has dependency on other libraries. Please note that when installing the DataHeroes library, older versions of other libraries you may be using, may get automatically updated. The table below shows the minimum version required from each library the dataheroes library depends on.</p> Library Minimum Version numpy 1.19.0 scipy 1.7.0 scikit-learn 0.24.0 pandas 1.0.0 joblib 0.17.0 threadpoolctl 2.1.0 networkx 2.5 pydot 1.4.1 matplotlib 3.3.0 opentelemetry-sdk 1.14.0 opentelemetry-api 1.14.0 opentelemetry-exporter-otlp 1.14.0 psutil 5.8.0 licensing 0.31 tables 3.6.1 decorator 5.1.1 kdepy 1.1.4"},{"location":"CODE_WIZARD/","title":"Code Wizard","text":"Code Wizard Code Wizard Dataset type Tabular Use case Model training and tuning                              Model retraining                              ML algorithm Library used to train models In which form is your dataset available? File pandas DataFrame X and y numpy arrays File type CSV TSV NPY Parquet Do you have separate file(s) or directory(ies) for the target and features? No Yes How is your data organized? Single File Multiple Files Single Directory Multiple Directories Do you have separate DataFrame for the target and features? No Yes Do you have a Single DataFrame / Multiple DataFrame? Single DataFrame Multiple DataFrames Do you have a Single numpy array? Single numpy array Multiple numpy arrays <p>Warning</p> <pre>\n                    <code>\n                        from sklearn.preprocessing import LabelEncoder\n                        from scipy.sparse import issparse\n                        from scipy import sparse\n                        def sensitivity_logit(X, y, w=None, *, sketch_size=None, use_y=True, class_weight: Dict[Any, float] = None):\n                            \"\"\"\n                            Logit sampling from 2018 Paper On Coresets for Logistic Regression.\n                            \"\"\"\n                            X = np.concatenate([X, np.ones([X.shape[0], 1])], axis=1)\n                            if w is not None:\n                                w = _check_sample_weight(w, X, dtype=X.dtype)\n                                X, y = w_dot_X(X, y=y, w=w)\n                            else:\n                                w = _check_sample_weight(w, X, dtype=X.dtype)\n                        </code>\n                </pre>"},{"location":"README-DEPLOY/","title":"README DEPLOY","text":""},{"location":"README-DEPLOY/#documentation-deployment","title":"Documentation Deployment","text":"<p>During the deployment process, you will be prompted to enter GitHub credentials. Use your personal access token as the password. The documentation is maintained in the <code>docs-versions</code> branch.</p>"},{"location":"README-DEPLOY/#steps-for-deployment","title":"Steps for Deployment","text":""},{"location":"README-DEPLOY/#on-master-branch","title":"On master branch","text":"<ol> <li>Install Required Packages <pre><code>pip install -r requirements_mkdocs.txt\n</code></pre></li> <li>Merge Inherited Members <pre><code>docs/merge_inherited_members.py\n</code></pre></li> <li>Deploy the Documentation <pre><code>mike deploy -b docs-versions -p -u 0.11.0 latest\n</code></pre></li> <li>Set Default Version <pre><code>mike set-default -b docs-versions -p 0.11.0\n</code></pre></li> </ol>"},{"location":"README-DEPLOY/#temporary-github-pages-bug-workaround","title":"Temporary GitHub Pages Bug Workaround","text":"<p>GitHub Pages has a known bug that affects deployment. Follow these steps to work around it:</p> <ol> <li>Switch to the <code>docs-versions</code> branch.</li> <li>Delete the symlink <code>latest</code> in the root of the project (it appears as a folder in the working copy).</li> <li>Create a folder named <code>latest</code> in the root of the project.</li> <li>Copy the content from the folder of the latest version (e.g., <code>0.10.0</code>) to the <code>latest</code> folder.</li> <li>Commit and push these changes to the <code>docs-versions</code> branch.</li> <li>Check the deployment of the new documentation version on our production site: https://data-heroes.github.io/dh-library. It might take a few minutes.</li> </ol>"},{"location":"README-DEPLOY/#deleting-a-specific-version","title":"Deleting a Specific Version","text":"<p>To delete a specific version, use the following command: <pre><code>mike delete -b docs-versions -p 0.1.0\n</code></pre></p>"},{"location":"README-DEPLOY/#additional-documentation-comments","title":"Additional Documentation Comments","text":"<ol> <li>To test the current version locally, use:    <pre><code>mkdocs serve\n</code></pre></li> <li>To get the full code reference section (https://data-heroes.github.io/dh-library/0.10.0/reference/services/coreset_tree/dtc/), you must first run <code>merge_inherited_members.py</code>. This script adds signatures to user-level classes because our doc-generator engine cannot show inherited methods by default.</li> <li>If you change the class hierarchy or add new classes, update <code>merge_inherited_members.py</code>. This script contains lists of base and end classes that need to be modified accordingly.</li> <li>The code reference page has two content lists: modules on the right and members of the current module (classes and class methods) on the left. To maintain a clear hierarchy, module names are replaced with class names in <code>gen_ref_nav.py</code>. Follow the existing pattern when adding new modules.</li> <li>When adding new modules, update all relevant files in the <code>docs</code> folder by searching for old module names and following the established patterns.</li> <li>The only file related to documentation outside the <code>docs</code> folder is <code>mkdocs.yml</code>. Ensure it is updated as well.</li> <li>The deployment procedure posts a new version as a folder named after the version (e.g., <code>0.10.0</code>) to the <code>docs-versions</code> branch. Any manual commit to this branch also triggers deployment. This setting can be viewed or changed here.</li> <li>Deployment records can be found here.</li> <li>When creating new signatures, follow the formatting of existing docstrings to ensure correct display on the site. Verify how new documentation is generated and compare it with existing sections.</li> <li>For documentation enhancements, refer to the MkDocs Material reference.</li> <li>Always thoroughly check the documentation after generating a new version and fix any issues that arise.</li> </ol>"},{"location":"RELEASE_NOTES/","title":"Release Notes","text":"<p>Last updated on September 30, 2024</p>"},{"location":"RELEASE_NOTES/#new-features-improvements-and-bug-fixes-in-version-0120","title":"New Features, Improvements and Bug Fixes in version 0.12.0","text":"<ul> <li>Adding the support for Target Encoding, which is the default way to handle categorical features in binary classification problems.</li> <li>Significantly improving the <code>build</code> functions\u2019 runtime and their memory consumption, especially for datasets with categorical features and missing values.</li> <li>Significantly improving the runtime of the <code>auto_preprocessing</code>, <code>predict</code> and <code>predict_proba</code> functions.</li> <li>Fixing various problems concerning the handling of missing values.</li> <li>Fixing the problem where incorrect data was saved for validation purposes when calling the save function.</li> </ul>"},{"location":"RELEASE_NOTES/#improvements-and-bug-fixes-in-version-0112","title":"Improvements and Bug Fixes in version 0.11.2","text":"<ul> <li>Solving a memory leak that happened sometimes during the various <code>build</code> and <code>partial_build</code> functions.</li> <li>Improving the <code>build</code> function's runtime and their memory consumption.</li> </ul>"},{"location":"RELEASE_NOTES/#improvements-and-bug-fixes-in-version-0111","title":"Improvements and Bug Fixes in version 0.11.1","text":"<ul> <li>Improving the <code>grid_search</code> runtime, when XGBoost, LightGBM and CatBoost are used.</li> <li>The <code>get_coreset_size</code> function is no longer limited by license.</li> <li>Fixing the problem preventing the build of the <code>CoresetTreeServiceDTR</code> in some cases.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-improvements-and-bug-fixes-in-version-0110","title":"New Features, Improvements and Bug Fixes in version 0.11.0","text":"<ul> <li>Adding logging capabilities to the library, to improve its debugging capabilities.</li> <li>Adding a verbose parameter to the various build and partial_build functions, to provide a better indication to the length of the operation.</li> <li>Adding the <code>get_hyperparameter_tuning_data</code> function, which allows retrieving the data from the Coreset tree in such a format that allows running <code>GridSearchCV</code>, <code>BayesSearchCV</code> and other hyperparameter tuning methods.</li> <li>Improving the structure returned by <code>get_coreset</code>.</li> <li>Improving the <code>build</code> functions\u2019 runtime and their memory consumption.</li> <li>Fixing various problems when saving the Coreset tree.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-improvements-and-bug-fixes-in-version-0103","title":"New Features, Improvements and Bug Fixes in version 0.10.3","text":"<ul> <li>Improving <code>grid_search</code> runtime, by improving its parallelism. The number of jobs run in parallel during <code>grid_search</code> can be controlled by defining the <code>n_jobs</code> parameter passed to the function.</li> <li>Fixing the problem when a Coreset tree was built with a <code>seq_column</code>, <code>grid_search</code> with <code>refit=True</code> would ignore the sequence-related parameters passed to the function.</li> <li>Fixing the problem when a Coreset tree was built with a <code>seq_column</code>, <code>fit</code> and <code>grid_search</code> would not always select the optimal nodes from the Coreset tree.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-improvements-and-bug-fixes-in-version-0102","title":"New Features, Improvements and Bug Fixes in version 0.10.2","text":"<ul> <li>Allowing to configure the <code>coreset_size</code> also as a float, representing the ratio between each chunk and the resulting coreset.</li> <li>Fixing the problem when the returned column types were incorrect in some cases when calling <code>get_coreset</code>, <code>fit</code> or <code>grid_search</code> with <code>preprocessing_stage=user</code>.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-improvements-and-bug-fixes-in-version-0101","title":"New Features, Improvements and Bug Fixes in version 0.10.1","text":"<ul> <li>Improving the <code>grid_search</code> time for datasets with categorical features and missing values.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-improvements-and-bug-fixes-in-version-0100","title":"New Features, Improvements and Bug Fixes in version 0.10.0","text":"<ul> <li>Adding an <code>enhancement</code> parameter to the <code>fit</code>, <code>grid_search</code> and the validation functions of the <code>CoresetTreeServiceDTC</code> and <code>CoresetTreeServiceDTR</code>. Setting a value of 1 to 3 for this parameter will enhance the default decision tree based training, which can improve the strength of the model, but will increase the training run time.</li> </ul>"},{"location":"RELEASE_NOTES/#bug-fixes-in-version-091","title":"Bug Fixes in version 0.9.1","text":"<ul> <li>Fixing the problem when a Coreset tree was built with a <code>seq_column</code> on a dataset including categorical features, <code>predict</code> and <code>predict_proba</code> would sometimes fail.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-improvements-and-bug-fixes-in-version-090","title":"New Features, Improvements and Bug Fixes in version 0.9.0","text":"<ul> <li>Allowing to execute <code>get_coreset</code>, <code>fit</code>, <code>grid_search</code> and the validation functions on a subset of the data of the Coreset tree (such as certain date ranges), by defining a sequence column (<code>seq_column</code>), in the <code>DataParams</code> structure passed during the initialization of the class, which can then be used to filter the data.</li> <li>Improving the <code>build</code> time, by improving the parallelism of the Coreset Tree construction. The number of jobs run in parallel during <code>build</code> can be controlled by defining the <code>n_jobs</code> parameter passed to the function.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-improvements-and-bug-fixes-in-version-081","title":"New Features, Improvements and Bug Fixes in version 0.8.1","text":"<ul> <li>Fixing a licensing issue.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-improvements-and-bug-fixes-in-version-080","title":"New Features, Improvements and Bug Fixes in version 0.8.0","text":"<ul> <li>An additional CoresetTreeService for all  decision tree regression-based problems has been added to the library. This service can be used to create regression-based Coresets for all libraries including: XGBoost, LightGBM, CatBoost, Scikit-learn and others.</li> <li>Improving the default Coreset tree created when <code>chunk_size</code> and <code>coreset_size</code> are not provided during the initialization of the class.</li> <li> <code>grid_search</code> was extended to return a Pandas DataFrame with the score of each hyperparameter combination and fold.</li> <li>Improving the  <code>grid_search</code> time.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-improvements-and-bug-fixes-in-version-070","title":"New Features, Improvements and Bug Fixes in version 0.7.0","text":"<ul> <li>Coresets can now be built when the dataset has missing values. The library will automatically handle them during the build process (as Coresets can only be built without missing values).</li> <li>Significantly improving the <code>predict</code> and <code>predict_proba</code> time for datasets with categorical features.</li> <li><code>predict</code> and <code>predict_proba</code> will now automatically preprocesses the data according to the <code>preprocessing_stage</code> used to train the model.</li> <li>Improving the automatic detection of categorical features.</li> <li>It is now possible to define the model class, used to train the model on the coreset, when initializing the CoresetTreeService class, using the <code>model_cls</code> parameter.</li> <li>Enhancing the <code>grid_search</code> function to run on unsupervised datasets.</li> <li>Fixing the problem when <code>grid_search</code> would fail after <code>remove_samples</code> or <code>filter_out_samples</code> were called.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-improvements-and-bug-fixes-in-version-060","title":"New Features, Improvements and Bug Fixes in version 0.6.0","text":"<ul> <li>Replacing the <code>save_all</code> parameter passed when initializing all classes, with the <code>chunk_sample_ratio</code> parameter, which indicates the size of the sample that will be taken and saved from each chunk on top of the Coreset for the validation methods.</li> <li>Significantly improving the build time for datasets with categorical features.</li> <li><code>grid_search</code>, <code>cross_validate</code> and <code>holdout_validate</code> all receive now the <code>preprocessing_stage</code> parameter, same as the <code>fit</code> function.</li> <li><code>fit</code> returns now the data in <code>preprocessing_stage=auto</code> by default when Scikit-learn or XGBoost are used to train the model and in <code>preprocessing_stage=user</code> by default when LightGBM or CatBoost are used to train the model.</li> <li>Fixing the problem when both <code>model</code> and <code>model_params</code> were passed to <code>fit</code>, <code>grid_search</code>, <code>cross_validate</code> and <code>holdout_validate</code>, <code>model_params</code> were ignored.</li> <li>Fixing the problem when a single Coreset was created for the entire dataset and <code>get_cleaning_samples</code> was called with <code>class_size={\"class XXX\": \"all\"}</code>, the returned result was faulty.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-and-improvements-in-version-050","title":"New Features and Improvements in version 0.5.0","text":"<ul> <li>Coresets can now be built using categorical features. The library will automatically one-hot encode them during the build process (as Coresets can only be built with numeric features).</li> <li><code>get_coreset</code> can now return the data according to three data <code>preprocessing_stage=original</code> \u2013 The dataset as it is handed to the Coreset\u2019s build function. <code>preprocessing_stage=user</code> \u2013 The dataset after any user defined data preprocessing (default). <code>preprocessing_stage=auto</code> \u2013 The dataset after any automatic data preprocessing done by the library, such as one-hot encoding and converting Boolean fields to numeric. The features (X), can also be returned as a sparse matrix or an array, controlled by the <code>sparse_output</code> parameter (applicable only for <code>preprocessing_stage=auto</code>).</li> <li><code>fit</code> can now return the data according to two <code>preprocessing_stage=auto</code> is the default when Scikit-learn is used to train the model. <code>preprocessing_stage=user</code> is the default when XGBoost, LightGBM or CatBoost are used to train the model.</li> <li>Adding a new <code>auto_preprocessing</code> function, allowing the user to preprocess the (test) data, as it was automatically done by the library during the Coreset\u2019s build function.</li> <li>Fixing the problem where the library\u2019s Docstrings did not show up in some IDEs.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-and-improvements-in-version-040","title":"New Features and Improvements in version 0.4.0","text":"<ul> <li>Adding support for Python 3.11.</li> <li>Allowing to create a CoresetTreeService, which can be used for both training and cleaning (<code>optimized_for=['cleaning', 'training']</code>).</li> <li>The CoresetTreeService can now handle datasets that do not fit into the device\u2019s memory also for cleaning purposes (for training purposes this was supported from the initial release).</li> <li>The <code>get_important_samples</code> function was renamed to <code>get_cleaning_samples</code> to improve the clarity of its purpose.</li> <li>Adding hyperparameter tuning capabilities to the library with the introduction of the <code>grid_search()</code> function, which works in a similar manner to Scikit-learn\u2019s GridSearchCV class, only dramatically faster, as it utilizes the Coreset tree. Introducing also the <code>cross_validate()</code> and <code>holdout_validate()</code> functions, which can be used directly or as the validation method as part of the <code>grid_search()</code> function.</li> <li>Further improving the error messages for some of the data processing problems users encountered.</li> </ul>"},{"location":"RELEASE_NOTES/#bug-fixes-in-version-031","title":"Bug Fixes in version 0.3.1","text":"<ul> <li>build and build_partial now read the data in chunks in the size of chunk_size when the file format allows it (CSV, TSV), to reduce the memory footprint when building the Coreset tree.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-and-improvements-in-version-030","title":"New Features and Improvements in version 0.3.0","text":"<ul> <li>An additional CoresetTreeService for all decision tree classification-based problems has been added to the library. This service can be used to create classification-based Coresets for all libraries including: XGBoost, LightGBM, CatBoost, Scikit-learn and others.</li> <li>Improving the results get_coreset returns in case the Coreset tree is not perfectly balanced.</li> <li>Improving the data handling capabilities, when processing the input data provided to the different build functions, such as supporting pandas.BooleanDtype and pandas.Series and returning clearer error messages for some of the data processing problems encountered.</li> </ul>"},{"location":"RELEASE_NOTES/#new-features-and-improvements-in-version-020","title":"New Features and Improvements in version 0.2.0","text":"<ul> <li>Additional CoresetTreeServices for linear regression, K-Means, PCA and SVD have been added to the library.</li> <li>Significantly reducing the memory footprint by up to 50% especially during the various build and partial_build functions.</li> <li>Data is read in chunks in the size of <code>chunk_size</code> when the file format allows it (CSV, TSV), to reduce the memory footprint when building the Coreset tree.</li> <li>Significantly improving the get_coreset time on large datasets.</li> <li>Significantly improving the save time and changing the default save format to pickle.</li> <li>Significantly improving the importance calculation when the number of data instances per class is lower than the number of features.</li> <li>Allowing to save the entire dataset and not just the selected samples, by setting the <code>save_all</code> parameter during the initialization of the class. When <code>optimized_for</code>=<code>'cleaning'</code> <code>save_all</code> is True by default and when <code>optimized_for</code>=<code>'training'</code> it is False by default.</li> <li>Allowing to define certain columns in the dataset as properties (<code>props</code>). Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the <code>select_from_function</code> of get_important_samples.</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"Example Dataset Type Description Data Cleaning: Coreset vs Random Image Classification Comparing cleaning the CIFAR-10 dataset using the dataheroes library vs. cleaning it randomly. Data Cleaning + Labeling Utility Image Classification Providing a simple utility, allowing to display the pictures and correct their labels using the dataheroes library, demonstrated on CIFAR-10. Data Cleaning Object Detection Demonstrating cleaning the COCO object detection dataset using the dataheroes library. Data Cleaning Image Classification Demonstrating cleaning the ImageNet dataset using the dataheroes library. Build Options Tabular Demonstrating different possibilities to construct the Coreset tree data structure. All Library Functions Tabular Demonstrating the usage of all library functions available for the Coreset tree data structure. Configuration File - An example configuration file."},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>services<ul> <li>coreset_tree<ul> <li>CoresetTreeServiceDTC</li> <li>CoresetTreeServiceDTR</li> <li>CoresetTreeServiceKMeans</li> <li>CoresetTreeServiceLG</li> <li>CoresetTreeServiceLR</li> <li>CoresetTreeServicePCA</li> <li>CoresetTreeServiceSVD</li> </ul> </li> </ul> </li> <li>DataParams</li> </ul>"},{"location":"reference/data/common/","title":"DataParams","text":""},{"location":"reference/data/common/#data.common.DataParams","title":"DataParams  <code>dataclass</code>","text":"<p>             Bases: <code>BaseDC</code></p> <p>A class including all required information to preprocess the data. When not defined all fields/columns in the data are treated as features.</p> <p>The example below shows how the class is used by the CoresetTreeService class. See a more extensive example at the end of the page. <pre><code>data_params = {\n    'target': {'name': 'Cover_Type'},\n    'index': {'name': 'index_column'}\n}\n\nservice_obj = CoresetTreeServiceLG(\n    optimized_for='training',\n    data_params=data_params\n)\n</code></pre></p>  Parameter nameTypeDescription General Parameters features ListThe list of the fields/columns used as features to build the Coreset and train the model. If not defined, the <code>columns_to_features</code> parameter should be defined. Each feature is defined as a dictionary with the following attributes (only name is mandatory): <ul> <li><code>name</code>: The feature name.</li> <li><code>dtype</code>: The feature data type.</li> <li><code>categorical</code>: Set to true if the feature is categorical.     For more information refer to the Categorical Features Parameters section.     </li> <li><code>fill_value</code>: In case the feature has missing values, how should they be filled.     For more information refer to the Missing Values Parameters section.     </li> <li><code>transform</code>: A function defining the required transformation for the feature.     For more information refer to the Data Transformation Parameters section.     </li> <ul> See the example at the end of the page. target dictThe target column. Example: <code>'target': {'name': 'Cover_Type'}</code> index dictThe index column. Example: <code>'index': {'name': 'index_column'} </code> properties ListA list of fields/columns which won\u2019t be used to build the Coreset or train the model, but it is possible to <code>filter_out_samples</code> on them or to pass them in the <code>select_from_function</code> of <code>get_cleaning_samples</code>. Example: <code>'properties': [{'name': 'date'},]</code> columns_to_features Union[bool, dict], default FalseEither <code>bool</code> or <code>dict</code> with two possible fields, <code>include</code> or <code>exclude</code>. When set to true, all fields/columns in the dataset are treated as features. When <code>include</code> is used, only fields/columns defined or those fitting the defined masks are treated as features. When <code>exclude</code> is used, only fields/columns defined or those fitting the defined masks are not treated as features.  Example: <code>{'exclude': ['internal_.*', 'bad']}</code> datetime_to_properties  boolean, default TrueBy default, all datetime fields/columns are turned into properties. Properties, won\u2019t be used to build the Coreset or train the model, but it is possible to <code>filter_out_samples</code> on them or to pass them in the <code>select_from_function</code> of <code>get_cleaning_samples</code>. To disable this functionality set <code>'datetime_to_properties': False</code>. save_origbool, default False When data transformations are defined (such as data_transform_before or feature level transform), the default behavior is to save the data only after the transformations. To save the data also in  it original format, as it was handed to the build function and before any user defined data  transformations, set `'save_orig': True`. To retrieve the Coreset in its original format  user `preprocessing_stage='original'` when calling the `get_coreset` function.  seq_columndict, default None Defining a sequence column (such as a date), allows to specify <code>seq_from</code> and <code>seq_to</code> parameters to <code>get_coreset</code>, <code>fit</code>, <code>grid_search</code> and the validation functions, so these functions would be executed on a subset of the data (such as certain date ranges). The <code>seq_column</code> is a dictionary containing the following parameters: <ul> <li><code>name/id/prop_id</code>: Required. The name, id or prop_id of the sequence column. name:     The name of the column. id: The index of the feature starting from 0. prop_id:     The index of the property starting from 0.</li> <li><code>granularity</code>: Required. The granularity in which the sequence column would be queried.     Can be either a pandas offset     or a callable function.     </li> <li><code>datetime_format</code>: Required in case the sequence column is a datetime formatted as string.     The datetime format of the sequence column.</li> <li><code>chunk_by</code>: Optional. When set, the Coreset tree will be built using the <code>chunk_by</code>     functionality according to the sequence column instead of using a fixed <code>chunk_size</code>.     </li> </ul> Example: <pre><code>    'seq_column':\n        {\n            'name': 'Transaction Date',\n            'granularity': 'D',\n            'datetime_format': '%yyyy-%mm-%dd',\n            'chunk_by': True\n        }\n</code></pre> Categorical Features Parameters detect_categorical  boolean, default True By default, all non-numeric and non-boolean fields/columns are automatically regarded as categorical features and one-hot encoded by the library. To disable this functionality set <code>'detect_categorical': False</code>. Note - coresets can only be built with numeric features. cat_encoding_method str, default None  Use this parameter to override the default categorical encoding strategy (valid non-default values are <code>\u2018OHE\u2019</code>, <code>\u2018TE\u2019</code>). If this parameter is left on default, the strategy for encoding categorical features is determined as follows: Target Encoding will be used in binary classification tasks; One Hot Encoding will be used in all other types of learning tasks (multiclass classification, regression, and unsupervised learning). Valid overriding is effective only for binary classification tasks (e.g., change of default <code>\u2018TE\u2019</code> to <code>\u2018OHE\u2019</code>). categorical_features  List Forcing specific features, which include only numeric values, to be categorical, can be done in two possible ways. On a feature-by-feature base (setting the <code>categorical</code> attribute to True in the <code>features</code> list) or using the <code>categorical_features</code> list, passing the feature names or the feature index in the dataset starting from 0.  See the example at the end of the page. ohe_min_frequency float between 0 and 1, default 0.01     Similarly to Skicit-learn's OneHotEncoder <code>min_frequency</code> parameter, specifies the minimum frequency below which a category will be considered infrequent. Example: <code>'ohe_min_frequency': 0</code> ohe_max_categories int, default 100  Similarly to Skicit-learn's OneHotEncoder <code>max_categories</code> parameter, specifies an upper limit to the number of output features for each input feature when considering infrequent categories. Example: <code>'ohe_max_categories': 500</code> te_cv int, default 5  If Target Encoding is employed, this parameter determines the number of folds in the 'cross fitting' strategy used in TargetEncoder\u2019s <code>fit_transform</code>. In practice, a lower number may be applied, based on the distribution of classes in the data. te_random_state  int, default None  If Target Encoding is employed, this parameter affects the ordering of the indices which controls the randomness of each fold in its 'cross fitting' strategy. Pass an int for reproducible output across multiple function calls. Missing Values Parameters detect_missing bool, default True By default, missing values are automatically detected and handled by the library. To disable this functionality set <code>'detect_missing': False</code>. Note - coresets can only be built when there are no missing values. drop_rows_belowfloat between 0 and 1, default 0 (nothing is dropped)  If the ratio of instances with missing values on any feature is lower than this ratio, those instances would be ignored during the coreset build.Example: <code>'drop_rows_below': 0.05</code> drop_cols_above  float between 0 and 1, default 1 (nothing is dropped). If the ratio of instances with missing values for a certain feature is higher than this ratio, this feature would be ignored during the coreset build.Example: <code>'drop_cols_above': 0.3</code> fill_value_num  float By default, missing values for numeric features would be replaced with the calculated mean. It is possible to change the default behavior for numeric features by defining a specific replacement number for all features using the <code>fill_value_num</code> or to use the <code>fill_value</code> attribute in the <code>features</code> list, to define a replacement on a feature-by-feature base.Example: <code>'fill_value_num':-1</code> fill_value_cat  Any  By default, missing values for categorical features would be treated just as another category/value when the feature is one-hot encoded by the library. It is possible to change the default behavior for categorical features by defining a specific replacement value or by specifying <code>take_most_common</code>, (which will fill the missing values with the most commonly  used value of the feature) for all categorical features using the  <code>fill_value_cat</code> or to use the <code>fill_value</code> attribute in the <code>features</code> list,  to define a replacement on a feature-by-feature base.  Example: <code>'fill_value_cat': 'take_most_common'</code> Data Transformation Parameters data_transform_before Transform A preprocessing function applied to the entire dataset. The function's signature is <code>func(dataset) -&gt; dataset</code>. See the example at the end of the page. feature_transform_default Transform A default feature transformation function applied on feature-by-feature base. Executed after the <code>data_transform_before</code>. The function can be overridden at the feature level with by defining the <code>transform</code> attribute in the <code>features</code> list. The function's signature is <code>func(dataset, transform_context:dict) -&gt; data</code>. The function returns the data of a single feature. Example: <code>'transform_context': {'feature_name': 'salary'}</code> data_transform_after Transform A preprocessing function similar to data_transform_before, executed after the feature-by-feature transformation.See the example at the end of the page. <p>Code example: <pre><code>def data_before_processing(dataset):\n    df = pd.DataFrame(dataset)\n    # remove dataset rows by condition\n    df = df[df['department'] != 'Head Office']\n    return df\n\ndef education_level_transform(dataset):\n    # replace categorical values with numeric\n    df = pd.DataFrame(dataset)\n    conditions = [\n        df['education_level'] = 'elementary_school',\n        df['education_level'] = 'high_school',\n        df['education_level'] = 'diploma',\n        df['education_level'] = 'associates',\n        df['education_level'] = 'bachelors',\n        df['education_level'] = 'masters',\n        df['education_level'] = 'doctorate',\n        ]\n    choices = [1, 2, 3, 4, 5, 6, 7]\n    df['education_level'] = np.select(conditions, choices)\n    return df['education_level']\n\ndef yearly_bonus_transform(dataset):\n    df = pd.DataFrame(dataset)\n    # for creating new feature\n    return df['h1_bonus'] + df['h2_bonus']\n\ndef transform_scaling(dataset):\n    from sklearn.preprocessing import MinMaxScaler\n    df = pd.DataFrame(dataset)\n    scaler = MinMaxScaler()\n    columns_to_scale = ['age', 'work_experience_in_months']\n    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n    return df\n\ndata_params = {\n    'features': [\n        {\n            'name': 'family_status',\n            'categorical': True, 'fill_value': 'single'\n        },\n        {'name': 'department', 'categorical': True},\n        {'name': 'gender'},\n        {'name': 'job_title'},\n        {'name': 'age', 'fill_value': 18},\n        {'name': 'work_experience_in_months'},\n        {\n            'name': 'education_level',\n            'transform': {'func': education_level_transform}\n        },\n        {\n            'name': 'yearly_bonus',\n            'transform': {'func': yearly_bonus_transform}\n        },\n    ],\n    'properties': [{'name': 'full_name'}, {'name': 'Hire Date'}],\n    'target': {'name': 'salary'},\n    'categorical_features': ['gender'],\n    'fill_value_cat': 'take_most_common',\n    'fill_value_num': 0,\n    'data_transform_before': {'func': data_before_processing},\n    'data_transform_after': {'func': transform_scaling},\n    'seq_column': {\n        'name': 'Hire Date',\n        'granularity': 'Y',\n        'datetime_format': '%d/%m/%Y',\n        'chunk_by': True,\n    },\n}\n</code></pre></p>"},{"location":"reference/services/coreset_tree/dtc/","title":"CoresetTreeServiceDTC","text":""},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC","title":"CoresetTreeServiceDTC","text":"<pre><code>CoresetTreeServiceDTC(*, data_manager=None, data_params=None, n_instances=None, max_memory_gb=None, n_classes=None, optimized_for, chunk_size=None, chunk_by=None, coreset_size=None, coreset_params=None, sample_all=None, working_directory=None, cache_dir=None, node_train_function=None, node_train_function_params=None, node_metadata_func=None, chunk_sample_ratio=None, model_cls=None, build_w_estimation=False)\n</code></pre> <p>             Bases: <code>DTMixin</code>, <code>CoresetTreeServiceClassifierMixin</code>, <code>CoresetTreeServiceSupervisedMixin</code>, <code>EstimationMixin</code>, <code>CoresetTreeService</code></p> <p>Subclass of CoresetTreeService for Decision Tree Classification-based problems. A service class for creating a coreset tree and working with it. optimized_for is a required parameter defining the main usage of the service: 'training', 'cleaning' or both, optimized_for=['training', 'cleaning']. The service will decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset, based on the quadruplet: n_instances, n_classes, max_memory_gb and the 'number of features' (deduced from the dataset). The chunk_size and coreset_size will be deduced based on the above quadruplet too. In case chunk_size and coreset_size are provided, they will override all above mentioned parameters (less recommended).</p> <p>If you intend passing class_weight to your classifier, it is recommended to pass it as a parameter to the class in coreset_params (see example below), so the coreset can be built while taking into account the class_weight. You can continue passing class_weight to your classifier, while retrieving the coreset using the get_coreset method with the parameter inverse_class_weight set to True (default). If you wish to stop passing class_weight to the classifier, retrieve the coreset using the get_coreset method with the parameter inverse_class_weight set to False.</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. The class used to interact with the provided data and store it locally. By default, only the sampled data is stored in HDF5 files format.</p> <code>None</code> <code>data_params</code> <code>Union[DataParams, dict]</code> <p>DataParams, optional. Preprocessing information.</p> <code>None</code> <code>n_instances</code> <code>int</code> <p>int. The total number of instances that are going to be processed (can be an estimation). This parameter is required and the only one from the above mentioned quadruplet, which isn't deduced from the data.</p> <code>None</code> <code>n_classes</code> <code>int</code> <p>int. The total number of classes (labels). When not provided, will be deduced from the provided data. When multiple files are provided n_classes will be deduced based on the first file only.</p> <code>None</code> <code>max_memory_gb</code> <code>int</code> <p>int, optional. The maximum memory in GB that should be used. When not provided, the server's total memory is used. In any case only 80% of the provided memory or the server's total memory is considered.</p> <code>None</code> <code>optimized_for</code> <code>Union[list, str]</code> <p>str or list Either 'training', 'cleaning' or or both ['training', 'cleaning']. The main usage of the service.</p> required <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances to be used when creating a coreset node in the tree. When defined, it will override the parameters of optimized_for, n_instances, n_classes and max_memory_gb. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>coreset_size</code> <code>Union[int, float, dict]</code> <p>int or float, optional. Represents the coreset size of each node in the coreset tree. If provided as a float, it represents the ratio between each chunk and the resulting coreset.In any case the coreset_size is limited to 60% of the chunk_size. The coreset is constructed by sampling data instances from the dataset based on their calculated importance. Since each instance may be sampled more than once, in practice, the actual size of the coreset is mostly smaller than coreset_size.</p> <code>None</code> <code>sample_all</code> <code>Iterable</code> <p>iterable, optional. Relevant for classification tasks only. A list of classes for which all instances should be taken, instead of applying sampling.</p> <code>None</code> <code>coreset_params</code> <code>Union[CoresetParams, dict]</code> <p>CoresetParams or dict, optional. Coreset algorithm specific parameters.</p> <p>For example: coreset_params = {     \"class_weight\": {\"a\": 0.5, \"b\": 0.5} }</p> <code>None</code> <code>node_train_function</code> <code>Callable[[ndarray, ndarray, ndarray], Any]</code> <p>Callable, optional. method for training model at tree node level.</p> <code>None</code> <code>node_train_function_params</code> <code>dict</code> <p>dict, optional. kwargs to be used when calling node_train_function.</p> <code>None</code> <code>node_metadata_func</code> <code>Callable[[Tuple[ndarray], ndarray, Union[list, None]], Union[list, dict, None]]</code> <p>callable, optional. A method for storing user meta data on each node.</p> <code>None</code> <code>working_directory</code> <code>Union[str, PathLike]</code> <p>str, path, optional. Local directory where intermediate data is stored.</p> <code>None</code> <code>cache_dir</code> <code>Union[str, PathLike]</code> <p>str, path, optional. For internal use when loading a saved service.</p> <code>None</code> <code>chunk_sample_ratio</code> <code>float</code> <p>float, optional. Indicates the size of the sample that will be taken and saved from each chunk on top of the Coreset for the validation methods. The values are from the range [0,1]. For example, chunk_sample_ratio=0.5, means that 50% of the data instances from each chunk will be saved.</p> <code>None</code> <code>model_cls</code> <code>Any</code> <p>A Scikit-learn compatible model class, optional. The model class used to train the model on the coreset, in case a specific model instance wasn't passed to fit or the validation methods. The default model class which will be selected for this class instance will be XGBClassifier, on condition the xgboost library is installed. Otherwise, LGBMClassifier will be chosen if the lightgbm library is installed. Else, in the presence of the Catboost library, the selected class will be the CatBoostClassifier. Lastly, if none of the mentioned three libraries are installed, sklearn's GradientBoostingClassifier will be chosen as the final fallback.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.auto_preprocessing","title":"auto_preprocessing","text":"<pre><code>auto_preprocessing(X=None, sparse_output=False, copy=False)\n</code></pre> <p>Apply auto-preprocessing on the provided (test) data, similarly to the way it is done by the fit or get_coreset methods. Preprocessing includes ohe-hot encoding and handling missing values depends.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features.</p> <code>None</code> <code>sparse_output</code> <code>bool</code> <p>boolean, default False. When set to True, the function will create a sparse matrix after preprocessing.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the following keys: data: A numpy array of the preprocessed data. features: A list of feature names corresponding to the data. sparse: A boolean indicating if the output is in sparse format.</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.build","title":"build","text":"<pre><code>build(X, y, indices=None, props=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree from the parameters X, y, indices and props (properties). build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> required <code>y</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like. An array or an iterator of targets.</p> required <code>indices</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator with indices of X.</p> <code>None</code> <code>props</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.build_from_df","title":"build_from_df","text":"<pre><code>build_from_df(datasets, target_datasets=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree from pandas DataFrame(s). build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator. Data includes features, may include labels and may include indices.</p> required <code>target_datasets</code> <code>Union[Iterator[Union[DataFrame, Series]], DataFrame, Series]</code> <p>pandas DataFrame or a DataFrame iterator, optional. Use when data is split to features and target. Should include only one column.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.build_from_file","title":"build_from_file","text":"<pre><code>build_from_file(file_path, target_file_path=None, *, reader_f=pd.read_csv, reader_kwargs=None, reader_chunk_size_param_name=None, chunk_size=None, chunk_by=None, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree based on data taken from local storage. build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories. Path(s) to the place where data is stored. Data includes features, may include targets and may include indices.</p> required <code>target_file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories, optional. Use when the dataset files are split to features and target. Each file should include only one column.</p> <code>None</code> <code>reader_f</code> <code>Callable</code> <p>pandas like read method, optional, default pandas read_csv. For example, to read excel files use pandas read_excel.</p> <code>read_csv</code> <code>reader_kwargs</code> <code>dict</code> <p>dict, optional. Keyword arguments used when calling reader_f method.</p> <code>None</code> <code>reader_chunk_size_param_name</code> <code>str</code> <p>str, optional. reader_f input parameter name for reading file in chunks. When not provided we'll try to figure it out our self. Based on the data, we decide on the optimal chunk size to read and use this parameter as input when calling reader_f. Use \"ignore\" to skip the automatic chunk reading logic.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.cross_validate","title":"cross_validate","text":"<pre><code>cross_validate(level=None, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage=None, sparse_threshold=0.01, enhancement=0, **model_params)\n</code></pre> <p>Method for cross-validation on the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : the computation time for each fold is displayed;     &gt;=2 : the score is also displayed;     &gt;=3 : starting time of the computation is also displayed.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>None</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>enhancement</code> <code>int</code> <p>int (0-3), optional, default 0 (no enhancement). Enhance the default decision tree based training. Can improve the strength of the model, but will increase the training run time.</p> <code>0</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>A list of scores, one for each fold. If return_model=True, a list of trained models is also returned (one model for each fold).</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.filter_out_samples","title":"filter_out_samples","text":"<pre><code>filter_out_samples(filter_function, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Remove samples from the coreset tree, based on the provided filter function. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>filter_function</code> <code>Callable[[Iterable, Iterable, Union[Iterable, None], Union[Iterable, None]], Iterable[Any]]</code> <p>function, optional. A function that returns a list of indices to be removed from the tree. The function should accept 4 parameters as input: indices, X, y, props and return a list(iterator) of indices to be removed from the coreset tree. For example, in order to remove all instances with a target equal to 6, use the following function: filter_function = lambda indices, X, y, props : indices[y = 6].</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.fit","title":"fit","text":"<pre><code>fit(level=0, seq_from=None, seq_to=None, enhancement=0, model=None, preprocessing_stage=None, sparse_threshold=0.01, **model_params)\n</code></pre> <p>Fit a model on the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>string/datetime, optional The starting sequence of the training set.</p> <code>None</code> <code>seq_to</code> <code>Any</code> <p>string/datetime, optional The ending sequence of the training set.</p> <code>None</code> <code>enhancement</code> <code>int</code> <p>int (0-3), optional, default 0 (no enhancement). Enhance the default decision tree based training. Can improve the strength of the model, but will increase the training run time.</p> <code>0</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>None</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>Model hyperparameters kwargs. Input when instantiating default model class.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Fitted estimator.</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.get_cleaning_samples","title":"get_cleaning_samples","text":"<pre><code>get_cleaning_samples(size=None, class_size=None, ignore_indices=None, select_from_indices=None, select_from_function=None, ignore_seen_samples=True)\n</code></pre> <p>Returns indices of samples in descending order of importance. Useful for identifying mislabeled instances and other anomalies in the data. Either class_size (recommended) or size must be provided. Must be called after build. This function is only applicable in case the coreset tree was optimized_for 'cleaning'. This function is not for retrieving the coreset (use get_coreset in this case).</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>int, optional Number of samples to return. When class_size is provided, remaining samples are taken from classes not appearing in class_size dictionary.</p> <code>None</code> <code>class_size</code> <code>Dict[Any, Union[int, str]]</code> <p>dict {class: int or \"all\" or \"any\"}, optional. Controls the number of samples to choose for each class. int: return at most size. \"all\": return all samples. \"any\": limits the returned samples to the specified classes.</p> <code>None</code> <code>ignore_indices</code> <code>Iterable</code> <p>array-like, optional. An array of indices to ignore when selecting cleaning samples.</p> <code>None</code> <code>select_from_indices</code> <code>Iterable</code> <p>array-like, optional.  An array of indices to consider when selecting cleaning samples.</p> <code>None</code> <code>select_from_function</code> <code>Callable[[Iterable, Iterable, Union[Iterable, None], Union[Iterable, None]], Iterable[Any]]</code> <p>function, optional.  Pass a function in order to limit the selection of the cleaning samples accordingly.  The function should accept 4 parameters as input: indices, X, y, properties  and return a list(iterator) of the desired indices.</p> <code>None</code> <code>ignore_seen_samples</code> <code>bool</code> <p>bool, optional, default True.  Exclude already seen samples and set the seen flag on any indices returned by the function.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Union[ValueError, dict]</code> <p>idx: array-like[int].     Cleaning samples indices. X: array-like[int].     X array. y: array-like[int].     y array. importance: array-like[float].     The importance property. Instances that receive a high Importance in the Coreset computation,     require attention as they usually indicate a labeling error,     anomaly, out-of-distribution problem or other data-related issue.</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.get_cleaning_samples--examples","title":"Examples","text":"Input <p>size=100, class_size={\"class A\": 10, \"class B\": 50, \"class C\": \"all\"}</p> Output <p>10 of \"class A\", 50 of \"class B\", 12 of \"class C\" (all), 28 of \"class D/E\"</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.get_coreset","title":"get_coreset","text":"<pre><code>get_coreset(level=0, seq_from=None, seq_to=None, preprocessing_stage='user', sparse_threshold=0.01, as_df=False, with_index=False, inverse_class_weight=True)\n</code></pre> <p>Get tree's coreset data in one of the preprocessing_stage(s) in the data preprocessing workflow. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>Any, optional. The start of the time range from which to return the coreset.</p> <code>None</code> <code>seq_to</code> <code>Any</code> <p>Any, optional. The end of the time range from which to return the coreset.</p> <code>None</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Returns the features (X) as a sparse matrix if the data density after preprocessing is below sparse_threshold, otherwise, will return the data as an array (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>as_df</code> <code>bool</code> <p>boolean, optional, default False. When True, returns the X as a pandas DataFrame.</p> <code>False</code> <code>with_index</code> <code>bool</code> <p>boolean, optional, default False. Relevant only when preprocessing_stage='auto'. Should the returned data include the index column.</p> <code>False</code> <code>inverse_class_weight</code> <code>bool</code> <p>boolean, default True. True - return weights / class_weights. False - return weights as they are. Relevant only for classification tasks and only if class_weight was passed in the coreset_params when initializing the class.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary representing the Coreset: ind: A numpy array of indices. X: A numpy array of the feature matrix. y: A numpy array of the target values. w: A numpy array of sample weights. n_represents: The number of instances represented by the coreset. features_out: A list of the output features, if preprocessing_stage=auto, otherwise None. props: A numpy array of properties, or None if not available.</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.get_coreset_size","title":"get_coreset_size","text":"<pre><code>get_coreset_size(level=0, seq_from=None, seq_to=None)\n</code></pre> <p>Returns the size of the tree's coreset data. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>string or datetime, optional, default None. The start sequence to filter samples by.</p> <code>None</code> <code>seq_to</code> <code>Union[str, datetime]</code> <p>string or datetime, optional, default None. The end sequence to filter samples by.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>coreset size</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.get_hyperparameter_tuning_data","title":"get_hyperparameter_tuning_data","text":"<pre><code>get_hyperparameter_tuning_data(level=None, validation_method='cross validation', preprocessing_stage='user', sparse_threshold=0.01, validation_size=0.2, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, inverse_class_weight=True, as_df=True)\n</code></pre> <p>A method for retrieving the data for hyperparameter tuning with cross validation, using the coreset tree. The returned data can be used with Scikit-learn\u2019s GridSearchCV, with skopt\u2019s BayesSearchCV and with any other hyperparameter tuning method that can accept a fold iterator object. Note: When using this method with Scikit-learn's <code>GridSearchCV</code> and similar methods, the <code>refit</code> parameter must be set to <code>False</code>. This is because the returned dataset (X, y and w) includes both training and validation data due to the use of a splitter. The returned dataset (X, y and w) is the concatenation of training data for all folds followed by validation data for all folds. By default, GridSearchCV refits the estimator on the entire dataset, not just the training portion, and this behavior cannot be modified and is incorrect. In this case, refit should be handled manually after the cross-validation process, by calling get_coreset with the same parameters that were passed to this function to retrieve the data and then fitting on the returned data using the best hyperparameters found in GridSearchCV. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>None</code> <code>validation_method</code> <code>str</code> <p>str, optional. Indicates which validation method will be used. The possible values are 'cross validation', 'hold-out validation' and 'seq-dependent validation'.</p> <code>'cross validation'</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Returns the features (X) as a sparse matrix if the data density after preprocessing is below sparse_threshold, otherwise, will return the data as an array (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>validation_size</code> <code>float</code> <p>float, optional, default 0.2. The size of the validation set, as a percentage of the training set size for hold-out validation.</p> <code>0.2</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>as_df</code> <code>bool</code> <p>boolean, optional, default False. When True, returns the X as a pandas DataFrame.</p> <code>True</code> <code>inverse_class_weight</code> <code>bool</code> <p>boolean, default True. True - return weights / class_weights. False - return weights as they are. Relevant only for classification tasks and only if class_weight was passed in the coreset_params when initializing the class.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, FoldIterator, Any]]</code> <p>A dictionary with the following keys: ind: The indices of the data. X: The data. y: The labels. w: The weights. splitter: The fold iterator. model_params: The model parameters.</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.get_max_level","title":"get_max_level","text":"<pre><code>get_max_level()\n</code></pre> <p>Return the maximal level of the coreset tree. Level 0 is the head of the tree. Level 1 is the level below the head of the tree, etc.</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.grid_search","title":"grid_search","text":"<pre><code>grid_search(param_grid, level=None, validation_method='cross validation', model=None, scoring=None, refit=True, verbose=0, preprocessing_stage=None, sparse_threshold=0.01, enhancement=0, error_score=np.nan, validation_size=0.2, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, n_jobs=None)\n</code></pre> <p>A method for performing hyperparameter selection by grid search, using the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>param_grid</code> <code>Union[Dict[str, List], List[Dict[str, List]]]</code> <p>dict or list of dicts. Dictionary with parameters names (str) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.</p> required <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>validation_method</code> <code>str</code> <p>str, optional. Indicates which validation method will be used. The possible values are 'cross validation', 'hold-out validation' and 'seq-dependent validation'. If 'cross validation' is selected, the process involves progressing through folds. We first train and validate all hyperparameter combinations for each fold, before moving on to the subsequent folds.</p> <code>'cross validation'</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. The model class needs to implement the usual scikit-learn interface.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>refit</code> <code>bool</code> <p>bool, optional. If True, retrain the model on the whole coreset using the best found hyperparameters, and return the model.</p> <code>True</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : the computation time for each fold and parameter candidate is displayed;     &gt;=2 : the score is also displayed;     &gt;=3 : starting time of the computation is also displayed.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>None</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>enhancement</code> <code>int</code> <p>int (0-3), optional, default 0 (no enhancement). Enhance the default decision tree based training. Can improve the strength of the model, but will increase the training run time.</p> <code>0</code> <code>error_score</code> <code>Union[str, float, int]</code> <p>\"raise\" or numeric, optional. Value to assign to the score if an error occurs in model training. If set to \"raise\", the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.</p> <code>nan</code> <code>validation_size</code> <code>float</code> <p>float, optional, default 0.2. The size of the validation set, as a percentage of the training set size for hold-out validation.</p> <code>0.2</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>int, optional. Default: number of CPUs. Number of jobs to run in parallel during grid search.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Dict, DataFrame, BaseEstimator], Tuple[Dict, DataFrame]]</code> <p>A dict with the best hyperparameters setting, among those provided by the user. The keys are the hyperparameters names, while the dicts' values are the hyperparameters values. A Pandas DataFrame holding the score for each hyperparameter combination and fold. For the 'cross validation' method the average across all folds for each hyperparameter combination is included too. If refit=True, the retrained model is also returned.</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.holdout_validate","title":"holdout_validate","text":"<pre><code>holdout_validate(level=None, validation_size=0.2, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage=None, sparse_threshold=0.01, enhancement=0, **model_params)\n</code></pre> <p>A method for hold-out validation on the coreset tree. The validation set is always the last part of the dataset. This function is only applicable in case the coreset tree was optimized_for <code>training</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>validation_size</code> <code>float</code> <p>float, optional. The percentage of the dataset that will be used for validating the model.</p> <code>0.2</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : the training and validation time is displayed;     &gt;=2 : the validation score is also displayed;     &gt;=3 : starting time of the computation is also displayed.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>None</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>enhancement</code> <code>int</code> <p>int (0-3), optional, default 0 (no enhancement). Enhance the default decision tree based training. Can improve the strength of the model, but will increase the training run time.</p> <code>0</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>The validation score. If return_model=True, the trained model is also returned.</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.is_dirty","title":"is_dirty","text":"<pre><code>is_dirty()\n</code></pre> <p>Returns:</p> Type Description <code>bool</code> <p>Indicates whether the coreset tree has nodes marked as dirty, meaning they were affected by any of the methods: remove_samples, update_targets, update_features or filter_out_samples, when they were called with force_do_nothing.</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(dir_path, name=None, *, data_manager=None, load_buffer=True, working_directory=None)\n</code></pre> <p>Restore a service object from a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>str, path. Local directory where service data is stored.</p> required <code>name</code> <code>str</code> <p>string, optional, default service class name (lower case). The name prefix of the subdirectory to load. When several subdirectories having the same name prefix are found, the last one, ordered by name, is selected. For example when saving with override=False, the chosen subdirectory is the last saved.</p> <code>None</code> <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. When specified, input data manger will be used instead of restoring it from the saved configuration.</p> <code>None</code> <code>load_buffer</code> <code>bool</code> <p>boolean, optional, default True. If set, load saved buffer (a partial node of the tree) from disk and add it to the tree.</p> <code>True</code> <code>working_directory</code> <code>Union[str, PathLike]</code> <p>str, path, optional, default use working_directory from saved configuration. Local directory where intermediate data is stored.</p> <code>None</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>CoresetTreeService object</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.partial_build","title":"partial_build","text":"<pre><code>partial_build(X, y, indices=None, props=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree from parameters X, y, indices and props (properties). Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> required <code>y</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like. An array or an iterator of targets.</p> required <code>indices</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator with indices of X.</p> <code>None</code> <code>props</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.partial_build_from_df","title":"partial_build_from_df","text":"<pre><code>partial_build_from_df(datasets, target_datasets=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree based on the pandas DataFrame iterator. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator. Data includes features, may include targets and may include indices.</p> required <code>target_datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator, optional. Use when data is split to features and target. Should include only one column.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional, default previous used chunk_size. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.partial_build_from_file","title":"partial_build_from_file","text":"<pre><code>partial_build_from_file(file_path, target_file_path=None, *, reader_f=pd.read_csv, reader_kwargs=None, reader_chunk_size_param_name=None, chunk_size=None, chunk_by=None, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree based on data taken from local storage. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories. Path(s) to the place where data is stored. Data includes features, may include targets and may include indices.</p> required <code>target_file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories, optional. Use when files are split to features and target. Each file should include only one column.</p> <code>None</code> <code>reader_f</code> <code>Callable</code> <p>pandas like read method, optional, default pandas read_csv. For example, to read excel files use pandas read_excel.</p> <code>read_csv</code> <code>reader_kwargs</code> <code>dict</code> <p>dict, optional. Keyword arguments used when calling reader_f method.</p> <code>None</code> <code>reader_chunk_size_param_name</code> <code>str</code> <p>str, optional. reader_f input parameter name for reading file in chunks. When not provided we'll try to figure it out our self. Based on the data, we decide on the optimal chunk size to read and use this parameter as input when calling reader_f. Use \"ignore\" to skip the automatic chunk reading logic.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional, default previous used chunk_size. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.plot","title":"plot","text":"<pre><code>plot(dir_path=None, selected_trees=None)\n</code></pre> <p>Produce a tree graph plot and save figure as a local png file.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike. Path to save the plot figure in; if not provided, or if isn't valid/doesn't exist, the figure will be saved in the current directory (from which this method is called).</p> <code>None</code> <code>selected_trees</code> <code>dict</code> <p>dict, optional. A dictionary containing the names of the image file(s) to be generated.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Image file path</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.predict","title":"predict","text":"<pre><code>predict(X, sparse_output=False, copy=False)\n</code></pre> <p>Run prediction on the trained model. This function is only applicable in case the coreset tree was optimized_for 'training' and in case fit() or grid_search(refit=True) where called before. The function automatically preprocesses the data according to the preprocessing_stage used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>An array of features.</p> required <code>sparse_output</code> <code>bool</code> <p>boolean, optional, default False. When set to True, the function will create a sparse matrix after preprocessing and pass it to the predict function.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <p>Model prediction results.</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X, sparse_output=False, copy=False)\n</code></pre> <p>Run prediction on the trained model. This function is only applicable in case the coreset tree was optimized_for 'training' and in case fit() or grid_search(refit=True) where called before. The function automatically preprocesses the data according to the preprocessing_stage used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>An array of features.</p> required <code>sparse_output</code> <code>bool</code> <p>boolean, optional, default False. When set to True, the function will create a sparse matrix after preprocessing and pass it to the predict_proba function.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <p>Returns the probability of the sample for each class in the model.</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.print","title":"print","text":"<pre><code>print(selected_tree=None)\n</code></pre> <p>Print the tree's string representation.</p> <p>Parameters:</p> Name Type Description Default <code>selected_tree</code> <code>str</code> <p>string, optional. Which tree to print. Defaults to printing all.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.remove_samples","title":"remove_samples","text":"<pre><code>remove_samples(indices, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Remove samples from the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be removed from the coreset tree.</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.save","title":"save","text":"<pre><code>save(dir_path=None, name=None, save_buffer=True, override=False, allow_pickle=True)\n</code></pre> <p>Save service configuration and relevant data to a local directory. Use this method when the service needs to be restored.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike, optional, default self.working_directory. A local directory for saving service's files.</p> <code>None</code> <code>name</code> <code>str</code> <p>string, optional, default service class name (lower case). Name of the subdirectory where the data will be stored.</p> <code>None</code> <code>save_buffer</code> <code>bool</code> <p>boolean, default True. Save also the data in the buffer (a partial node of the tree) along with the rest of the saved data.</p> <code>True</code> <code>override</code> <code>bool</code> <p>bool, optional, default False. False: add a timestamp suffix so each save won\u2019t override the previous ones. True: The existing subdirectory with the provided name is overridden.</p> <code>False</code> <code>allow_pickle</code> <code>bool</code> <p>bool, optional, default True. True: Saves the Coreset tree in pickle format (much faster). False: Saves the Coreset tree in JSON format.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>Save directory path.</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.save_coreset","title":"save_coreset","text":"<pre><code>save_coreset(file_path, level=0, preprocessing_stage='user', with_index=True)\n</code></pre> <p>Get the coreset from the tree and save it to a file. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike. Local file path to store the coreset.</p> required <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>with_index</code> <code>bool</code> <p>boolean, optional, default False. Relevant only when preprocessing_stage=<code>auto</code>. Should the returned data include the index column.</p> <code>True</code>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.seq_dependent_validate","title":"seq_dependent_validate","text":"<pre><code>seq_dependent_validate(level=None, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, enhancement=0, **model_params)\n</code></pre> <p>The method allows to train and validate on a subset of the Coreset tree, according to the <code>seq_column</code> defined in the <code>DataParams</code> structure passed to the init. This function is only applicable in case the coreset tree was optimized_for <code>training</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree from which the search for the best matching nodes starts. Nodes closer to the leaf level than the specified level, may be selected to better match the provided seq parameters.If None, the search starts from level 0, the head of the tree. If None, the best level will be selected.</p> <code>None</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>enhancement</code> <code>int</code> <p>int (0-3), optional, default 0 (no enhancement). Enhance the default decision tree based training. Can improve the strength of the model, but will increase the training run time.</p> <code>0</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>The validation score. If return_model=True, the trained model is also returned.</p>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.set_model_cls","title":"set_model_cls","text":"<pre><code>set_model_cls(model_cls)\n</code></pre> <p>Set the model class used to train the model on the coreset, in case a specific model instance wasn't passed to fit or the validation methods.</p> <p>Parameters:</p> Name Type Description Default <code>model_cls</code> <code>Any</code> <p>A Scikit-learn compatible model class.</p> required"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.set_seen_indication","title":"set_seen_indication","text":"<pre><code>set_seen_indication(seen_flag=True, indices=None)\n</code></pre> <p>Set samples as 'seen' or 'unseen'. Not providing an indices list defaults to setting the flag on all samples. This function is only applicable in case the coreset tree was optimized_for 'cleaning'.</p> <p>Parameters:</p> Name Type Description Default <code>seen_flag</code> <code>bool</code> <p>bool, optional, default True. Set 'seen' or 'unseen' flag</p> <code>True</code> <code>indices</code> <code>Iterable</code> <p>array like, optional. Set flag only for the provided list of indices. Defaults to all indices.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.update_dirty","title":"update_dirty","text":"<pre><code>update_dirty(force_resample_all=None, force_sensitivity_recalc=None)\n</code></pre> <p>Calculate the sensitivity and resample the nodes that were marked as dirty, meaning they were affected by any of the methods: remove_samples, update_targets, update_features or filter_out_samples, when they were called with force_do_nothing.</p> <p>Parameters:</p> Name Type Description Default <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.update_features","title":"update_features","text":"<pre><code>update_features(indices, X, feature_names=None, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Update the features for selected samples on the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be updated.</p> required <code>X</code> <code>Iterable</code> <p>array-like. An array of features. Should have the same length as indices.</p> required <code>feature_names</code> <code>Iterable[str]</code> <p>If the quantity of features in X is not equal to the quantity of features in the original coreset, this param should contain list of names of passed features.</p> <code>None</code> <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/dtc/#services.coreset_tree.dtc.CoresetTreeServiceDTC.update_targets","title":"update_targets","text":"<pre><code>update_targets(indices, y, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Update the targets for selected samples on the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be updated.</p> required <code>y</code> <code>Iterable</code> <p>array-like. An array of classes/labels. Should have the same length as indices.</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/dtr/","title":"CoresetTreeServiceDTR","text":""},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR","title":"CoresetTreeServiceDTR","text":"<pre><code>CoresetTreeServiceDTR(*, data_manager=None, data_params=None, n_instances=None, max_memory_gb=None, n_classes=None, optimized_for, chunk_size=None, chunk_by=None, coreset_size=None, coreset_params=None, working_directory=None, cache_dir=None, node_train_function=None, node_train_function_params=None, node_metadata_func=None, chunk_sample_ratio=None, model_cls=None)\n</code></pre> <p>             Bases: <code>DTMixin</code>, <code>CoresetTreeServiceSupervisedMixin</code>, <code>CoresetTreeService</code></p> <p>Subclass of CoresetTreeService for Decision Tree Regression-based problems. A service class for creating a coreset tree and working with it. optimized_for is a required parameter defining the main usage of the service: 'training', 'cleaning' or both, optimized_for=['training', 'cleaning']. The service will decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset, based on the quadruplet: n_instances, n_classes, max_memory_gb and the 'number of features' (deduced from the dataset). The chunk_size and coreset_size will be deduced based on the above quadruplet too. In case chunk_size and coreset_size are provided, they will override all above mentioned parameters (less recommended).</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. The class used to interact with the provided data and store it locally. By default, only the sampled data is stored in HDF5 files format.</p> <code>None</code> <code>data_params</code> <code>Union[DataParams, dict]</code> <p>DataParams, optional. Data preprocessing information.</p> <code>None</code> <code>n_instances</code> <code>int</code> <p>int. The total number of instances that are going to be processed (can be an estimation). This parameter is required and the only one from the above mentioned quadruplet, which isn't deduced from the data.</p> <code>None</code> <code>max_memory_gb</code> <code>int</code> <p>int, optional. The maximum memory in GB that should be used. When not provided, the server's total memory is used. In any case only 80% of the provided memory or the server's total memory is considered.</p> <code>None</code> <code>optimized_for</code> <code>Union[list, str]</code> <p>str or list Either 'training', 'cleaning' or or both ['training', 'cleaning']. The main usage of the service.</p> required <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances to be used when creating a coreset node in the tree. When defined, it will override the parameters of optimized_for, n_instances, n_classes and max_memory_gb. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>coreset_size</code> <code>Union[int, float, dict]</code> <p>int or float, optional. Represents the coreset size of each node in the coreset tree. If provided as a float, it represents the ratio between each chunk and the resulting coreset.In any case the coreset_size is limited to 60% of the chunk_size. The coreset is constructed by sampling data instances from the dataset based on their calculated importance. Since each instance may be sampled more than once, in practice, the actual size of the coreset is mostly smaller than coreset_size.</p> <code>None</code> <code>coreset_params</code> <code>Union[CoresetParams, dict]</code> <p>CoresetParams or dict, optional. Coreset algorithm specific parameters.</p> <code>None</code> <code>node_train_function</code> <code>Callable[[ndarray, ndarray, ndarray], Any]</code> <p>Callable, optional. method for training model at tree node level.</p> <code>None</code> <code>node_train_function_params</code> <code>dict</code> <p>dict, optional. kwargs to be used when calling node_train_function.</p> <code>None</code> <code>node_metadata_func</code> <code>Callable[[Tuple[ndarray], ndarray, Union[list, None]], Union[list, dict, None]]</code> <p>callable, optional. A method for storing user meta data on each node.</p> <code>None</code> <code>working_directory</code> <code>Union[str, PathLike]</code> <p>str, path, optional. Local directory where intermediate data is stored.</p> <code>None</code> <code>cache_dir</code> <code>Union[str, PathLike]</code> <p>str, path, optional. For internal use when loading a saved service.</p> <code>None</code> <code>chunk_sample_ratio</code> <code>float</code> <p>float, optional. Indicates the size of the sample that will be taken and saved from each chunk on top of the Coreset for the validation methods. The values are from the range [0,1]. For example, chunk_sample_ratio=0.5, means that 50% of the data instances from each chunk will be saved.</p> <code>None</code> <code>model_cls</code> <code>Any</code> <p>A Scikit-learn compatible model class, optional. The model class used to train the model on the coreset, in case a specific model instance wasn't passed to fit or the validation methods. The default model class which will be selected for this class instance will be XGBRegressor, on condition the xgboost library is installed. Otherwise, LGBMRegressor will be chosen if the lightgbm library is installed. Else, in the presence of the Catboost library, the selected class will be the CatBoostRegressor. Lastly, if none of the mentioned three libraries are installed, sklearn's GradientBoostingRegressor will be chosen as the final fallback.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.auto_preprocessing","title":"auto_preprocessing","text":"<pre><code>auto_preprocessing(X=None, sparse_output=False, copy=False)\n</code></pre> <p>Apply auto-preprocessing on the provided (test) data, similarly to the way it is done by the fit or get_coreset methods. Preprocessing includes ohe-hot encoding and handling missing values depends.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features.</p> <code>None</code> <code>sparse_output</code> <code>bool</code> <p>boolean, default False. When set to True, the function will create a sparse matrix after preprocessing.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the following keys: data: A numpy array of the preprocessed data. features: A list of feature names corresponding to the data. sparse: A boolean indicating if the output is in sparse format.</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.build","title":"build","text":"<pre><code>build(X, y, indices=None, props=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree from the parameters X, y, indices and props (properties). build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> required <code>y</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like. An array or an iterator of targets.</p> required <code>indices</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator with indices of X.</p> <code>None</code> <code>props</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.build_from_df","title":"build_from_df","text":"<pre><code>build_from_df(datasets, target_datasets=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree from pandas DataFrame(s). build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator. Data includes features, may include labels and may include indices.</p> required <code>target_datasets</code> <code>Union[Iterator[Union[DataFrame, Series]], DataFrame, Series]</code> <p>pandas DataFrame or a DataFrame iterator, optional. Use when data is split to features and target. Should include only one column.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.build_from_file","title":"build_from_file","text":"<pre><code>build_from_file(file_path, target_file_path=None, *, reader_f=pd.read_csv, reader_kwargs=None, reader_chunk_size_param_name=None, chunk_size=None, chunk_by=None, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree based on data taken from local storage. build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories. Path(s) to the place where data is stored. Data includes features, may include targets and may include indices.</p> required <code>target_file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories, optional. Use when the dataset files are split to features and target. Each file should include only one column.</p> <code>None</code> <code>reader_f</code> <code>Callable</code> <p>pandas like read method, optional, default pandas read_csv. For example, to read excel files use pandas read_excel.</p> <code>read_csv</code> <code>reader_kwargs</code> <code>dict</code> <p>dict, optional. Keyword arguments used when calling reader_f method.</p> <code>None</code> <code>reader_chunk_size_param_name</code> <code>str</code> <p>str, optional. reader_f input parameter name for reading file in chunks. When not provided we'll try to figure it out our self. Based on the data, we decide on the optimal chunk size to read and use this parameter as input when calling reader_f. Use \"ignore\" to skip the automatic chunk reading logic.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.cross_validate","title":"cross_validate","text":"<pre><code>cross_validate(level=None, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage=None, sparse_threshold=0.01, enhancement=0, **model_params)\n</code></pre> <p>Method for cross-validation on the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : the computation time for each fold is displayed;     &gt;=2 : the score is also displayed;     &gt;=3 : starting time of the computation is also displayed.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>None</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>enhancement</code> <code>int</code> <p>int (0-3), optional, default 0 (no enhancement). Enhance the default decision tree based training. Can improve the strength of the model, but will increase the training run time.</p> <code>0</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>A list of scores, one for each fold. If return_model=True, a list of trained models is also returned (one model for each fold).</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.filter_out_samples","title":"filter_out_samples","text":"<pre><code>filter_out_samples(filter_function, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Remove samples from the coreset tree, based on the provided filter function. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>filter_function</code> <code>Callable[[Iterable, Iterable, Union[Iterable, None], Union[Iterable, None]], Iterable[Any]]</code> <p>function, optional. A function that returns a list of indices to be removed from the tree. The function should accept 4 parameters as input: indices, X, y, props and return a list(iterator) of indices to be removed from the coreset tree. For example, in order to remove all instances with a target equal to 6, use the following function: filter_function = lambda indices, X, y, props : indices[y = 6].</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.fit","title":"fit","text":"<pre><code>fit(level=0, seq_from=None, seq_to=None, enhancement=0, model=None, preprocessing_stage=None, sparse_threshold=0.01, **model_params)\n</code></pre> <p>Fit a model on the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>string/datetime, optional The starting sequence of the training set.</p> <code>None</code> <code>seq_to</code> <code>Any</code> <p>string/datetime, optional The ending sequence of the training set.</p> <code>None</code> <code>enhancement</code> <code>int</code> <p>int (0-3), optional, default 0 (no enhancement). Enhance the default decision tree based training. Can improve the strength of the model, but will increase the training run time.</p> <code>0</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>None</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>Model hyperparameters kwargs. Input when instantiating default model class.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Fitted estimator.</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.get_cleaning_samples","title":"get_cleaning_samples","text":"<pre><code>get_cleaning_samples(size=None, ignore_indices=None, select_from_indices=None, select_from_function=None, ignore_seen_samples=True)\n</code></pre> <p>Returns indices of samples in descending order of importance. Useful for identifying mislabeled instances and other anomalies in the data. size must be provided. Function must be called after build. This function is only applicable in case the coreset tree was optimized_for 'cleaning'. This function is not for retrieving the coreset (use get_coreset in this case).</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>required, optional. Number of samples to return.</p> <code>None</code> <code>ignore_indices</code> <code>Iterable</code> <p>array-like, optional. An array of indices to ignore when selecting cleaning samples.</p> <code>None</code> <code>select_from_indices</code> <code>Iterable</code> <p>array-like, optional.  An array of indices to consider when selecting cleaning samples.</p> <code>None</code> <code>select_from_function</code> <code>Callable[[Iterable, Iterable, Union[Iterable, None], Union[Iterable, None]], Iterable[Any]]</code> <p>function, optional.  Pass a function in order to limit the selection of the cleaning samples accordingly.  The function should accept 4 parameters as input: indices, X, y, props.  and return a list(iterator) of the desired indices.</p> <code>None</code> <code>ignore_seen_samples</code> <code>bool</code> <p>bool, optional, default True.  Exclude already seen samples and set the seen flag on any indices returned by the function.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Union[ValueError, dict]</code> <p>idx: array-like[int].     Cleaning samples indices. X: array-like[int].     X array. y: array-like[int].     y array. importance: array-like[float].     The importance property. Instances that receive a high Importance in the Coreset computation,     require attention as they usually indicate a labeling error,     anomaly, out-of-distribution problem or other data-related issue.</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.get_coreset","title":"get_coreset","text":"<pre><code>get_coreset(level=0, preprocessing_stage='user', sparse_threshold=0.01, as_df=False, with_index=False, seq_from=None, seq_to=None)\n</code></pre> <p>Get tree's coreset data in one of the preprocessing_stage(s) in the data preprocessing workflow. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Returns the features (X) as a sparse matrix if the data density after preprocessing is below sparse_threshold, otherwise, will return the data as an array (Applicable only for preprocessing_stage=<code>auto</code>).</p> <code>0.01</code> <code>as_df</code> <code>bool</code> <p>boolean, optional, default False. When True, returns the X as a pandas DataFrame.</p> <code>False</code> <code>with_index</code> <code>bool</code> <p>boolean, optional, default False. Relevant only when preprocessing_stage=<code>auto</code>. Should the returned data include the index column.</p> <code>False</code> <code>seq_from</code> <code>Any</code> <p>string or datetime, optional, default None. The start sequence to filter samples by.</p> <code>None</code> <code>seq_to</code> <code>Any</code> <p>string or datetime, optional, default None. The end sequence to filter samples by.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary representing the Coreset: ind: A numpy array of indices. X: A numpy array of the feature matrix. y: A numpy array of the target values. w: A numpy array of sample weights. n_represents: The number of instances represented by the coreset. features_out: A list of the output features, if preprocessing_stage=auto, otherwise None. props: A numpy array of properties, or None if not available.</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.get_coreset_size","title":"get_coreset_size","text":"<pre><code>get_coreset_size(level=0, seq_from=None, seq_to=None)\n</code></pre> <p>Returns the size of the tree's coreset data. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>string or datetime, optional, default None. The start sequence to filter samples by.</p> <code>None</code> <code>seq_to</code> <code>Union[str, datetime]</code> <p>string or datetime, optional, default None. The end sequence to filter samples by.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>coreset size</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.get_hyperparameter_tuning_data","title":"get_hyperparameter_tuning_data","text":"<pre><code>get_hyperparameter_tuning_data(level=None, validation_method='cross validation', preprocessing_stage='user', sparse_threshold=0.01, validation_size=0.2, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, as_df=True)\n</code></pre> <p>A method for retrieving the data for hyperparameter tuning with cross validation, using the coreset tree. The returned data can be used with Scikit-learn\u2019s GridSearchCV, with skopt\u2019s BayesSearchCV and with any other hyperparameter tuning method that can accept a fold iterator object. Note: When using this method with Scikit-learn's <code>GridSearchCV</code> and similar methods, the <code>refit</code> parameter must be set to <code>False</code>. This is because the returned dataset (X, y and w) includes both training and validation data due to the use of a splitter. The returned dataset (X, y and w) is the concatenation of training data for all folds followed by validation data for all folds. By default, GridSearchCV refits the estimator on the entire dataset, not just the training portion, and this behavior cannot be modified and is incorrect. In this case, refit should be handled manually after the cross-validation process, by calling get_coreset with the same parameters that were passed to this function to retrieve the data and then fitting on the returned data using the best hyperparameters found in GridSearchCV. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>None</code> <code>validation_method</code> <code>str</code> <p>str, optional. Indicates which validation method will be used. The possible values are 'cross validation', 'hold-out validation' and 'seq-dependent validation'.</p> <code>'cross validation'</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Returns the features (X) as a sparse matrix if the data density after preprocessing is below sparse_threshold, otherwise, will return the data as an array (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>validation_size</code> <code>float</code> <p>float, optional, default 0.2. The size of the validation set, as a percentage of the training set size for hold-out validation.</p> <code>0.2</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>as_df</code> <code>bool</code> <p>boolean, optional, default False. When True, returns the X as a pandas DataFrame.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, FoldIterator, Any]]</code> <p>A dictionary with the following keys: ind: The indices of the data. X: The data. y: The labels. w: The weights. splitter: The fold iterator. model_params: The model parameters.</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.get_max_level","title":"get_max_level","text":"<pre><code>get_max_level()\n</code></pre> <p>Return the maximal level of the coreset tree. Level 0 is the head of the tree. Level 1 is the level below the head of the tree, etc.</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.grid_search","title":"grid_search","text":"<pre><code>grid_search(param_grid, level=None, validation_method='cross validation', model=None, scoring=None, refit=True, verbose=0, preprocessing_stage=None, sparse_threshold=0.01, enhancement=0, error_score=np.nan, validation_size=0.2, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, n_jobs=None)\n</code></pre> <p>A method for performing hyperparameter selection by grid search, using the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>param_grid</code> <code>Union[Dict[str, List], List[Dict[str, List]]]</code> <p>dict or list of dicts. Dictionary with parameters names (str) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.</p> required <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>validation_method</code> <code>str</code> <p>str, optional. Indicates which validation method will be used. The possible values are 'cross validation', 'hold-out validation' and 'seq-dependent validation'. If 'cross validation' is selected, the process involves progressing through folds. We first train and validate all hyperparameter combinations for each fold, before moving on to the subsequent folds.</p> <code>'cross validation'</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. The model class needs to implement the usual scikit-learn interface.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>refit</code> <code>bool</code> <p>bool, optional. If True, retrain the model on the whole coreset using the best found hyperparameters, and return the model.</p> <code>True</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : the computation time for each fold and parameter candidate is displayed;     &gt;=2 : the score is also displayed;     &gt;=3 : starting time of the computation is also displayed.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>None</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>enhancement</code> <code>int</code> <p>int (0-3), optional, default 0 (no enhancement). Enhance the default decision tree based training. Can improve the strength of the model, but will increase the training run time.</p> <code>0</code> <code>error_score</code> <code>Union[str, float, int]</code> <p>\"raise\" or numeric, optional. Value to assign to the score if an error occurs in model training. If set to \"raise\", the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.</p> <code>nan</code> <code>validation_size</code> <code>float</code> <p>float, optional, default 0.2. The size of the validation set, as a percentage of the training set size for hold-out validation.</p> <code>0.2</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>int, optional. Default: number of CPUs. Number of jobs to run in parallel during grid search.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Dict, DataFrame, BaseEstimator], Tuple[Dict, DataFrame]]</code> <p>A dict with the best hyperparameters setting, among those provided by the user. The keys are the hyperparameters names, while the dicts' values are the hyperparameters values. A Pandas DataFrame holding the score for each hyperparameter combination and fold. For the 'cross validation' method the average across all folds for each hyperparameter combination is included too. If refit=True, the retrained model is also returned.</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.holdout_validate","title":"holdout_validate","text":"<pre><code>holdout_validate(level=None, validation_size=0.2, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage=None, sparse_threshold=0.01, enhancement=0, **model_params)\n</code></pre> <p>A method for hold-out validation on the coreset tree. The validation set is always the last part of the dataset. This function is only applicable in case the coreset tree was optimized_for <code>training</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>validation_size</code> <code>float</code> <p>float, optional. The percentage of the dataset that will be used for validating the model.</p> <code>0.2</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : the training and validation time is displayed;     &gt;=2 : the validation score is also displayed;     &gt;=3 : starting time of the computation is also displayed.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>None</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>enhancement</code> <code>int</code> <p>int (0-3), optional, default 0 (no enhancement). Enhance the default decision tree based training. Can improve the strength of the model, but will increase the training run time.</p> <code>0</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>The validation score. If return_model=True, the trained model is also returned.</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.is_dirty","title":"is_dirty","text":"<pre><code>is_dirty()\n</code></pre> <p>Returns:</p> Type Description <code>bool</code> <p>Indicates whether the coreset tree has nodes marked as dirty, meaning they were affected by any of the methods: remove_samples, update_targets, update_features or filter_out_samples, when they were called with force_do_nothing.</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(dir_path, name=None, *, data_manager=None, load_buffer=True, working_directory=None)\n</code></pre> <p>Restore a service object from a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>str, path. Local directory where service data is stored.</p> required <code>name</code> <code>str</code> <p>string, optional, default service class name (lower case). The name prefix of the subdirectory to load. When several subdirectories having the same name prefix are found, the last one, ordered by name, is selected. For example when saving with override=False, the chosen subdirectory is the last saved.</p> <code>None</code> <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. When specified, input data manger will be used instead of restoring it from the saved configuration.</p> <code>None</code> <code>load_buffer</code> <code>bool</code> <p>boolean, optional, default True. If set, load saved buffer (a partial node of the tree) from disk and add it to the tree.</p> <code>True</code> <code>working_directory</code> <code>Union[str, PathLike]</code> <p>str, path, optional, default use working_directory from saved configuration. Local directory where intermediate data is stored.</p> <code>None</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>CoresetTreeService object</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.partial_build","title":"partial_build","text":"<pre><code>partial_build(X, y, indices=None, props=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree from parameters X, y, indices and props (properties). Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> required <code>y</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like. An array or an iterator of targets.</p> required <code>indices</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator with indices of X.</p> <code>None</code> <code>props</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.partial_build_from_df","title":"partial_build_from_df","text":"<pre><code>partial_build_from_df(datasets, target_datasets=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree based on the pandas DataFrame iterator. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator. Data includes features, may include targets and may include indices.</p> required <code>target_datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator, optional. Use when data is split to features and target. Should include only one column.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional, default previous used chunk_size. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.partial_build_from_file","title":"partial_build_from_file","text":"<pre><code>partial_build_from_file(file_path, target_file_path=None, *, reader_f=pd.read_csv, reader_kwargs=None, reader_chunk_size_param_name=None, chunk_size=None, chunk_by=None, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree based on data taken from local storage. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories. Path(s) to the place where data is stored. Data includes features, may include targets and may include indices.</p> required <code>target_file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories, optional. Use when files are split to features and target. Each file should include only one column.</p> <code>None</code> <code>reader_f</code> <code>Callable</code> <p>pandas like read method, optional, default pandas read_csv. For example, to read excel files use pandas read_excel.</p> <code>read_csv</code> <code>reader_kwargs</code> <code>dict</code> <p>dict, optional. Keyword arguments used when calling reader_f method.</p> <code>None</code> <code>reader_chunk_size_param_name</code> <code>str</code> <p>str, optional. reader_f input parameter name for reading file in chunks. When not provided we'll try to figure it out our self. Based on the data, we decide on the optimal chunk size to read and use this parameter as input when calling reader_f. Use \"ignore\" to skip the automatic chunk reading logic.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional, default previous used chunk_size. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.plot","title":"plot","text":"<pre><code>plot(dir_path=None, selected_trees=None)\n</code></pre> <p>Produce a tree graph plot and save figure as a local png file.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike. Path to save the plot figure in; if not provided, or if isn't valid/doesn't exist, the figure will be saved in the current directory (from which this method is called).</p> <code>None</code> <code>selected_trees</code> <code>dict</code> <p>dict, optional. A dictionary containing the names of the image file(s) to be generated.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Image file path</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.predict","title":"predict","text":"<pre><code>predict(X, sparse_output=False, copy=False)\n</code></pre> <p>Run prediction on the trained model. This function is only applicable in case the coreset tree was optimized_for 'training' and in case fit() or grid_search(refit=True) where called before. The function automatically preprocesses the data according to the preprocessing_stage used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>An array of features.</p> required <code>sparse_output</code> <code>bool</code> <p>boolean, optional, default False. When set to True, the function will create a sparse matrix after preprocessing and pass it to the predict function.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <p>Model prediction results.</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X, sparse_output=False, copy=False)\n</code></pre> <p>Run prediction on the trained model. This function is only applicable in case the coreset tree was optimized_for 'training' and in case fit() or grid_search(refit=True) where called before. The function automatically preprocesses the data according to the preprocessing_stage used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>An array of features.</p> required <code>sparse_output</code> <code>bool</code> <p>boolean, optional, default False. When set to True, the function will create a sparse matrix after preprocessing and pass it to the predict_proba function.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <p>Returns the probability of the sample for each class in the model.</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.print","title":"print","text":"<pre><code>print(selected_tree=None)\n</code></pre> <p>Print the tree's string representation.</p> <p>Parameters:</p> Name Type Description Default <code>selected_tree</code> <code>str</code> <p>string, optional. Which tree to print. Defaults to printing all.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.remove_samples","title":"remove_samples","text":"<pre><code>remove_samples(indices, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Remove samples from the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be removed from the coreset tree.</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.save","title":"save","text":"<pre><code>save(dir_path=None, name=None, save_buffer=True, override=False, allow_pickle=True)\n</code></pre> <p>Save service configuration and relevant data to a local directory. Use this method when the service needs to be restored.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike, optional, default self.working_directory. A local directory for saving service's files.</p> <code>None</code> <code>name</code> <code>str</code> <p>string, optional, default service class name (lower case). Name of the subdirectory where the data will be stored.</p> <code>None</code> <code>save_buffer</code> <code>bool</code> <p>boolean, default True. Save also the data in the buffer (a partial node of the tree) along with the rest of the saved data.</p> <code>True</code> <code>override</code> <code>bool</code> <p>bool, optional, default False. False: add a timestamp suffix so each save won\u2019t override the previous ones. True: The existing subdirectory with the provided name is overridden.</p> <code>False</code> <code>allow_pickle</code> <code>bool</code> <p>bool, optional, default True. True: Saves the Coreset tree in pickle format (much faster). False: Saves the Coreset tree in JSON format.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>Save directory path.</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.save_coreset","title":"save_coreset","text":"<pre><code>save_coreset(file_path, level=0, preprocessing_stage='user', with_index=True)\n</code></pre> <p>Get the coreset from the tree and save it to a file. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike. Local file path to store the coreset.</p> required <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>with_index</code> <code>bool</code> <p>boolean, optional, default False. Relevant only when preprocessing_stage=<code>auto</code>. Should the returned data include the index column.</p> <code>True</code>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.seq_dependent_validate","title":"seq_dependent_validate","text":"<pre><code>seq_dependent_validate(level=None, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, enhancement=0, **model_params)\n</code></pre> <p>The method allows to train and validate on a subset of the Coreset tree, according to the <code>seq_column</code> defined in the <code>DataParams</code> structure passed to the init. This function is only applicable in case the coreset tree was optimized_for <code>training</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree from which the search for the best matching nodes starts. Nodes closer to the leaf level than the specified level, may be selected to better match the provided seq parameters.If None, the search starts from level 0, the head of the tree. If None, the best level will be selected.</p> <code>None</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>enhancement</code> <code>int</code> <p>int (0-3), optional, default 0 (no enhancement). Enhance the default decision tree based training. Can improve the strength of the model, but will increase the training run time.</p> <code>0</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>The validation score. If return_model=True, the trained model is also returned.</p>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.set_model_cls","title":"set_model_cls","text":"<pre><code>set_model_cls(model_cls)\n</code></pre> <p>Set the model class used to train the model on the coreset, in case a specific model instance wasn't passed to fit or the validation methods.</p> <p>Parameters:</p> Name Type Description Default <code>model_cls</code> <code>Any</code> <p>A Scikit-learn compatible model class.</p> required"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.set_seen_indication","title":"set_seen_indication","text":"<pre><code>set_seen_indication(seen_flag=True, indices=None)\n</code></pre> <p>Set samples as 'seen' or 'unseen'. Not providing an indices list defaults to setting the flag on all samples. This function is only applicable in case the coreset tree was optimized_for 'cleaning'.</p> <p>Parameters:</p> Name Type Description Default <code>seen_flag</code> <code>bool</code> <p>bool, optional, default True. Set 'seen' or 'unseen' flag</p> <code>True</code> <code>indices</code> <code>Iterable</code> <p>array like, optional. Set flag only for the provided list of indices. Defaults to all indices.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.update_dirty","title":"update_dirty","text":"<pre><code>update_dirty(force_resample_all=None, force_sensitivity_recalc=None)\n</code></pre> <p>Calculate the sensitivity and resample the nodes that were marked as dirty, meaning they were affected by any of the methods: remove_samples, update_targets, update_features or filter_out_samples, when they were called with force_do_nothing.</p> <p>Parameters:</p> Name Type Description Default <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.update_features","title":"update_features","text":"<pre><code>update_features(indices, X, feature_names=None, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Update the features for selected samples on the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be updated.</p> required <code>X</code> <code>Iterable</code> <p>array-like. An array of features. Should have the same length as indices.</p> required <code>feature_names</code> <code>Iterable[str]</code> <p>If the quantity of features in X is not equal to the quantity of features in the original coreset, this param should contain list of names of passed features.</p> <code>None</code> <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/dtr/#services.coreset_tree.dtr.CoresetTreeServiceDTR.update_targets","title":"update_targets","text":"<pre><code>update_targets(indices, y, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Update the targets for selected samples on the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be updated.</p> required <code>y</code> <code>Iterable</code> <p>array-like. An array of classes/labels. Should have the same length as indices.</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/kmeans/","title":"CoresetTreeServiceKMeans","text":""},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans","title":"CoresetTreeServiceKMeans","text":"<pre><code>CoresetTreeServiceKMeans(*, data_manager=None, data_params=None, n_instances=None, max_memory_gb=None, optimized_for, chunk_size=None, chunk_by=None, k=8, coreset_size=None, coreset_params=None, working_directory=None, cache_dir=None, node_train_function=None, node_train_function_params=None, node_metadata_func=None, chunk_sample_ratio=None, model_cls=None)\n</code></pre> <p>             Bases: <code>CoresetTreeServiceUnsupervisedMixin</code>, <code>CoresetTreeService</code></p> <p>Subclass of CoresetTreeService for KMeans. A service class for creating a coreset tree and working with it. optimized_for is a required parameter defining the main usage of the service: 'training', 'cleaning' or both, optimized_for=['training', 'cleaning']. The service will decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset, based on the triplet: n_instances, max_memory_gb and the 'number of features' (deduced from the dataset). The chunk_size and coreset_size will be deduced based on the above triplet too. In case chunk_size and coreset_size are provided, they will override all above mentioned parameters (less recommended).</p> <p>When fitting KMeans on the Coreset, it is highly recommended to use the built-in fit function of the CoresetTreeServiceKMeans class. Sklearn uses by default k-means++ as its initialization method. While sklearn's KMeans implementation supports the receipt of sample_weight, the kmeans_plusplus implementation does not. When building the Coreset, samples are selected and weights are assigned to them, therefore, not using these weights will significantly degrade the quality of the results. The fit implementation of the CoresetTreeServiceKMeans solves this problem, by extending kmeans_plusplus to receive sample_weight.</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. The class used to interact with the provided data and store it locally. By default, only the sampled data is stored in HDF5 files format.</p> <code>None</code> <code>data_params</code> <code>Union[DataParams, dict]</code> <p>DataParams, optional. Data preprocessing information.</p> <code>None</code> <code>n_instances</code> <code>int</code> <p>int. The total number of instances that are going to be processed (can be an estimation). This parameter is required and the only one from the above mentioned quadruplet, which isn't deduced from the data.</p> <code>None</code> <code>max_memory_gb</code> <code>int</code> <p>int, optional. The maximum memory in GB that should be used. When not provided, the server's total memory is used. In any case only 80% of the provided memory or the server's total memory is considered.</p> <code>None</code> <code>optimized_for</code> <code>Union[list, str]</code> <p>str or list Either 'training', 'cleaning' or or both ['training', 'cleaning']. The main usage of the service.</p> required <code>k</code> <p>int, default=8. Only relevant when tree is optimized_for cleaning. The number of clusters to form as well as the number of centroids to generate.</p> <code>8</code> <code>chunk_size</code> <code>Union[dict, int]</code> <p>int, optional. The number of instances to be used when creating a coreset node in the tree. When defined, it will override the parameters of optimized_for, n_instances, n_classes and max_memory_gb. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>coreset_size</code> <code>Union[int, float, dict]</code> <p>int or float, optional. Represents the coreset size of each node in the coreset tree. If provided as a float, it represents the ratio between each chunk and the resulting coreset.In any case the coreset_size is limited to 60% of the chunk_size. The coreset is constructed by sampling data instances from the dataset based on their calculated importance. Since each instance may be sampled more than once, in practice, the actual size of the coreset is mostly smaller than coreset_size.</p> <code>None</code> <code>coreset_params</code> <code>Union[CoresetParams, dict]</code> <p>CoresetParams or dict, optional. Coreset algorithm specific parameters.</p> <code>None</code> <code>node_train_function</code> <code>Callable[[ndarray, ndarray, ndarray], Any]</code> <p>Callable, optional. method for training model at tree node level.</p> <code>None</code> <code>node_train_function_params</code> <code>dict</code> <p>dict, optional. kwargs to be used when calling node_train_function.</p> <code>None</code> <code>node_metadata_func</code> <code>Callable[[Tuple[ndarray], ndarray, Union[list, None]], Union[list, dict, None]]</code> <p>callable, optional. A method for storing user meta data on each node.</p> <code>None</code> <code>working_directory</code> <code>Union[str, PathLike]</code> <p>str, path, optional. Local directory where intermediate data is stored.</p> <code>None</code> <code>cache_dir</code> <code>Union[str, PathLike]</code> <p>str, path, optional. For internal use when loading a saved service.</p> <code>None</code> <code>chunk_sample_ratio</code> <code>float</code> <p>float, optional. Indicates the size of the sample that will be taken and saved from each chunk on top of the Coreset for the validation methods. The values are from the range [0,1]. For example, chunk_sample_ratio=0.5, means that 50% of the data instances from each chunk will be saved.</p> <code>None</code> <code>model_cls</code> <code>Any</code> <p>A Scikit-learn compatible model class, optional. The model class used to train the model on the coreset, in case a specific model instance wasn't passed to fit or the validation methods. The default model class is sklearn's KMeans, with our extension to kmeans_plusplus to support sample_weight.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.auto_preprocessing","title":"auto_preprocessing","text":"<pre><code>auto_preprocessing(X=None, sparse_output=False, copy=False)\n</code></pre> <p>Apply auto-preprocessing on the provided (test) data, similarly to the way it is done by the fit or get_coreset methods. Preprocessing includes ohe-hot encoding and handling missing values depends.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features.</p> <code>None</code> <code>sparse_output</code> <code>bool</code> <p>boolean, default False. When set to True, the function will create a sparse matrix after preprocessing.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the following keys: data: A numpy array of the preprocessed data. features: A list of feature names corresponding to the data. sparse: A boolean indicating if the output is in sparse format.</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.build","title":"build","text":"<pre><code>build(X, y=None, indices=None, props=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree from the parameters X, y, indices and props (properties). build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> required <code>y</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of targets. The target will be ignored when the Coreset is built.</p> <code>None</code> <code>indices</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator with indices of X.</p> <code>None</code> <code>props</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.build_from_df","title":"build_from_df","text":"<pre><code>build_from_df(datasets, target_datasets=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree from pandas DataFrame(s). build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator. Data includes features, may include labels and may include indices.</p> required <code>target_datasets</code> <code>Union[Iterator[Union[DataFrame, Series]], DataFrame, Series]</code> <p>pandas DataFrame or a DataFrame iterator, optional. Use when data is split to features and target. Should include only one column.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.build_from_file","title":"build_from_file","text":"<pre><code>build_from_file(file_path, target_file_path=None, *, reader_f=pd.read_csv, reader_kwargs=None, reader_chunk_size_param_name=None, chunk_size=None, chunk_by=None, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree based on data taken from local storage. build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories. Path(s) to the place where data is stored. Data includes features, may include targets and may include indices.</p> required <code>target_file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories, optional. Use when the dataset files are split to features and target. Each file should include only one column.</p> <code>None</code> <code>reader_f</code> <code>Callable</code> <p>pandas like read method, optional, default pandas read_csv. For example, to read excel files use pandas read_excel.</p> <code>read_csv</code> <code>reader_kwargs</code> <code>dict</code> <p>dict, optional. Keyword arguments used when calling reader_f method.</p> <code>None</code> <code>reader_chunk_size_param_name</code> <code>str</code> <p>str, optional. reader_f input parameter name for reading file in chunks. When not provided we'll try to figure it out our self. Based on the data, we decide on the optimal chunk size to read and use this parameter as input when calling reader_f. Use \"ignore\" to skip the automatic chunk reading logic.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.cross_validate","title":"cross_validate","text":"<pre><code>cross_validate(level=None, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>Method for cross-validation on the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of folds and hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : the score is also displayed;</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>A list of scores, one for each fold. If return_model=True, a list of trained models is also returned (one model for each fold).</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.filter_out_samples","title":"filter_out_samples","text":"<pre><code>filter_out_samples(filter_function, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Remove samples from the coreset tree, based on the provided filter function. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>filter_function</code> <code>Callable[[Iterable, Iterable, Union[Iterable, None], Union[Iterable, None]], Iterable[Any]]</code> <p>function, optional. A function that returns a list of indices to be removed from the tree. The function should accept 4 parameters as input: indices, X, y, props and return a list(iterator) of indices to be removed from the coreset tree. For example, in order to remove all instances with a target equal to 6, use the following function: filter_function = lambda indices, X, y, props : indices[y = 6].</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.fit","title":"fit","text":"<pre><code>fit(level=0, seq_from=None, seq_to=None, model=None, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>Fit a model on the coreset tree. This model will be used when predict and predict_proba are called. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>string/datetime, optional The starting sequence of the training set.</p> <code>None</code> <code>seq_to</code> <code>Any</code> <p>string/datetime, optional The ending sequence of the training set.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>Model hyperparameters kwargs. Input when instantiating default model class.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Fitted estimator.</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.get_cleaning_samples","title":"get_cleaning_samples","text":"<pre><code>get_cleaning_samples(size=None, ignore_indices=None, select_from_indices=None, select_from_function=None, ignore_seen_samples=True)\n</code></pre> <p>Returns indices of samples in descending order of importance. Useful for identifying mislabeled instances and other anomalies in the data. size must be provided. Function must be called after build. This function is only applicable in case the coreset tree was optimized_for 'cleaning'. This function is not for retrieving the coreset (use get_coreset in this case).</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>required, optional. Number of samples to return.</p> <code>None</code> <code>ignore_indices</code> <code>Iterable</code> <p>array-like, optional. An array of indices to ignore when selecting cleaning samples.</p> <code>None</code> <code>select_from_indices</code> <code>Iterable</code> <p>array-like, optional.  An array of indices to consider when selecting cleaning samples.</p> <code>None</code> <code>select_from_function</code> <code>Callable[[Iterable, Iterable, Union[Iterable, None], Union[Iterable, None]], Iterable[Any]]</code> <p>function, optional.  Pass a function in order to limit the selection of the cleaning samples accordingly.  The function should accept 4 parameters as input: indices, X, y, props.  and return a list(iterator) of the desired indices.</p> <code>None</code> <code>ignore_seen_samples</code> <code>bool</code> <p>bool, optional, default True.  Exclude already seen samples and set the seen flag on any indices returned by the function.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Union[ValueError, dict]</code> <p>idx: array-like[int].     Cleaning samples indices. X: array-like[int].     X array. y: array-like[int].     y array. importance: array-like[float].     The importance property. Instances that receive a high Importance in the Coreset computation,     require attention as they usually indicate a labeling error,     anomaly, out-of-distribution problem or other data-related issue.</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.get_coreset","title":"get_coreset","text":"<pre><code>get_coreset(level=0, preprocessing_stage='user', sparse_threshold=0.01, as_df=False, with_index=False, seq_from=None, seq_to=None)\n</code></pre> <p>Get tree's coreset data in one of the preprocessing_stage(s) in the data preprocessing workflow. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Returns the features (X) as a sparse matrix if the data density after preprocessing is below sparse_threshold, otherwise, will return the data as an array (Applicable only for preprocessing_stage=<code>auto</code>).</p> <code>0.01</code> <code>as_df</code> <code>bool</code> <p>boolean, optional, default False. When True, returns the X as a pandas DataFrame.</p> <code>False</code> <code>with_index</code> <code>bool</code> <p>boolean, optional, default False. Relevant only when preprocessing_stage=<code>auto</code>. Should the returned data include the index column.</p> <code>False</code> <code>seq_from</code> <code>Any</code> <p>string or datetime, optional, default None. The start sequence to filter samples by.</p> <code>None</code> <code>seq_to</code> <code>Any</code> <p>string or datetime, optional, default None. The end sequence to filter samples by.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary representing the Coreset: ind: A numpy array of indices. X: A numpy array of the feature matrix. y: A numpy array of the target values. w: A numpy array of sample weights. n_represents: The number of instances represented by the coreset. features_out: A list of the output features, if preprocessing_stage=auto, otherwise None. props: A numpy array of properties, or None if not available.</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.get_coreset_size","title":"get_coreset_size","text":"<pre><code>get_coreset_size(level=0, seq_from=None, seq_to=None)\n</code></pre> <p>Returns the size of the tree's coreset data. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>string or datetime, optional, default None. The start sequence to filter samples by.</p> <code>None</code> <code>seq_to</code> <code>Union[str, datetime]</code> <p>string or datetime, optional, default None. The end sequence to filter samples by.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>coreset size</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.get_hyperparameter_tuning_data","title":"get_hyperparameter_tuning_data","text":"<pre><code>get_hyperparameter_tuning_data(level=None, validation_method='cross validation', preprocessing_stage='user', sparse_threshold=0.01, validation_size=0.2, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, as_df=True)\n</code></pre> <p>A method for retrieving the data for hyperparameter tuning with cross validation, using the coreset tree. The returned data can be used with Scikit-learn\u2019s GridSearchCV, with skopt\u2019s BayesSearchCV and with any other hyperparameter tuning method that can accept a fold iterator object. Note: When using this method with Scikit-learn's <code>GridSearchCV</code> and similar methods, the <code>refit</code> parameter must be set to <code>False</code>. This is because the returned dataset (X, y and w) includes both training and validation data due to the use of a splitter. The returned dataset (X, y and w) is the concatenation of training data for all folds followed by validation data for all folds. By default, GridSearchCV refits the estimator on the entire dataset, not just the training portion, and this behavior cannot be modified and is incorrect. In this case, refit should be handled manually after the cross-validation process, by calling get_coreset with the same parameters that were passed to this function to retrieve the data and then fitting on the returned data using the best hyperparameters found in GridSearchCV. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>None</code> <code>validation_method</code> <code>str</code> <p>str, optional. Indicates which validation method will be used. The possible values are 'cross validation', 'hold-out validation' and 'seq-dependent validation'.</p> <code>'cross validation'</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Returns the features (X) as a sparse matrix if the data density after preprocessing is below sparse_threshold, otherwise, will return the data as an array (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>validation_size</code> <code>float</code> <p>float, optional, default 0.2. The size of the validation set, as a percentage of the training set size for hold-out validation.</p> <code>0.2</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>as_df</code> <code>bool</code> <p>boolean, optional, default False. When True, returns the X as a pandas DataFrame.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, FoldIterator, Any]]</code> <p>A dictionary with the following keys: ind: The indices of the data. X: The data. y: The labels. w: The weights. splitter: The fold iterator. model_params: The model parameters.</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.get_max_level","title":"get_max_level","text":"<pre><code>get_max_level()\n</code></pre> <p>Return the maximal level of the coreset tree. Level 0 is the head of the tree. Level 1 is the level below the head of the tree, etc.</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.grid_search","title":"grid_search","text":"<pre><code>grid_search(param_grid, level=None, validation_method='cross validation', model=None, scoring=None, refit=True, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, error_score=np.nan, validation_size=0.2, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, n_jobs=None)\n</code></pre> <p>A method for performing hyperparameter selection by grid search, using the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>param_grid</code> <code>Union[Dict[str, List], List[Dict[str, List]]]</code> <p>dict or list of dicts. Dictionary with parameters names (str) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.</p> required <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>validation_method</code> <code>str</code> <p>str, optional. Indicates which validation method will be used. The possible values are 'cross validation', 'hold-out validation' and 'seq-dependent validation'. If 'cross validation' is selected, the process involves progressing through folds. We first train and validate all hyperparameter combinations for each fold, before moving on to the subsequent folds.</p> <code>'cross validation'</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. The model class needs to implement the usual scikit-learn interface.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>refit</code> <code>bool</code> <p>bool, optional. If True, retrain the model on the whole coreset using the best found hyperparameters, and return the model. This model will be used when predict and predict_proba are called.</p> <code>True</code> <code>verbose</code> <code>int</code> <p>int, optional Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of folds and hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each fold and hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>error_score</code> <code>Union[str, float, int]</code> <p>\"raise\" or numeric, optional. Value to assign to the score if an error occurs in model training. If set to \"raise\", the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.</p> <code>nan</code> <code>validation_size</code> <code>float</code> <p>float, optional, default 0.2. The size of the validation set, as a percentage of the training set size for hold-out validation.</p> <code>0.2</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>int, optional. Default: number of CPUs. Number of jobs to run in parallel during grid search.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Dict, DataFrame, BaseEstimator], Tuple[Dict, DataFrame]]</code> <p>A dict with the best hyperparameters setting, among those provided by the user. The keys are the hyperparameters names, while the dicts' values are the hyperparameters values. A Pandas DataFrame holding the score for each hyperparameter combination and fold. For the 'cross validation' method the average across all folds for each hyperparameter combination is included too. If refit=True, the retrained model is also returned.</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.holdout_validate","title":"holdout_validate","text":"<pre><code>holdout_validate(level=None, validation_size=0.2, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>A method for hold-out validation on the coreset tree. The validation set is always the last part of the dataset. This function is only applicable in case the coreset tree was optimized_for <code>training</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>validation_size</code> <code>float</code> <p>float, optional. The percentage of the dataset that will be used for validating the model.</p> <code>0.2</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>The validation score. If return_model=True, the trained model is also returned.</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.is_dirty","title":"is_dirty","text":"<pre><code>is_dirty()\n</code></pre> <p>Returns:</p> Type Description <code>bool</code> <p>Indicates whether the coreset tree has nodes marked as dirty, meaning they were affected by any of the methods: remove_samples, update_targets, update_features or filter_out_samples, when they were called with force_do_nothing.</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(dir_path, name=None, *, data_manager=None, load_buffer=True, working_directory=None)\n</code></pre> <p>Restore a service object from a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>str, path. Local directory where service data is stored.</p> required <code>name</code> <code>str</code> <p>string, optional, default service class name (lower case). The name prefix of the subdirectory to load. When several subdirectories having the same name prefix are found, the last one, ordered by name, is selected. For example when saving with override=False, the chosen subdirectory is the last saved.</p> <code>None</code> <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. When specified, input data manger will be used instead of restoring it from the saved configuration.</p> <code>None</code> <code>load_buffer</code> <code>bool</code> <p>boolean, optional, default True. If set, load saved buffer (a partial node of the tree) from disk and add it to the tree.</p> <code>True</code> <code>working_directory</code> <code>Union[str, PathLike]</code> <p>str, path, optional, default use working_directory from saved configuration. Local directory where intermediate data is stored.</p> <code>None</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>CoresetTreeService object</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.partial_build","title":"partial_build","text":"<pre><code>partial_build(X, y=None, indices=None, props=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree from parameters X, y, indices and props (properties). Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> required <code>y</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of targets. The target will be ignored when the Coreset is built.</p> <code>None</code> <code>indices</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator with indices of X.</p> <code>None</code> <code>props</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.partial_build_from_df","title":"partial_build_from_df","text":"<pre><code>partial_build_from_df(datasets, target_datasets=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree based on the pandas DataFrame iterator. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator. Data includes features, may include targets and may include indices.</p> required <code>target_datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator, optional. Use when data is split to features and target. Should include only one column.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional, default previous used chunk_size. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:     self</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.partial_build_from_file","title":"partial_build_from_file","text":"<pre><code>partial_build_from_file(file_path, target_file_path=None, *, reader_f=pd.read_csv, reader_kwargs=None, reader_chunk_size_param_name=None, chunk_size=None, chunk_by=None, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree based on data taken from local storage. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories. Path(s) to the place where data is stored. Data includes features, may include targets and may include indices.</p> required <code>target_file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories, optional. Use when files are split to features and target. Each file should include only one column.</p> <code>None</code> <code>reader_f</code> <code>Callable</code> <p>pandas like read method, optional, default pandas read_csv. For example, to read excel files use pandas read_excel.</p> <code>read_csv</code> <code>reader_kwargs</code> <code>dict</code> <p>dict, optional. Keyword arguments used when calling reader_f method.</p> <code>None</code> <code>reader_chunk_size_param_name</code> <code>str</code> <p>str, optional. reader_f input parameter name for reading file in chunks. When not provided we'll try to figure it out our self. Based on the data, we decide on the optimal chunk size to read and use this parameter as input when calling reader_f. Use \"ignore\" to skip the automatic chunk reading logic.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional, default previous used chunk_size. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.plot","title":"plot","text":"<pre><code>plot(dir_path=None, selected_trees=None)\n</code></pre> <p>Produce a tree graph plot and save figure as a local png file.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike. Path to save the plot figure in; if not provided, or if isn't valid/doesn't exist, the figure will be saved in the current directory (from which this method is called).</p> <code>None</code> <code>selected_trees</code> <code>dict</code> <p>dict, optional. A dictionary containing the names of the image file(s) to be generated.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Image file path</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.predict","title":"predict","text":"<pre><code>predict(X, sparse_output=False, copy=False)\n</code></pre> <p>Run prediction on the trained model. This function is only applicable in case the coreset tree was optimized_for 'training' and in case fit() or grid_search(refit=True) where called before. The function automatically preprocesses the data according to the preprocessing_stage used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>An array of features.</p> required <code>sparse_output</code> <code>bool</code> <p>boolean, optional, default False. When set to True, the function will create a sparse matrix after preprocessing and pass it to the predict function.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <p>Model prediction results.</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X, sparse_output=False, copy=False)\n</code></pre> <p>Run prediction on the trained model. This function is only applicable in case the coreset tree was optimized_for 'training' and in case fit() or grid_search(refit=True) where called before. The function automatically preprocesses the data according to the preprocessing_stage used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>An array of features.</p> required <code>sparse_output</code> <code>bool</code> <p>boolean, optional, default False. When set to True, the function will create a sparse matrix after preprocessing and pass it to the predict_proba function.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <p>Returns the probability of the sample for each class in the model.</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.print","title":"print","text":"<pre><code>print(selected_tree=None)\n</code></pre> <p>Print the tree's string representation.</p> <p>Parameters:</p> Name Type Description Default <code>selected_tree</code> <code>str</code> <p>string, optional. Which tree to print. Defaults to printing all.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.remove_samples","title":"remove_samples","text":"<pre><code>remove_samples(indices, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Remove samples from the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be removed from the coreset tree.</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.save","title":"save","text":"<pre><code>save(dir_path=None, name=None, save_buffer=True, override=False, allow_pickle=True)\n</code></pre> <p>Save service configuration and relevant data to a local directory. Use this method when the service needs to be restored.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike, optional, default self.working_directory. A local directory for saving service's files.</p> <code>None</code> <code>name</code> <code>str</code> <p>string, optional, default service class name (lower case). Name of the subdirectory where the data will be stored.</p> <code>None</code> <code>save_buffer</code> <code>bool</code> <p>boolean, default True. Save also the data in the buffer (a partial node of the tree) along with the rest of the saved data.</p> <code>True</code> <code>override</code> <code>bool</code> <p>bool, optional, default False. False: add a timestamp suffix so each save won\u2019t override the previous ones. True: The existing subdirectory with the provided name is overridden.</p> <code>False</code> <code>allow_pickle</code> <code>bool</code> <p>bool, optional, default True. True: Saves the Coreset tree in pickle format (much faster). False: Saves the Coreset tree in JSON format.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>Save directory path.</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.save_coreset","title":"save_coreset","text":"<pre><code>save_coreset(file_path, level=0, preprocessing_stage='user', with_index=True)\n</code></pre> <p>Get the coreset from the tree and save it to a file. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike. Local file path to store the coreset.</p> required <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>with_index</code> <code>bool</code> <p>boolean, optional, default False. Relevant only when preprocessing_stage=<code>auto</code>. Should the returned data include the index column.</p> <code>True</code>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.seq_dependent_validate","title":"seq_dependent_validate","text":"<pre><code>seq_dependent_validate(level=None, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>The method allows to train and validate on a subset of the Coreset tree, according to the <code>seq_column</code> defined in the <code>DataParams</code> structure passed to the init. This function is only applicable in case the coreset tree was optimized_for <code>training</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree from which the search for the best matching nodes starts. Nodes closer to the leaf level than the specified level, may be selected to better match the provided seq parameters.If None, the search starts from level 0, the head of the tree. If None, the best level will be selected.</p> <code>None</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>The validation score. If return_model=True, the trained model is also returned.</p>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.set_model_cls","title":"set_model_cls","text":"<pre><code>set_model_cls(model_cls)\n</code></pre> <p>Set the model class used to train the model on the coreset, in case a specific model instance wasn't passed to fit or the validation methods.</p> <p>Parameters:</p> Name Type Description Default <code>model_cls</code> <code>Any</code> <p>A Scikit-learn compatible model class.</p> required"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.set_seen_indication","title":"set_seen_indication","text":"<pre><code>set_seen_indication(seen_flag=True, indices=None)\n</code></pre> <p>Set samples as 'seen' or 'unseen'. Not providing an indices list defaults to setting the flag on all samples. This function is only applicable in case the coreset tree was optimized_for 'cleaning'.</p> <p>Parameters:</p> Name Type Description Default <code>seen_flag</code> <code>bool</code> <p>bool, optional, default True. Set 'seen' or 'unseen' flag</p> <code>True</code> <code>indices</code> <code>Iterable</code> <p>array like, optional. Set flag only for the provided list of indices. Defaults to all indices.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.update_dirty","title":"update_dirty","text":"<pre><code>update_dirty(force_resample_all=None, force_sensitivity_recalc=None)\n</code></pre> <p>Calculate the sensitivity and resample the nodes that were marked as dirty, meaning they were affected by any of the methods: remove_samples, update_targets, update_features or filter_out_samples, when they were called with force_do_nothing.</p> <p>Parameters:</p> Name Type Description Default <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.update_features","title":"update_features","text":"<pre><code>update_features(indices, X, feature_names=None, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Update the features for selected samples on the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be updated.</p> required <code>X</code> <code>Iterable</code> <p>array-like. An array of features. Should have the same length as indices.</p> required <code>feature_names</code> <code>Iterable[str]</code> <p>If the quantity of features in X is not equal to the quantity of features in the original coreset, this param should contain list of names of passed features.</p> <code>None</code> <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/kmeans/#services.coreset_tree.kmeans.CoresetTreeServiceKMeans.update_targets","title":"update_targets","text":"<pre><code>update_targets(indices, y, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Update the targets for selected samples on the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be updated.</p> required <code>y</code> <code>Iterable</code> <p>array-like. An array of classes/labels. Should have the same length as indices.</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/lg/","title":"CoresetTreeServiceLG","text":""},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG","title":"CoresetTreeServiceLG","text":"<pre><code>CoresetTreeServiceLG(*, data_manager=None, data_params=None, n_instances=None, max_memory_gb=None, n_classes=None, optimized_for, chunk_size=None, chunk_by=None, coreset_size=None, coreset_params=None, sample_all=None, working_directory=None, cache_dir=None, node_train_function=None, node_train_function_params=None, node_metadata_func=None, chunk_sample_ratio=None, model_cls=None, build_w_estimation=False)\n</code></pre> <p>             Bases: <code>EstimationMixin</code>, <code>CoresetTreeServiceClassifierMixin</code>, <code>CoresetTreeServiceSupervisedMixin</code>, <code>CoresetTreeService</code></p> <p>Subclass of CoresetTreeService for Logistic Regression. A service class for creating a coreset tree and working with it. optimized_for is a required parameter defining the main usage of the service: 'training', 'cleaning' or both, optimized_for=['training', 'cleaning']. The service will decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset, based on the quadruplet: n_instances, n_classes, max_memory_gb and the 'number of features' (deduced from the dataset). The chunk_size and coreset_size will be deduced based on the above quadruplet too. In case chunk_size and coreset_size are provided, they will override all above mentioned parameters (less recommended).</p> <p>If you intend passing class_weight to your classifier, it is recommended to pass it as a parameter to the class in coreset_params (see example below), so the coreset can be built while taking into account the class_weight. You can continue passing class_weight to your classifier, while retrieving the coreset using the get_coreset method with the parameter inverse_class_weight set to True (default). If you wish to stop passing class_weight to the classifier, retrieve the coreset using the get_coreset method with the parameter inverse_class_weight set to False.</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. The class used to interact with the provided data and store it locally. By default, only the sampled data is stored in HDF5 files format.</p> <code>None</code> <code>data_params</code> <code>Union[DataParams, dict]</code> <p>DataParams, optional. Data preprocessing information.</p> <code>None</code> <code>n_instances</code> <code>int</code> <p>int. The total number of instances that are going to be processed (can be an estimation). This parameter is required and the only one from the above mentioned quadruplet, which isn't deduced from the data.</p> <code>None</code> <code>n_classes</code> <code>int</code> <p>int. The total number of classes (labels). When not provided, will be deduced from the provided data. When multiple files are provided n_classes will be deduced based on the first file only.</p> <code>None</code> <code>max_memory_gb</code> <code>int</code> <p>int, optional. The maximum memory in GB that should be used. When not provided, the server's total memory is used. In any case only 80% of the provided memory or the server's total memory is considered.</p> <code>None</code> <code>optimized_for</code> <code>Union[list, str]</code> <p>str or list Either 'training', 'cleaning' or or both ['training', 'cleaning']. The main usage of the service.</p> required <code>chunk_size</code> <code>Union[dict, int]</code> <p>int, optional. The number of instances to be used when creating a coreset node in the tree. When defined, it will override the parameters of optimized_for, n_instances, n_classes and max_memory_gb. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>coreset_size</code> <code>Union[int, float, dict]</code> <p>int or float, optional. Represents the coreset size of each node in the coreset tree. If provided as a float, it represents the ratio between each chunk and the resulting coreset.In any case the coreset_size is limited to 60% of the chunk_size. The coreset is constructed by sampling data instances from the dataset based on their calculated importance. Since each instance may be sampled more than once, in practice, the actual size of the coreset is mostly smaller than coreset_size.</p> <code>None</code> <code>sample_all</code> <code>Union[Iterable, dict]</code> <p>iterable, optional. Relevant for classification tasks only. A list of classes for which all instances should be taken, instead of applying sampling.</p> <code>None</code> <code>coreset_params</code> <code>Union[CoresetParams, dict]</code> <p>CoresetParams or dict, optional. Coreset algorithm specific parameters.</p> <p>For example: coreset_params = {     \"class_weight\": {\"a\": 0.5, \"b\": 0.5} }</p> <code>None</code> <code>node_train_function</code> <code>Callable[[ndarray, ndarray, ndarray], Any]</code> <p>Callable, optional. method for training model at tree node level.</p> <code>None</code> <code>node_train_function_params</code> <code>dict</code> <p>dict, optional. kwargs to be used when calling node_train_function.</p> <code>None</code> <code>node_metadata_func</code> <code>Callable[[Tuple[ndarray], ndarray, Union[list, None]], Union[list, dict, None]]</code> <p>callable, optional. A method for storing user meta data on each node.</p> <code>None</code> <code>working_directory</code> <code>Union[str, PathLike]</code> <p>str, path, optional. Local directory where intermediate data is stored.</p> <code>None</code> <code>cache_dir</code> <code>Union[str, PathLike]</code> <p>str, path, optional. For internal use when loading a saved service.</p> <code>None</code> <code>chunk_sample_ratio</code> <code>float</code> <p>float, optional. Indicates the size of the sample that will be taken and saved from each chunk on top of the Coreset for the validation methods. The values are from the range [0,1]. For example, chunk_sample_ratio=0.5, means that 50% of the data instances from each chunk will be saved.</p> <code>None</code> <code>model_cls</code> <code>Any</code> <p>A Scikit-learn compatible model class, optional. The model class used to train the model on the coreset, in case a specific model instance wasn't passed to fit or the validation methods. The default model class is sklearn's LogisticRegression.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.auto_preprocessing","title":"auto_preprocessing","text":"<pre><code>auto_preprocessing(X=None, sparse_output=False, copy=False)\n</code></pre> <p>Apply auto-preprocessing on the provided (test) data, similarly to the way it is done by the fit or get_coreset methods. Preprocessing includes ohe-hot encoding and handling missing values depends.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features.</p> <code>None</code> <code>sparse_output</code> <code>bool</code> <p>boolean, default False. When set to True, the function will create a sparse matrix after preprocessing.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the following keys: data: A numpy array of the preprocessed data. features: A list of feature names corresponding to the data. sparse: A boolean indicating if the output is in sparse format.</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.build","title":"build","text":"<pre><code>build(X, y, indices=None, props=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree from the parameters X, y, indices and props (properties). build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> required <code>y</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like. An array or an iterator of targets.</p> required <code>indices</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator with indices of X.</p> <code>None</code> <code>props</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.build_from_df","title":"build_from_df","text":"<pre><code>build_from_df(datasets, target_datasets=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree from pandas DataFrame(s). build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator. Data includes features, may include labels and may include indices.</p> required <code>target_datasets</code> <code>Union[Iterator[Union[DataFrame, Series]], DataFrame, Series]</code> <p>pandas DataFrame or a DataFrame iterator, optional. Use when data is split to features and target. Should include only one column.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.build_from_file","title":"build_from_file","text":"<pre><code>build_from_file(file_path, target_file_path=None, *, reader_f=pd.read_csv, reader_kwargs=None, reader_chunk_size_param_name=None, chunk_size=None, chunk_by=None, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree based on data taken from local storage. build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories. Path(s) to the place where data is stored. Data includes features, may include targets and may include indices.</p> required <code>target_file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories, optional. Use when the dataset files are split to features and target. Each file should include only one column.</p> <code>None</code> <code>reader_f</code> <code>Callable</code> <p>pandas like read method, optional, default pandas read_csv. For example, to read excel files use pandas read_excel.</p> <code>read_csv</code> <code>reader_kwargs</code> <code>dict</code> <p>dict, optional. Keyword arguments used when calling reader_f method.</p> <code>None</code> <code>reader_chunk_size_param_name</code> <code>str</code> <p>str, optional. reader_f input parameter name for reading file in chunks. When not provided we'll try to figure it out our self. Based on the data, we decide on the optimal chunk size to read and use this parameter as input when calling reader_f. Use \"ignore\" to skip the automatic chunk reading logic.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.cross_validate","title":"cross_validate","text":"<pre><code>cross_validate(level=None, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>Method for cross-validation on the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of folds and hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : the score is also displayed;</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>A list of scores, one for each fold. If return_model=True, a list of trained models is also returned (one model for each fold).</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.filter_out_samples","title":"filter_out_samples","text":"<pre><code>filter_out_samples(filter_function, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Remove samples from the coreset tree, based on the provided filter function. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>filter_function</code> <code>Callable[[Iterable, Iterable, Union[Iterable, None], Union[Iterable, None]], Iterable[Any]]</code> <p>function, optional. A function that returns a list of indices to be removed from the tree. The function should accept 4 parameters as input: indices, X, y, props and return a list(iterator) of indices to be removed from the coreset tree. For example, in order to remove all instances with a target equal to 6, use the following function: filter_function = lambda indices, X, y, props : indices[y = 6].</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.fit","title":"fit","text":"<pre><code>fit(level=0, seq_from=None, seq_to=None, model=None, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>Fit a model on the coreset tree. This model will be used when predict and predict_proba are called. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>string/datetime, optional The starting sequence of the training set.</p> <code>None</code> <code>seq_to</code> <code>Any</code> <p>string/datetime, optional The ending sequence of the training set.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>Model hyperparameters kwargs. Input when instantiating default model class.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Fitted estimator.</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.get_cleaning_samples","title":"get_cleaning_samples","text":"<pre><code>get_cleaning_samples(size=None, class_size=None, ignore_indices=None, select_from_indices=None, select_from_function=None, ignore_seen_samples=True)\n</code></pre> <p>Returns indices of samples in descending order of importance. Useful for identifying mislabeled instances and other anomalies in the data. Either class_size (recommended) or size must be provided. Must be called after build. This function is only applicable in case the coreset tree was optimized_for 'cleaning'. This function is not for retrieving the coreset (use get_coreset in this case).</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>int, optional Number of samples to return. When class_size is provided, remaining samples are taken from classes not appearing in class_size dictionary.</p> <code>None</code> <code>class_size</code> <code>Dict[Any, Union[int, str]]</code> <p>dict {class: int or \"all\" or \"any\"}, optional. Controls the number of samples to choose for each class. int: return at most size. \"all\": return all samples. \"any\": limits the returned samples to the specified classes.</p> <code>None</code> <code>ignore_indices</code> <code>Iterable</code> <p>array-like, optional. An array of indices to ignore when selecting cleaning samples.</p> <code>None</code> <code>select_from_indices</code> <code>Iterable</code> <p>array-like, optional.  An array of indices to consider when selecting cleaning samples.</p> <code>None</code> <code>select_from_function</code> <code>Callable[[Iterable, Iterable, Union[Iterable, None], Union[Iterable, None]], Iterable[Any]]</code> <p>function, optional.  Pass a function in order to limit the selection of the cleaning samples accordingly.  The function should accept 4 parameters as input: indices, X, y, properties  and return a list(iterator) of the desired indices.</p> <code>None</code> <code>ignore_seen_samples</code> <code>bool</code> <p>bool, optional, default True.  Exclude already seen samples and set the seen flag on any indices returned by the function.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Union[ValueError, dict]</code> <p>idx: array-like[int].     Cleaning samples indices. X: array-like[int].     X array. y: array-like[int].     y array. importance: array-like[float].     The importance property. Instances that receive a high Importance in the Coreset computation,     require attention as they usually indicate a labeling error,     anomaly, out-of-distribution problem or other data-related issue.</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.get_cleaning_samples--examples","title":"Examples","text":"Input <p>size=100, class_size={\"class A\": 10, \"class B\": 50, \"class C\": \"all\"}</p> Output <p>10 of \"class A\", 50 of \"class B\", 12 of \"class C\" (all), 28 of \"class D/E\"</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.get_coreset","title":"get_coreset","text":"<pre><code>get_coreset(level=0, seq_from=None, seq_to=None, preprocessing_stage='user', sparse_threshold=0.01, as_df=False, with_index=False, inverse_class_weight=True)\n</code></pre> <p>Get tree's coreset data in one of the preprocessing_stage(s) in the data preprocessing workflow. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>Any, optional. The start of the time range from which to return the coreset.</p> <code>None</code> <code>seq_to</code> <code>Any</code> <p>Any, optional. The end of the time range from which to return the coreset.</p> <code>None</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Returns the features (X) as a sparse matrix if the data density after preprocessing is below sparse_threshold, otherwise, will return the data as an array (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>as_df</code> <code>bool</code> <p>boolean, optional, default False. When True, returns the X as a pandas DataFrame.</p> <code>False</code> <code>with_index</code> <code>bool</code> <p>boolean, optional, default False. Relevant only when preprocessing_stage='auto'. Should the returned data include the index column.</p> <code>False</code> <code>inverse_class_weight</code> <code>bool</code> <p>boolean, default True. True - return weights / class_weights. False - return weights as they are. Relevant only for classification tasks and only if class_weight was passed in the coreset_params when initializing the class.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary representing the Coreset: ind: A numpy array of indices. X: A numpy array of the feature matrix. y: A numpy array of the target values. w: A numpy array of sample weights. n_represents: The number of instances represented by the coreset. features_out: A list of the output features, if preprocessing_stage=auto, otherwise None. props: A numpy array of properties, or None if not available.</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.get_coreset_size","title":"get_coreset_size","text":"<pre><code>get_coreset_size(level=0, seq_from=None, seq_to=None)\n</code></pre> <p>Returns the size of the tree's coreset data. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>string or datetime, optional, default None. The start sequence to filter samples by.</p> <code>None</code> <code>seq_to</code> <code>Union[str, datetime]</code> <p>string or datetime, optional, default None. The end sequence to filter samples by.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>coreset size</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.get_hyperparameter_tuning_data","title":"get_hyperparameter_tuning_data","text":"<pre><code>get_hyperparameter_tuning_data(level=None, validation_method='cross validation', preprocessing_stage='user', sparse_threshold=0.01, validation_size=0.2, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, inverse_class_weight=True, as_df=True)\n</code></pre> <p>A method for retrieving the data for hyperparameter tuning with cross validation, using the coreset tree. The returned data can be used with Scikit-learn\u2019s GridSearchCV, with skopt\u2019s BayesSearchCV and with any other hyperparameter tuning method that can accept a fold iterator object. Note: When using this method with Scikit-learn's <code>GridSearchCV</code> and similar methods, the <code>refit</code> parameter must be set to <code>False</code>. This is because the returned dataset (X, y and w) includes both training and validation data due to the use of a splitter. The returned dataset (X, y and w) is the concatenation of training data for all folds followed by validation data for all folds. By default, GridSearchCV refits the estimator on the entire dataset, not just the training portion, and this behavior cannot be modified and is incorrect. In this case, refit should be handled manually after the cross-validation process, by calling get_coreset with the same parameters that were passed to this function to retrieve the data and then fitting on the returned data using the best hyperparameters found in GridSearchCV. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>None</code> <code>validation_method</code> <code>str</code> <p>str, optional. Indicates which validation method will be used. The possible values are 'cross validation', 'hold-out validation' and 'seq-dependent validation'.</p> <code>'cross validation'</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Returns the features (X) as a sparse matrix if the data density after preprocessing is below sparse_threshold, otherwise, will return the data as an array (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>validation_size</code> <code>float</code> <p>float, optional, default 0.2. The size of the validation set, as a percentage of the training set size for hold-out validation.</p> <code>0.2</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>as_df</code> <code>bool</code> <p>boolean, optional, default False. When True, returns the X as a pandas DataFrame.</p> <code>True</code> <code>inverse_class_weight</code> <code>bool</code> <p>boolean, default True. True - return weights / class_weights. False - return weights as they are. Relevant only for classification tasks and only if class_weight was passed in the coreset_params when initializing the class.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, FoldIterator, Any]]</code> <p>A dictionary with the following keys: ind: The indices of the data. X: The data. y: The labels. w: The weights. splitter: The fold iterator. model_params: The model parameters.</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.get_max_level","title":"get_max_level","text":"<pre><code>get_max_level()\n</code></pre> <p>Return the maximal level of the coreset tree. Level 0 is the head of the tree. Level 1 is the level below the head of the tree, etc.</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.grid_search","title":"grid_search","text":"<pre><code>grid_search(param_grid, level=None, validation_method='cross validation', model=None, scoring=None, refit=True, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, error_score=np.nan, validation_size=0.2, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, n_jobs=None)\n</code></pre> <p>A method for performing hyperparameter selection by grid search, using the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>param_grid</code> <code>Union[Dict[str, List], List[Dict[str, List]]]</code> <p>dict or list of dicts. Dictionary with parameters names (str) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.</p> required <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>validation_method</code> <code>str</code> <p>str, optional. Indicates which validation method will be used. The possible values are 'cross validation', 'hold-out validation' and 'seq-dependent validation'. If 'cross validation' is selected, the process involves progressing through folds. We first train and validate all hyperparameter combinations for each fold, before moving on to the subsequent folds.</p> <code>'cross validation'</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. The model class needs to implement the usual scikit-learn interface.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>refit</code> <code>bool</code> <p>bool, optional. If True, retrain the model on the whole coreset using the best found hyperparameters, and return the model. This model will be used when predict and predict_proba are called.</p> <code>True</code> <code>verbose</code> <code>int</code> <p>int, optional Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of folds and hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each fold and hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>error_score</code> <code>Union[str, float, int]</code> <p>\"raise\" or numeric, optional. Value to assign to the score if an error occurs in model training. If set to \"raise\", the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.</p> <code>nan</code> <code>validation_size</code> <code>float</code> <p>float, optional, default 0.2. The size of the validation set, as a percentage of the training set size for hold-out validation.</p> <code>0.2</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>int, optional. Default: number of CPUs. Number of jobs to run in parallel during grid search.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Dict, DataFrame, BaseEstimator], Tuple[Dict, DataFrame]]</code> <p>A dict with the best hyperparameters setting, among those provided by the user. The keys are the hyperparameters names, while the dicts' values are the hyperparameters values. A Pandas DataFrame holding the score for each hyperparameter combination and fold. For the 'cross validation' method the average across all folds for each hyperparameter combination is included too. If refit=True, the retrained model is also returned.</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.holdout_validate","title":"holdout_validate","text":"<pre><code>holdout_validate(level=None, validation_size=0.2, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>A method for hold-out validation on the coreset tree. The validation set is always the last part of the dataset. This function is only applicable in case the coreset tree was optimized_for <code>training</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>validation_size</code> <code>float</code> <p>float, optional. The percentage of the dataset that will be used for validating the model.</p> <code>0.2</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>The validation score. If return_model=True, the trained model is also returned.</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.is_dirty","title":"is_dirty","text":"<pre><code>is_dirty()\n</code></pre> <p>Returns:</p> Type Description <code>bool</code> <p>Indicates whether the coreset tree has nodes marked as dirty, meaning they were affected by any of the methods: remove_samples, update_targets, update_features or filter_out_samples, when they were called with force_do_nothing.</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(dir_path, name=None, *, data_manager=None, load_buffer=True, working_directory=None)\n</code></pre> <p>Restore a service object from a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>str, path. Local directory where service data is stored.</p> required <code>name</code> <code>str</code> <p>string, optional, default service class name (lower case). The name prefix of the subdirectory to load. When several subdirectories having the same name prefix are found, the last one, ordered by name, is selected. For example when saving with override=False, the chosen subdirectory is the last saved.</p> <code>None</code> <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. When specified, input data manger will be used instead of restoring it from the saved configuration.</p> <code>None</code> <code>load_buffer</code> <code>bool</code> <p>boolean, optional, default True. If set, load saved buffer (a partial node of the tree) from disk and add it to the tree.</p> <code>True</code> <code>working_directory</code> <code>Union[str, PathLike]</code> <p>str, path, optional, default use working_directory from saved configuration. Local directory where intermediate data is stored.</p> <code>None</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>CoresetTreeService object</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.partial_build","title":"partial_build","text":"<pre><code>partial_build(X, y, indices=None, props=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree from parameters X, y, indices and props (properties). Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> required <code>y</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like. An array or an iterator of targets.</p> required <code>indices</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator with indices of X.</p> <code>None</code> <code>props</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.partial_build_from_df","title":"partial_build_from_df","text":"<pre><code>partial_build_from_df(datasets, target_datasets=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree based on the pandas DataFrame iterator. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator. Data includes features, may include targets and may include indices.</p> required <code>target_datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator, optional. Use when data is split to features and target. Should include only one column.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional, default previous used chunk_size. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.partial_build_from_file","title":"partial_build_from_file","text":"<pre><code>partial_build_from_file(file_path, target_file_path=None, *, reader_f=pd.read_csv, reader_kwargs=None, reader_chunk_size_param_name=None, chunk_size=None, chunk_by=None, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree based on data taken from local storage. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories. Path(s) to the place where data is stored. Data includes features, may include targets and may include indices.</p> required <code>target_file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories, optional. Use when files are split to features and target. Each file should include only one column.</p> <code>None</code> <code>reader_f</code> <code>Callable</code> <p>pandas like read method, optional, default pandas read_csv. For example, to read excel files use pandas read_excel.</p> <code>read_csv</code> <code>reader_kwargs</code> <code>dict</code> <p>dict, optional. Keyword arguments used when calling reader_f method.</p> <code>None</code> <code>reader_chunk_size_param_name</code> <code>str</code> <p>str, optional. reader_f input parameter name for reading file in chunks. When not provided we'll try to figure it out our self. Based on the data, we decide on the optimal chunk size to read and use this parameter as input when calling reader_f. Use \"ignore\" to skip the automatic chunk reading logic.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional, default previous used chunk_size. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.plot","title":"plot","text":"<pre><code>plot(dir_path=None, selected_trees=None)\n</code></pre> <p>Produce a tree graph plot and save figure as a local png file.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike. Path to save the plot figure in; if not provided, or if isn't valid/doesn't exist, the figure will be saved in the current directory (from which this method is called).</p> <code>None</code> <code>selected_trees</code> <code>dict</code> <p>dict, optional. A dictionary containing the names of the image file(s) to be generated.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Image file path</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.predict","title":"predict","text":"<pre><code>predict(X, sparse_output=False, copy=False)\n</code></pre> <p>Run prediction on the trained model. This function is only applicable in case the coreset tree was optimized_for 'training' and in case fit() or grid_search(refit=True) where called before. The function automatically preprocesses the data according to the preprocessing_stage used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>An array of features.</p> required <code>sparse_output</code> <code>bool</code> <p>boolean, optional, default False. When set to True, the function will create a sparse matrix after preprocessing and pass it to the predict function.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <p>Model prediction results.</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X, sparse_output=False, copy=False)\n</code></pre> <p>Run prediction on the trained model. This function is only applicable in case the coreset tree was optimized_for 'training' and in case fit() or grid_search(refit=True) where called before. The function automatically preprocesses the data according to the preprocessing_stage used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>An array of features.</p> required <code>sparse_output</code> <code>bool</code> <p>boolean, optional, default False. When set to True, the function will create a sparse matrix after preprocessing and pass it to the predict_proba function.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <p>Returns the probability of the sample for each class in the model.</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.print","title":"print","text":"<pre><code>print(selected_tree=None)\n</code></pre> <p>Print the tree's string representation.</p> <p>Parameters:</p> Name Type Description Default <code>selected_tree</code> <code>str</code> <p>string, optional. Which tree to print. Defaults to printing all.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.remove_samples","title":"remove_samples","text":"<pre><code>remove_samples(indices, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Remove samples from the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be removed from the coreset tree.</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.save","title":"save","text":"<pre><code>save(dir_path=None, name=None, save_buffer=True, override=False, allow_pickle=True)\n</code></pre> <p>Save service configuration and relevant data to a local directory. Use this method when the service needs to be restored.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike, optional, default self.working_directory. A local directory for saving service's files.</p> <code>None</code> <code>name</code> <code>str</code> <p>string, optional, default service class name (lower case). Name of the subdirectory where the data will be stored.</p> <code>None</code> <code>save_buffer</code> <code>bool</code> <p>boolean, default True. Save also the data in the buffer (a partial node of the tree) along with the rest of the saved data.</p> <code>True</code> <code>override</code> <code>bool</code> <p>bool, optional, default False. False: add a timestamp suffix so each save won\u2019t override the previous ones. True: The existing subdirectory with the provided name is overridden.</p> <code>False</code> <code>allow_pickle</code> <code>bool</code> <p>bool, optional, default True. True: Saves the Coreset tree in pickle format (much faster). False: Saves the Coreset tree in JSON format.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>Save directory path.</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.save_coreset","title":"save_coreset","text":"<pre><code>save_coreset(file_path, level=0, preprocessing_stage='user', with_index=True)\n</code></pre> <p>Get the coreset from the tree and save it to a file. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike. Local file path to store the coreset.</p> required <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>with_index</code> <code>bool</code> <p>boolean, optional, default False. Relevant only when preprocessing_stage=<code>auto</code>. Should the returned data include the index column.</p> <code>True</code>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.seq_dependent_validate","title":"seq_dependent_validate","text":"<pre><code>seq_dependent_validate(level=None, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>The method allows to train and validate on a subset of the Coreset tree, according to the <code>seq_column</code> defined in the <code>DataParams</code> structure passed to the init. This function is only applicable in case the coreset tree was optimized_for <code>training</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree from which the search for the best matching nodes starts. Nodes closer to the leaf level than the specified level, may be selected to better match the provided seq parameters.If None, the search starts from level 0, the head of the tree. If None, the best level will be selected.</p> <code>None</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>The validation score. If return_model=True, the trained model is also returned.</p>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.set_model_cls","title":"set_model_cls","text":"<pre><code>set_model_cls(model_cls)\n</code></pre> <p>Set the model class used to train the model on the coreset, in case a specific model instance wasn't passed to fit or the validation methods.</p> <p>Parameters:</p> Name Type Description Default <code>model_cls</code> <code>Any</code> <p>A Scikit-learn compatible model class.</p> required"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.set_seen_indication","title":"set_seen_indication","text":"<pre><code>set_seen_indication(seen_flag=True, indices=None)\n</code></pre> <p>Set samples as 'seen' or 'unseen'. Not providing an indices list defaults to setting the flag on all samples. This function is only applicable in case the coreset tree was optimized_for 'cleaning'.</p> <p>Parameters:</p> Name Type Description Default <code>seen_flag</code> <code>bool</code> <p>bool, optional, default True. Set 'seen' or 'unseen' flag</p> <code>True</code> <code>indices</code> <code>Iterable</code> <p>array like, optional. Set flag only for the provided list of indices. Defaults to all indices.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.update_dirty","title":"update_dirty","text":"<pre><code>update_dirty(force_resample_all=None, force_sensitivity_recalc=None)\n</code></pre> <p>Calculate the sensitivity and resample the nodes that were marked as dirty, meaning they were affected by any of the methods: remove_samples, update_targets, update_features or filter_out_samples, when they were called with force_do_nothing.</p> <p>Parameters:</p> Name Type Description Default <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.update_features","title":"update_features","text":"<pre><code>update_features(indices, X, feature_names=None, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Update the features for selected samples on the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be updated.</p> required <code>X</code> <code>Iterable</code> <p>array-like. An array of features. Should have the same length as indices.</p> required <code>feature_names</code> <code>Iterable[str]</code> <p>If the quantity of features in X is not equal to the quantity of features in the original coreset, this param should contain list of names of passed features.</p> <code>None</code> <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/lg/#services.coreset_tree.lg.CoresetTreeServiceLG.update_targets","title":"update_targets","text":"<pre><code>update_targets(indices, y, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Update the targets for selected samples on the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be updated.</p> required <code>y</code> <code>Iterable</code> <p>array-like. An array of classes/labels. Should have the same length as indices.</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/lr/","title":"CoresetTreeServiceLR","text":""},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR","title":"CoresetTreeServiceLR","text":"<pre><code>CoresetTreeServiceLR(*, data_manager=None, data_params=None, n_instances=None, max_memory_gb=None, optimized_for, chunk_size=None, chunk_by=None, coreset_size=None, coreset_params=None, working_directory=None, cache_dir=None, node_train_function=None, node_train_function_params=None, node_metadata_func=None, chunk_sample_ratio=None, model_cls=None)\n</code></pre> <p>             Bases: <code>CoresetTreeServiceSupervisedMixin</code>, <code>CoresetTreeService</code></p> <p>Subclass of CoresetTreeService for Linear Regression. A service class for creating a coreset tree and working with it. optimized_for is a required parameter defining the main usage of the service: 'training', 'cleaning' or both, optimized_for=['training', 'cleaning']. The service will decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset, based on the triplet: n_instances, max_memory_gb and the 'number of features' (deduced from the dataset). The chunk_size and coreset_size will be deduced based on the above triplet too. In case chunk_size and coreset_size are provided, they will override all above mentioned parameters (less recommended).</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. The class used to interact with the provided data and store it locally. By default, only the sampled data is stored in HDF5 files format.</p> <code>None</code> <code>data_params</code> <code>Union[DataParams, dict]</code> <p>DataParams, optional. Data preprocessing information.</p> <code>None</code> <code>n_instances</code> <code>int</code> <p>int. The total number of instances that are going to be processed (can be an estimation). This parameter is required and the only one from the above mentioned quadruplet, which isn't deduced from the data.</p> <code>None</code> <code>max_memory_gb</code> <code>int</code> <p>int, optional. The maximum memory in GB that should be used. When not provided, the server's total memory is used. In any case only 80% of the provided memory or the server's total memory is considered.</p> <code>None</code> <code>optimized_for</code> <code>Union[list, str]</code> <p>str or list Either 'training', 'cleaning' or or both ['training', 'cleaning']. The main usage of the service.</p> required <code>chunk_size</code> <code>Union[dict, int]</code> <p>int, optional. The number of instances to be used when creating a coreset node in the tree. When defined, it will override the parameters of optimized_for, n_instances, n_classes and max_memory_gb. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>coreset_size</code> <code>Union[int, float, dict]</code> <p>int or float, optional. Represents the coreset size of each node in the coreset tree. If provided as a float, it represents the ratio between each chunk and the resulting coreset.In any case the coreset_size is limited to 60% of the chunk_size. The coreset is constructed by sampling data instances from the dataset based on their calculated importance. Since each instance may be sampled more than once, in practice, the actual size of the coreset is mostly smaller than coreset_size.</p> <code>None</code> <code>coreset_params</code> <code>Union[CoresetParams, dict]</code> <p>CoresetParams or dict, optional. Coreset algorithm specific parameters.</p> <code>None</code> <code>node_train_function</code> <code>Callable[[ndarray, ndarray, ndarray], Any]</code> <p>Callable, optional. method for training model at tree node level.</p> <code>None</code> <code>node_train_function_params</code> <code>dict</code> <p>dict, optional. kwargs to be used when calling node_train_function.</p> <code>None</code> <code>node_metadata_func</code> <code>Callable[[Tuple[ndarray], ndarray, Union[list, None]], Union[list, dict, None]]</code> <p>callable, optional. A method for storing user meta data on each node.</p> <code>None</code> <code>working_directory</code> <code>Union[str, PathLike]</code> <p>str, path, optional. Local directory where intermediate data is stored.</p> <code>None</code> <code>cache_dir</code> <code>Union[str, PathLike]</code> <p>str, path, optional. For internal use when loading a saved service.</p> <code>None</code> <code>chunk_sample_ratio</code> <code>float</code> <p>float, optional. Indicates the size of the sample that will be taken and saved from each chunk on top of the Coreset for the validation methods. The values are from the range [0,1]. For example, chunk_sample_ratio=0.5, means that 50% of the data instances from each chunk will be saved.</p> <code>None</code> <code>model_cls</code> <code>Any</code> <p>A Scikit-learn compatible model class, optional. The model class used to train the model on the coreset, in case a specific model instance wasn't passed to fit or the validation methods. The default model class is sklearn's LinearRegression.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.auto_preprocessing","title":"auto_preprocessing","text":"<pre><code>auto_preprocessing(X=None, sparse_output=False, copy=False)\n</code></pre> <p>Apply auto-preprocessing on the provided (test) data, similarly to the way it is done by the fit or get_coreset methods. Preprocessing includes ohe-hot encoding and handling missing values depends.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features.</p> <code>None</code> <code>sparse_output</code> <code>bool</code> <p>boolean, default False. When set to True, the function will create a sparse matrix after preprocessing.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the following keys: data: A numpy array of the preprocessed data. features: A list of feature names corresponding to the data. sparse: A boolean indicating if the output is in sparse format.</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.build","title":"build","text":"<pre><code>build(X, y, indices=None, props=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree from the parameters X, y, indices and props (properties). build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> required <code>y</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like. An array or an iterator of targets.</p> required <code>indices</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator with indices of X.</p> <code>None</code> <code>props</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.build_from_df","title":"build_from_df","text":"<pre><code>build_from_df(datasets, target_datasets=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree from pandas DataFrame(s). build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator. Data includes features, may include labels and may include indices.</p> required <code>target_datasets</code> <code>Union[Iterator[Union[DataFrame, Series]], DataFrame, Series]</code> <p>pandas DataFrame or a DataFrame iterator, optional. Use when data is split to features and target. Should include only one column.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.build_from_file","title":"build_from_file","text":"<pre><code>build_from_file(file_path, target_file_path=None, *, reader_f=pd.read_csv, reader_kwargs=None, reader_chunk_size_param_name=None, chunk_size=None, chunk_by=None, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree based on data taken from local storage. build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories. Path(s) to the place where data is stored. Data includes features, may include targets and may include indices.</p> required <code>target_file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories, optional. Use when the dataset files are split to features and target. Each file should include only one column.</p> <code>None</code> <code>reader_f</code> <code>Callable</code> <p>pandas like read method, optional, default pandas read_csv. For example, to read excel files use pandas read_excel.</p> <code>read_csv</code> <code>reader_kwargs</code> <code>dict</code> <p>dict, optional. Keyword arguments used when calling reader_f method.</p> <code>None</code> <code>reader_chunk_size_param_name</code> <code>str</code> <p>str, optional. reader_f input parameter name for reading file in chunks. When not provided we'll try to figure it out our self. Based on the data, we decide on the optimal chunk size to read and use this parameter as input when calling reader_f. Use \"ignore\" to skip the automatic chunk reading logic.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.cross_validate","title":"cross_validate","text":"<pre><code>cross_validate(level=None, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>Method for cross-validation on the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of folds and hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : the score is also displayed;</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>A list of scores, one for each fold. If return_model=True, a list of trained models is also returned (one model for each fold).</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.filter_out_samples","title":"filter_out_samples","text":"<pre><code>filter_out_samples(filter_function, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Remove samples from the coreset tree, based on the provided filter function. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>filter_function</code> <code>Callable[[Iterable, Iterable, Union[Iterable, None], Union[Iterable, None]], Iterable[Any]]</code> <p>function, optional. A function that returns a list of indices to be removed from the tree. The function should accept 4 parameters as input: indices, X, y, props and return a list(iterator) of indices to be removed from the coreset tree. For example, in order to remove all instances with a target equal to 6, use the following function: filter_function = lambda indices, X, y, props : indices[y = 6].</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.fit","title":"fit","text":"<pre><code>fit(level=0, seq_from=None, seq_to=None, model=None, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>Fit a model on the coreset tree. This model will be used when predict and predict_proba are called. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>string/datetime, optional The starting sequence of the training set.</p> <code>None</code> <code>seq_to</code> <code>Any</code> <p>string/datetime, optional The ending sequence of the training set.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>Model hyperparameters kwargs. Input when instantiating default model class.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Fitted estimator.</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.get_cleaning_samples","title":"get_cleaning_samples","text":"<pre><code>get_cleaning_samples(size=None, ignore_indices=None, select_from_indices=None, select_from_function=None, ignore_seen_samples=True)\n</code></pre> <p>Returns indices of samples in descending order of importance. Useful for identifying mislabeled instances and other anomalies in the data. size must be provided. Function must be called after build. This function is only applicable in case the coreset tree was optimized_for 'cleaning'. This function is not for retrieving the coreset (use get_coreset in this case).</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>required, optional. Number of samples to return.</p> <code>None</code> <code>ignore_indices</code> <code>Iterable</code> <p>array-like, optional. An array of indices to ignore when selecting cleaning samples.</p> <code>None</code> <code>select_from_indices</code> <code>Iterable</code> <p>array-like, optional.  An array of indices to consider when selecting cleaning samples.</p> <code>None</code> <code>select_from_function</code> <code>Callable[[Iterable, Iterable, Union[Iterable, None], Union[Iterable, None]], Iterable[Any]]</code> <p>function, optional.  Pass a function in order to limit the selection of the cleaning samples accordingly.  The function should accept 4 parameters as input: indices, X, y, props.  and return a list(iterator) of the desired indices.</p> <code>None</code> <code>ignore_seen_samples</code> <code>bool</code> <p>bool, optional, default True.  Exclude already seen samples and set the seen flag on any indices returned by the function.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Union[ValueError, dict]</code> <p>idx: array-like[int].     Cleaning samples indices. X: array-like[int].     X array. y: array-like[int].     y array. importance: array-like[float].     The importance property. Instances that receive a high Importance in the Coreset computation,     require attention as they usually indicate a labeling error,     anomaly, out-of-distribution problem or other data-related issue.</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.get_coreset","title":"get_coreset","text":"<pre><code>get_coreset(level=0, preprocessing_stage='user', sparse_threshold=0.01, as_df=False, with_index=False, seq_from=None, seq_to=None)\n</code></pre> <p>Get tree's coreset data in one of the preprocessing_stage(s) in the data preprocessing workflow. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Returns the features (X) as a sparse matrix if the data density after preprocessing is below sparse_threshold, otherwise, will return the data as an array (Applicable only for preprocessing_stage=<code>auto</code>).</p> <code>0.01</code> <code>as_df</code> <code>bool</code> <p>boolean, optional, default False. When True, returns the X as a pandas DataFrame.</p> <code>False</code> <code>with_index</code> <code>bool</code> <p>boolean, optional, default False. Relevant only when preprocessing_stage=<code>auto</code>. Should the returned data include the index column.</p> <code>False</code> <code>seq_from</code> <code>Any</code> <p>string or datetime, optional, default None. The start sequence to filter samples by.</p> <code>None</code> <code>seq_to</code> <code>Any</code> <p>string or datetime, optional, default None. The end sequence to filter samples by.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary representing the Coreset: ind: A numpy array of indices. X: A numpy array of the feature matrix. y: A numpy array of the target values. w: A numpy array of sample weights. n_represents: The number of instances represented by the coreset. features_out: A list of the output features, if preprocessing_stage=auto, otherwise None. props: A numpy array of properties, or None if not available.</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.get_coreset_size","title":"get_coreset_size","text":"<pre><code>get_coreset_size(level=0, seq_from=None, seq_to=None)\n</code></pre> <p>Returns the size of the tree's coreset data. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>string or datetime, optional, default None. The start sequence to filter samples by.</p> <code>None</code> <code>seq_to</code> <code>Union[str, datetime]</code> <p>string or datetime, optional, default None. The end sequence to filter samples by.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>coreset size</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.get_hyperparameter_tuning_data","title":"get_hyperparameter_tuning_data","text":"<pre><code>get_hyperparameter_tuning_data(level=None, validation_method='cross validation', preprocessing_stage='user', sparse_threshold=0.01, validation_size=0.2, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, as_df=True)\n</code></pre> <p>A method for retrieving the data for hyperparameter tuning with cross validation, using the coreset tree. The returned data can be used with Scikit-learn\u2019s GridSearchCV, with skopt\u2019s BayesSearchCV and with any other hyperparameter tuning method that can accept a fold iterator object. Note: When using this method with Scikit-learn's <code>GridSearchCV</code> and similar methods, the <code>refit</code> parameter must be set to <code>False</code>. This is because the returned dataset (X, y and w) includes both training and validation data due to the use of a splitter. The returned dataset (X, y and w) is the concatenation of training data for all folds followed by validation data for all folds. By default, GridSearchCV refits the estimator on the entire dataset, not just the training portion, and this behavior cannot be modified and is incorrect. In this case, refit should be handled manually after the cross-validation process, by calling get_coreset with the same parameters that were passed to this function to retrieve the data and then fitting on the returned data using the best hyperparameters found in GridSearchCV. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>None</code> <code>validation_method</code> <code>str</code> <p>str, optional. Indicates which validation method will be used. The possible values are 'cross validation', 'hold-out validation' and 'seq-dependent validation'.</p> <code>'cross validation'</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Returns the features (X) as a sparse matrix if the data density after preprocessing is below sparse_threshold, otherwise, will return the data as an array (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>validation_size</code> <code>float</code> <p>float, optional, default 0.2. The size of the validation set, as a percentage of the training set size for hold-out validation.</p> <code>0.2</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>as_df</code> <code>bool</code> <p>boolean, optional, default False. When True, returns the X as a pandas DataFrame.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, FoldIterator, Any]]</code> <p>A dictionary with the following keys: ind: The indices of the data. X: The data. y: The labels. w: The weights. splitter: The fold iterator. model_params: The model parameters.</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.get_max_level","title":"get_max_level","text":"<pre><code>get_max_level()\n</code></pre> <p>Return the maximal level of the coreset tree. Level 0 is the head of the tree. Level 1 is the level below the head of the tree, etc.</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.grid_search","title":"grid_search","text":"<pre><code>grid_search(param_grid, level=None, validation_method='cross validation', model=None, scoring=None, refit=True, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, error_score=np.nan, validation_size=0.2, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, n_jobs=None)\n</code></pre> <p>A method for performing hyperparameter selection by grid search, using the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>param_grid</code> <code>Union[Dict[str, List], List[Dict[str, List]]]</code> <p>dict or list of dicts. Dictionary with parameters names (str) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.</p> required <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>validation_method</code> <code>str</code> <p>str, optional. Indicates which validation method will be used. The possible values are 'cross validation', 'hold-out validation' and 'seq-dependent validation'. If 'cross validation' is selected, the process involves progressing through folds. We first train and validate all hyperparameter combinations for each fold, before moving on to the subsequent folds.</p> <code>'cross validation'</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. The model class needs to implement the usual scikit-learn interface.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>refit</code> <code>bool</code> <p>bool, optional. If True, retrain the model on the whole coreset using the best found hyperparameters, and return the model. This model will be used when predict and predict_proba are called.</p> <code>True</code> <code>verbose</code> <code>int</code> <p>int, optional Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of folds and hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each fold and hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>error_score</code> <code>Union[str, float, int]</code> <p>\"raise\" or numeric, optional. Value to assign to the score if an error occurs in model training. If set to \"raise\", the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.</p> <code>nan</code> <code>validation_size</code> <code>float</code> <p>float, optional, default 0.2. The size of the validation set, as a percentage of the training set size for hold-out validation.</p> <code>0.2</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>int, optional. Default: number of CPUs. Number of jobs to run in parallel during grid search.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Dict, DataFrame, BaseEstimator], Tuple[Dict, DataFrame]]</code> <p>A dict with the best hyperparameters setting, among those provided by the user. The keys are the hyperparameters names, while the dicts' values are the hyperparameters values. A Pandas DataFrame holding the score for each hyperparameter combination and fold. For the 'cross validation' method the average across all folds for each hyperparameter combination is included too. If refit=True, the retrained model is also returned.</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.holdout_validate","title":"holdout_validate","text":"<pre><code>holdout_validate(level=None, validation_size=0.2, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>A method for hold-out validation on the coreset tree. The validation set is always the last part of the dataset. This function is only applicable in case the coreset tree was optimized_for <code>training</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>validation_size</code> <code>float</code> <p>float, optional. The percentage of the dataset that will be used for validating the model.</p> <code>0.2</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>The validation score. If return_model=True, the trained model is also returned.</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.is_dirty","title":"is_dirty","text":"<pre><code>is_dirty()\n</code></pre> <p>Returns:</p> Type Description <code>bool</code> <p>Indicates whether the coreset tree has nodes marked as dirty, meaning they were affected by any of the methods: remove_samples, update_targets, update_features or filter_out_samples, when they were called with force_do_nothing.</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(dir_path, name=None, *, data_manager=None, load_buffer=True, working_directory=None)\n</code></pre> <p>Restore a service object from a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>str, path. Local directory where service data is stored.</p> required <code>name</code> <code>str</code> <p>string, optional, default service class name (lower case). The name prefix of the subdirectory to load. When several subdirectories having the same name prefix are found, the last one, ordered by name, is selected. For example when saving with override=False, the chosen subdirectory is the last saved.</p> <code>None</code> <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. When specified, input data manger will be used instead of restoring it from the saved configuration.</p> <code>None</code> <code>load_buffer</code> <code>bool</code> <p>boolean, optional, default True. If set, load saved buffer (a partial node of the tree) from disk and add it to the tree.</p> <code>True</code> <code>working_directory</code> <code>Union[str, PathLike]</code> <p>str, path, optional, default use working_directory from saved configuration. Local directory where intermediate data is stored.</p> <code>None</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>CoresetTreeService object</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.partial_build","title":"partial_build","text":"<pre><code>partial_build(X, y, indices=None, props=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree from parameters X, y, indices and props (properties). Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> required <code>y</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like. An array or an iterator of targets.</p> required <code>indices</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator with indices of X.</p> <code>None</code> <code>props</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.partial_build_from_df","title":"partial_build_from_df","text":"<pre><code>partial_build_from_df(datasets, target_datasets=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree based on the pandas DataFrame iterator. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator. Data includes features, may include targets and may include indices.</p> required <code>target_datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator, optional. Use when data is split to features and target. Should include only one column.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional, default previous used chunk_size. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.partial_build_from_file","title":"partial_build_from_file","text":"<pre><code>partial_build_from_file(file_path, target_file_path=None, *, reader_f=pd.read_csv, reader_kwargs=None, reader_chunk_size_param_name=None, chunk_size=None, chunk_by=None, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree based on data taken from local storage. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories. Path(s) to the place where data is stored. Data includes features, may include targets and may include indices.</p> required <code>target_file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories, optional. Use when files are split to features and target. Each file should include only one column.</p> <code>None</code> <code>reader_f</code> <code>Callable</code> <p>pandas like read method, optional, default pandas read_csv. For example, to read excel files use pandas read_excel.</p> <code>read_csv</code> <code>reader_kwargs</code> <code>dict</code> <p>dict, optional. Keyword arguments used when calling reader_f method.</p> <code>None</code> <code>reader_chunk_size_param_name</code> <code>str</code> <p>str, optional. reader_f input parameter name for reading file in chunks. When not provided we'll try to figure it out our self. Based on the data, we decide on the optimal chunk size to read and use this parameter as input when calling reader_f. Use \"ignore\" to skip the automatic chunk reading logic.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional, default previous used chunk_size. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.plot","title":"plot","text":"<pre><code>plot(dir_path=None, selected_trees=None)\n</code></pre> <p>Produce a tree graph plot and save figure as a local png file.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike. Path to save the plot figure in; if not provided, or if isn't valid/doesn't exist, the figure will be saved in the current directory (from which this method is called).</p> <code>None</code> <code>selected_trees</code> <code>dict</code> <p>dict, optional. A dictionary containing the names of the image file(s) to be generated.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Image file path</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.predict","title":"predict","text":"<pre><code>predict(X, sparse_output=False, copy=False)\n</code></pre> <p>Run prediction on the trained model. This function is only applicable in case the coreset tree was optimized_for 'training' and in case fit() or grid_search(refit=True) where called before. The function automatically preprocesses the data according to the preprocessing_stage used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>An array of features.</p> required <code>sparse_output</code> <code>bool</code> <p>boolean, optional, default False. When set to True, the function will create a sparse matrix after preprocessing and pass it to the predict function.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <p>Model prediction results.</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X, sparse_output=False, copy=False)\n</code></pre> <p>Run prediction on the trained model. This function is only applicable in case the coreset tree was optimized_for 'training' and in case fit() or grid_search(refit=True) where called before. The function automatically preprocesses the data according to the preprocessing_stage used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>An array of features.</p> required <code>sparse_output</code> <code>bool</code> <p>boolean, optional, default False. When set to True, the function will create a sparse matrix after preprocessing and pass it to the predict_proba function.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <p>Returns the probability of the sample for each class in the model.</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.print","title":"print","text":"<pre><code>print(selected_tree=None)\n</code></pre> <p>Print the tree's string representation.</p> <p>Parameters:</p> Name Type Description Default <code>selected_tree</code> <code>str</code> <p>string, optional. Which tree to print. Defaults to printing all.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.remove_samples","title":"remove_samples","text":"<pre><code>remove_samples(indices, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Remove samples from the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be removed from the coreset tree.</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.save","title":"save","text":"<pre><code>save(dir_path=None, name=None, save_buffer=True, override=False, allow_pickle=True)\n</code></pre> <p>Save service configuration and relevant data to a local directory. Use this method when the service needs to be restored.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike, optional, default self.working_directory. A local directory for saving service's files.</p> <code>None</code> <code>name</code> <code>str</code> <p>string, optional, default service class name (lower case). Name of the subdirectory where the data will be stored.</p> <code>None</code> <code>save_buffer</code> <code>bool</code> <p>boolean, default True. Save also the data in the buffer (a partial node of the tree) along with the rest of the saved data.</p> <code>True</code> <code>override</code> <code>bool</code> <p>bool, optional, default False. False: add a timestamp suffix so each save won\u2019t override the previous ones. True: The existing subdirectory with the provided name is overridden.</p> <code>False</code> <code>allow_pickle</code> <code>bool</code> <p>bool, optional, default True. True: Saves the Coreset tree in pickle format (much faster). False: Saves the Coreset tree in JSON format.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>Save directory path.</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.save_coreset","title":"save_coreset","text":"<pre><code>save_coreset(file_path, level=0, preprocessing_stage='user', with_index=True)\n</code></pre> <p>Get the coreset from the tree and save it to a file. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike. Local file path to store the coreset.</p> required <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>with_index</code> <code>bool</code> <p>boolean, optional, default False. Relevant only when preprocessing_stage=<code>auto</code>. Should the returned data include the index column.</p> <code>True</code>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.seq_dependent_validate","title":"seq_dependent_validate","text":"<pre><code>seq_dependent_validate(level=None, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>The method allows to train and validate on a subset of the Coreset tree, according to the <code>seq_column</code> defined in the <code>DataParams</code> structure passed to the init. This function is only applicable in case the coreset tree was optimized_for <code>training</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree from which the search for the best matching nodes starts. Nodes closer to the leaf level than the specified level, may be selected to better match the provided seq parameters.If None, the search starts from level 0, the head of the tree. If None, the best level will be selected.</p> <code>None</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>The validation score. If return_model=True, the trained model is also returned.</p>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.set_model_cls","title":"set_model_cls","text":"<pre><code>set_model_cls(model_cls)\n</code></pre> <p>Set the model class used to train the model on the coreset, in case a specific model instance wasn't passed to fit or the validation methods.</p> <p>Parameters:</p> Name Type Description Default <code>model_cls</code> <code>Any</code> <p>A Scikit-learn compatible model class.</p> required"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.set_seen_indication","title":"set_seen_indication","text":"<pre><code>set_seen_indication(seen_flag=True, indices=None)\n</code></pre> <p>Set samples as 'seen' or 'unseen'. Not providing an indices list defaults to setting the flag on all samples. This function is only applicable in case the coreset tree was optimized_for 'cleaning'.</p> <p>Parameters:</p> Name Type Description Default <code>seen_flag</code> <code>bool</code> <p>bool, optional, default True. Set 'seen' or 'unseen' flag</p> <code>True</code> <code>indices</code> <code>Iterable</code> <p>array like, optional. Set flag only for the provided list of indices. Defaults to all indices.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.update_dirty","title":"update_dirty","text":"<pre><code>update_dirty(force_resample_all=None, force_sensitivity_recalc=None)\n</code></pre> <p>Calculate the sensitivity and resample the nodes that were marked as dirty, meaning they were affected by any of the methods: remove_samples, update_targets, update_features or filter_out_samples, when they were called with force_do_nothing.</p> <p>Parameters:</p> Name Type Description Default <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.update_features","title":"update_features","text":"<pre><code>update_features(indices, X, feature_names=None, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Update the features for selected samples on the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be updated.</p> required <code>X</code> <code>Iterable</code> <p>array-like. An array of features. Should have the same length as indices.</p> required <code>feature_names</code> <code>Iterable[str]</code> <p>If the quantity of features in X is not equal to the quantity of features in the original coreset, this param should contain list of names of passed features.</p> <code>None</code> <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/lr/#services.coreset_tree.lr.CoresetTreeServiceLR.update_targets","title":"update_targets","text":"<pre><code>update_targets(indices, y, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Update the targets for selected samples on the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be updated.</p> required <code>y</code> <code>Iterable</code> <p>array-like. An array of classes/labels. Should have the same length as indices.</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/pca/","title":"CoresetTreeServicePCA","text":""},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA","title":"CoresetTreeServicePCA","text":"<pre><code>CoresetTreeServicePCA(*, data_manager=None, data_params=None, n_instances=None, max_memory_gb=None, optimized_for, chunk_size=None, chunk_by=None, coreset_size=None, coreset_params=None, working_directory=None, cache_dir=None, node_train_function=None, node_train_function_params=None, node_metadata_func=None, chunk_sample_ratio=None, model_cls=None)\n</code></pre> <p>             Bases: <code>CoresetTreeServiceUnsupervisedMixin</code>, <code>CoresetTreeService</code></p> <p>Subclass of CoresetTreeService for PCA. A service class for creating a coreset tree and working with it. optimized_for is a required parameter defining the main usage of the service: 'training', 'cleaning' or both, optimized_for=['training', 'cleaning']. The service will decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset, based on the triplet: n_instances, n_classes, max_memory_gb and the 'number of features' (deduced from the dataset). The chunk_size and coreset_size will be deduced based on the above triplet too. In case chunk_size and coreset_size are provided, they will override all above mentioned parameters (less recommended).</p> <p>When building the Coreset, samples are selected and weights are assigned to them, therefore it is important to use functions that support the receipt of sample_weight. Sklearn's PCA implementation does not support the receipt of sample_weight, therefore, it is highly recommended to use the built-in fit or fit_transform functions of the CoresetTreeServicePCA class as they were extended to receive sample_weight.</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. The class used to interact with the provided data and store it locally. By default, only the sampled data is stored in HDF5 files format.</p> <code>None</code> <code>data_params</code> <code>Union[DataParams, dict]</code> <p>DataParams, optional. Data preprocessing information.</p> <code>None</code> <code>n_instances</code> <code>int</code> <p>int. The total number of instances that are going to be processed (can be an estimation). This parameter is required and the only one from the above mentioned quadruplet, which isn't deduced from the data.</p> <code>None</code> <code>max_memory_gb</code> <code>int</code> <p>int, optional. The maximum memory in GB that should be used. When not provided, the server's total memory is used. In any case only 80% of the provided memory or the server's total memory is considered.</p> <code>None</code> <code>optimized_for</code> <code>Union[list, str]</code> <p>str or list Either 'training', 'cleaning' or or both ['training', 'cleaning']. The main usage of the service.</p> required <code>chunk_size</code> <code>Union[dict, int]</code> <p>int, optional. The number of instances to be used when creating a coreset node in the tree. When defined, it will override the parameters of optimized_for, n_instances, n_classes and max_memory_gb. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>coreset_size</code> <code>Union[int, float, dict]</code> <p>int or float, optional. Represents the coreset size of each node in the coreset tree. If provided as a float, it represents the ratio between each chunk and the resulting coreset.In any case the coreset_size is limited to 60% of the chunk_size. The coreset is constructed by sampling data instances from the dataset based on their calculated importance. Since each instance may be sampled more than once, in practice, the actual size of the coreset is mostly smaller than coreset_size.</p> <code>None</code> <code>coreset_params</code> <code>Union[CoresetParams, dict]</code> <p>CoresetParams or dict, optional. Coreset algorithm specific parameters.</p> <code>None</code> <code>node_train_function</code> <code>Callable[[ndarray, ndarray, ndarray], Any]</code> <p>Callable, optional. method for training model at tree node level.</p> <code>None</code> <code>node_train_function_params</code> <code>dict</code> <p>dict, optional. kwargs to be used when calling node_train_function.</p> <code>None</code> <code>node_metadata_func</code> <code>Callable[[Tuple[ndarray], ndarray, Union[list, None]], Union[list, dict, None]]</code> <p>callable, optional. A method for storing user meta data on each node.</p> <code>None</code> <code>working_directory</code> <code>Union[str, PathLike]</code> <p>str, path, optional. Local directory where intermediate data is stored.</p> <code>None</code> <code>cache_dir</code> <code>Union[str, PathLike]</code> <p>str, path, optional. For internal use when loading a saved service.</p> <code>None</code> <code>chunk_sample_ratio</code> <code>float</code> <p>float, optional. Indicates the size of the sample that will be taken and saved from each chunk on top of the Coreset for the validation methods. The values are from the range [0,1]. For example, chunk_sample_ratio=0.5, means that 50% of the data instances from each chunk will be saved.</p> <code>None</code> <code>model_cls</code> <code>Any</code> <p>A Scikit-learn compatible model class, optional. The model class used to train the model on the coreset, in case a specific model instance wasn't passed to fit or the validation methods. The default model class is our WPCA class extending sklearn's PCA to support weights.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.auto_preprocessing","title":"auto_preprocessing","text":"<pre><code>auto_preprocessing(X=None, sparse_output=False, copy=False)\n</code></pre> <p>Apply auto-preprocessing on the provided (test) data, similarly to the way it is done by the fit or get_coreset methods. Preprocessing includes ohe-hot encoding and handling missing values depends.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features.</p> <code>None</code> <code>sparse_output</code> <code>bool</code> <p>boolean, default False. When set to True, the function will create a sparse matrix after preprocessing.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the following keys: data: A numpy array of the preprocessed data. features: A list of feature names corresponding to the data. sparse: A boolean indicating if the output is in sparse format.</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.build","title":"build","text":"<pre><code>build(X, y=None, indices=None, props=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree from the parameters X, y, indices and props (properties). build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> required <code>y</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of targets. The target will be ignored when the Coreset is built.</p> <code>None</code> <code>indices</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator with indices of X.</p> <code>None</code> <code>props</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.build_from_df","title":"build_from_df","text":"<pre><code>build_from_df(datasets, target_datasets=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree from pandas DataFrame(s). build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator. Data includes features, may include labels and may include indices.</p> required <code>target_datasets</code> <code>Union[Iterator[Union[DataFrame, Series]], DataFrame, Series]</code> <p>pandas DataFrame or a DataFrame iterator, optional. Use when data is split to features and target. Should include only one column.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.build_from_file","title":"build_from_file","text":"<pre><code>build_from_file(file_path, target_file_path=None, *, reader_f=pd.read_csv, reader_kwargs=None, reader_chunk_size_param_name=None, chunk_size=None, chunk_by=None, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree based on data taken from local storage. build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories. Path(s) to the place where data is stored. Data includes features, may include targets and may include indices.</p> required <code>target_file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories, optional. Use when the dataset files are split to features and target. Each file should include only one column.</p> <code>None</code> <code>reader_f</code> <code>Callable</code> <p>pandas like read method, optional, default pandas read_csv. For example, to read excel files use pandas read_excel.</p> <code>read_csv</code> <code>reader_kwargs</code> <code>dict</code> <p>dict, optional. Keyword arguments used when calling reader_f method.</p> <code>None</code> <code>reader_chunk_size_param_name</code> <code>str</code> <p>str, optional. reader_f input parameter name for reading file in chunks. When not provided we'll try to figure it out our self. Based on the data, we decide on the optimal chunk size to read and use this parameter as input when calling reader_f. Use \"ignore\" to skip the automatic chunk reading logic.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.cross_validate","title":"cross_validate","text":"<pre><code>cross_validate(level=None, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>Method for cross-validation on the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of folds and hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : the score is also displayed;</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>A list of scores, one for each fold. If return_model=True, a list of trained models is also returned (one model for each fold).</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.filter_out_samples","title":"filter_out_samples","text":"<pre><code>filter_out_samples(filter_function, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Remove samples from the coreset tree, based on the provided filter function. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>filter_function</code> <code>Callable[[Iterable, Iterable, Union[Iterable, None], Union[Iterable, None]], Iterable[Any]]</code> <p>function, optional. A function that returns a list of indices to be removed from the tree. The function should accept 4 parameters as input: indices, X, y, props and return a list(iterator) of indices to be removed from the coreset tree. For example, in order to remove all instances with a target equal to 6, use the following function: filter_function = lambda indices, X, y, props : indices[y = 6].</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.fit","title":"fit","text":"<pre><code>fit(level=0, seq_from=None, seq_to=None, model=None, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>Fit a model on the coreset tree. This model will be used when predict and predict_proba are called. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>string/datetime, optional The starting sequence of the training set.</p> <code>None</code> <code>seq_to</code> <code>Any</code> <p>string/datetime, optional The ending sequence of the training set.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>Model hyperparameters kwargs. Input when instantiating default model class.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Fitted estimator.</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.get_cleaning_samples","title":"get_cleaning_samples","text":"<pre><code>get_cleaning_samples(size=None, ignore_indices=None, select_from_indices=None, select_from_function=None, ignore_seen_samples=True)\n</code></pre> <p>Returns indices of samples in descending order of importance. Useful for identifying mislabeled instances and other anomalies in the data. size must be provided. Function must be called after build. This function is only applicable in case the coreset tree was optimized_for 'cleaning'. This function is not for retrieving the coreset (use get_coreset in this case).</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>required, optional. Number of samples to return.</p> <code>None</code> <code>ignore_indices</code> <code>Iterable</code> <p>array-like, optional. An array of indices to ignore when selecting cleaning samples.</p> <code>None</code> <code>select_from_indices</code> <code>Iterable</code> <p>array-like, optional.  An array of indices to consider when selecting cleaning samples.</p> <code>None</code> <code>select_from_function</code> <code>Callable[[Iterable, Iterable, Union[Iterable, None], Union[Iterable, None]], Iterable[Any]]</code> <p>function, optional.  Pass a function in order to limit the selection of the cleaning samples accordingly.  The function should accept 4 parameters as input: indices, X, y, props.  and return a list(iterator) of the desired indices.</p> <code>None</code> <code>ignore_seen_samples</code> <code>bool</code> <p>bool, optional, default True.  Exclude already seen samples and set the seen flag on any indices returned by the function.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Union[ValueError, dict]</code> <p>idx: array-like[int].     Cleaning samples indices. X: array-like[int].     X array. y: array-like[int].     y array. importance: array-like[float].     The importance property. Instances that receive a high Importance in the Coreset computation,     require attention as they usually indicate a labeling error,     anomaly, out-of-distribution problem or other data-related issue.</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.get_coreset","title":"get_coreset","text":"<pre><code>get_coreset(level=0, preprocessing_stage='user', sparse_threshold=0.01, as_df=False, with_index=False, seq_from=None, seq_to=None)\n</code></pre> <p>Get tree's coreset data in one of the preprocessing_stage(s) in the data preprocessing workflow. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Returns the features (X) as a sparse matrix if the data density after preprocessing is below sparse_threshold, otherwise, will return the data as an array (Applicable only for preprocessing_stage=<code>auto</code>).</p> <code>0.01</code> <code>as_df</code> <code>bool</code> <p>boolean, optional, default False. When True, returns the X as a pandas DataFrame.</p> <code>False</code> <code>with_index</code> <code>bool</code> <p>boolean, optional, default False. Relevant only when preprocessing_stage=<code>auto</code>. Should the returned data include the index column.</p> <code>False</code> <code>seq_from</code> <code>Any</code> <p>string or datetime, optional, default None. The start sequence to filter samples by.</p> <code>None</code> <code>seq_to</code> <code>Any</code> <p>string or datetime, optional, default None. The end sequence to filter samples by.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary representing the Coreset: ind: A numpy array of indices. X: A numpy array of the feature matrix. y: A numpy array of the target values. w: A numpy array of sample weights. n_represents: The number of instances represented by the coreset. features_out: A list of the output features, if preprocessing_stage=auto, otherwise None. props: A numpy array of properties, or None if not available.</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.get_coreset_size","title":"get_coreset_size","text":"<pre><code>get_coreset_size(level=0, seq_from=None, seq_to=None)\n</code></pre> <p>Returns the size of the tree's coreset data. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>string or datetime, optional, default None. The start sequence to filter samples by.</p> <code>None</code> <code>seq_to</code> <code>Union[str, datetime]</code> <p>string or datetime, optional, default None. The end sequence to filter samples by.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>coreset size</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.get_hyperparameter_tuning_data","title":"get_hyperparameter_tuning_data","text":"<pre><code>get_hyperparameter_tuning_data(level=None, validation_method='cross validation', preprocessing_stage='user', sparse_threshold=0.01, validation_size=0.2, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, as_df=True)\n</code></pre> <p>A method for retrieving the data for hyperparameter tuning with cross validation, using the coreset tree. The returned data can be used with Scikit-learn\u2019s GridSearchCV, with skopt\u2019s BayesSearchCV and with any other hyperparameter tuning method that can accept a fold iterator object. Note: When using this method with Scikit-learn's <code>GridSearchCV</code> and similar methods, the <code>refit</code> parameter must be set to <code>False</code>. This is because the returned dataset (X, y and w) includes both training and validation data due to the use of a splitter. The returned dataset (X, y and w) is the concatenation of training data for all folds followed by validation data for all folds. By default, GridSearchCV refits the estimator on the entire dataset, not just the training portion, and this behavior cannot be modified and is incorrect. In this case, refit should be handled manually after the cross-validation process, by calling get_coreset with the same parameters that were passed to this function to retrieve the data and then fitting on the returned data using the best hyperparameters found in GridSearchCV. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>None</code> <code>validation_method</code> <code>str</code> <p>str, optional. Indicates which validation method will be used. The possible values are 'cross validation', 'hold-out validation' and 'seq-dependent validation'.</p> <code>'cross validation'</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Returns the features (X) as a sparse matrix if the data density after preprocessing is below sparse_threshold, otherwise, will return the data as an array (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>validation_size</code> <code>float</code> <p>float, optional, default 0.2. The size of the validation set, as a percentage of the training set size for hold-out validation.</p> <code>0.2</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>as_df</code> <code>bool</code> <p>boolean, optional, default False. When True, returns the X as a pandas DataFrame.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, FoldIterator, Any]]</code> <p>A dictionary with the following keys: ind: The indices of the data. X: The data. y: The labels. w: The weights. splitter: The fold iterator. model_params: The model parameters.</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.get_max_level","title":"get_max_level","text":"<pre><code>get_max_level()\n</code></pre> <p>Return the maximal level of the coreset tree. Level 0 is the head of the tree. Level 1 is the level below the head of the tree, etc.</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.grid_search","title":"grid_search","text":"<pre><code>grid_search(param_grid, level=None, validation_method='cross validation', model=None, scoring=None, refit=True, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, error_score=np.nan, validation_size=0.2, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, n_jobs=None)\n</code></pre> <p>A method for performing hyperparameter selection by grid search, using the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>param_grid</code> <code>Union[Dict[str, List], List[Dict[str, List]]]</code> <p>dict or list of dicts. Dictionary with parameters names (str) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.</p> required <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>validation_method</code> <code>str</code> <p>str, optional. Indicates which validation method will be used. The possible values are 'cross validation', 'hold-out validation' and 'seq-dependent validation'. If 'cross validation' is selected, the process involves progressing through folds. We first train and validate all hyperparameter combinations for each fold, before moving on to the subsequent folds.</p> <code>'cross validation'</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. The model class needs to implement the usual scikit-learn interface.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>refit</code> <code>bool</code> <p>bool, optional. If True, retrain the model on the whole coreset using the best found hyperparameters, and return the model. This model will be used when predict and predict_proba are called.</p> <code>True</code> <code>verbose</code> <code>int</code> <p>int, optional Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of folds and hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each fold and hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>error_score</code> <code>Union[str, float, int]</code> <p>\"raise\" or numeric, optional. Value to assign to the score if an error occurs in model training. If set to \"raise\", the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.</p> <code>nan</code> <code>validation_size</code> <code>float</code> <p>float, optional, default 0.2. The size of the validation set, as a percentage of the training set size for hold-out validation.</p> <code>0.2</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>int, optional. Default: number of CPUs. Number of jobs to run in parallel during grid search.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Dict, DataFrame, BaseEstimator], Tuple[Dict, DataFrame]]</code> <p>A dict with the best hyperparameters setting, among those provided by the user. The keys are the hyperparameters names, while the dicts' values are the hyperparameters values. A Pandas DataFrame holding the score for each hyperparameter combination and fold. For the 'cross validation' method the average across all folds for each hyperparameter combination is included too. If refit=True, the retrained model is also returned.</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.holdout_validate","title":"holdout_validate","text":"<pre><code>holdout_validate(level=None, validation_size=0.2, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>A method for hold-out validation on the coreset tree. The validation set is always the last part of the dataset. This function is only applicable in case the coreset tree was optimized_for <code>training</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>validation_size</code> <code>float</code> <p>float, optional. The percentage of the dataset that will be used for validating the model.</p> <code>0.2</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>The validation score. If return_model=True, the trained model is also returned.</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.is_dirty","title":"is_dirty","text":"<pre><code>is_dirty()\n</code></pre> <p>Returns:</p> Type Description <code>bool</code> <p>Indicates whether the coreset tree has nodes marked as dirty, meaning they were affected by any of the methods: remove_samples, update_targets, update_features or filter_out_samples, when they were called with force_do_nothing.</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(dir_path, name=None, *, data_manager=None, load_buffer=True, working_directory=None)\n</code></pre> <p>Restore a service object from a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>str, path. Local directory where service data is stored.</p> required <code>name</code> <code>str</code> <p>string, optional, default service class name (lower case). The name prefix of the subdirectory to load. When several subdirectories having the same name prefix are found, the last one, ordered by name, is selected. For example when saving with override=False, the chosen subdirectory is the last saved.</p> <code>None</code> <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. When specified, input data manger will be used instead of restoring it from the saved configuration.</p> <code>None</code> <code>load_buffer</code> <code>bool</code> <p>boolean, optional, default True. If set, load saved buffer (a partial node of the tree) from disk and add it to the tree.</p> <code>True</code> <code>working_directory</code> <code>Union[str, PathLike]</code> <p>str, path, optional, default use working_directory from saved configuration. Local directory where intermediate data is stored.</p> <code>None</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>CoresetTreeService object</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.partial_build","title":"partial_build","text":"<pre><code>partial_build(X, y=None, indices=None, props=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree from parameters X, y, indices and props (properties). Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> required <code>y</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of targets. The target will be ignored when the Coreset is built.</p> <code>None</code> <code>indices</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator with indices of X.</p> <code>None</code> <code>props</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.partial_build_from_df","title":"partial_build_from_df","text":"<pre><code>partial_build_from_df(datasets, target_datasets=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree based on the pandas DataFrame iterator. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator. Data includes features, may include targets and may include indices.</p> required <code>target_datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator, optional. Use when data is split to features and target. Should include only one column.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional, default previous used chunk_size. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:     self</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.partial_build_from_file","title":"partial_build_from_file","text":"<pre><code>partial_build_from_file(file_path, target_file_path=None, *, reader_f=pd.read_csv, reader_kwargs=None, reader_chunk_size_param_name=None, chunk_size=None, chunk_by=None, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree based on data taken from local storage. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories. Path(s) to the place where data is stored. Data includes features, may include targets and may include indices.</p> required <code>target_file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories, optional. Use when files are split to features and target. Each file should include only one column.</p> <code>None</code> <code>reader_f</code> <code>Callable</code> <p>pandas like read method, optional, default pandas read_csv. For example, to read excel files use pandas read_excel.</p> <code>read_csv</code> <code>reader_kwargs</code> <code>dict</code> <p>dict, optional. Keyword arguments used when calling reader_f method.</p> <code>None</code> <code>reader_chunk_size_param_name</code> <code>str</code> <p>str, optional. reader_f input parameter name for reading file in chunks. When not provided we'll try to figure it out our self. Based on the data, we decide on the optimal chunk size to read and use this parameter as input when calling reader_f. Use \"ignore\" to skip the automatic chunk reading logic.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional, default previous used chunk_size. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.plot","title":"plot","text":"<pre><code>plot(dir_path=None, selected_trees=None)\n</code></pre> <p>Produce a tree graph plot and save figure as a local png file.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike. Path to save the plot figure in; if not provided, or if isn't valid/doesn't exist, the figure will be saved in the current directory (from which this method is called).</p> <code>None</code> <code>selected_trees</code> <code>dict</code> <p>dict, optional. A dictionary containing the names of the image file(s) to be generated.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Image file path</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.predict","title":"predict","text":"<pre><code>predict(X, sparse_output=False, copy=False)\n</code></pre> <p>Run prediction on the trained model. This function is only applicable in case the coreset tree was optimized_for 'training' and in case fit() or grid_search(refit=True) where called before. The function automatically preprocesses the data according to the preprocessing_stage used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>An array of features.</p> required <code>sparse_output</code> <code>bool</code> <p>boolean, optional, default False. When set to True, the function will create a sparse matrix after preprocessing and pass it to the predict function.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <p>Model prediction results.</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X, sparse_output=False, copy=False)\n</code></pre> <p>Run prediction on the trained model. This function is only applicable in case the coreset tree was optimized_for 'training' and in case fit() or grid_search(refit=True) where called before. The function automatically preprocesses the data according to the preprocessing_stage used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>An array of features.</p> required <code>sparse_output</code> <code>bool</code> <p>boolean, optional, default False. When set to True, the function will create a sparse matrix after preprocessing and pass it to the predict_proba function.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <p>Returns the probability of the sample for each class in the model.</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.print","title":"print","text":"<pre><code>print(selected_tree=None)\n</code></pre> <p>Print the tree's string representation.</p> <p>Parameters:</p> Name Type Description Default <code>selected_tree</code> <code>str</code> <p>string, optional. Which tree to print. Defaults to printing all.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.remove_samples","title":"remove_samples","text":"<pre><code>remove_samples(indices, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Remove samples from the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be removed from the coreset tree.</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.save","title":"save","text":"<pre><code>save(dir_path=None, name=None, save_buffer=True, override=False, allow_pickle=True)\n</code></pre> <p>Save service configuration and relevant data to a local directory. Use this method when the service needs to be restored.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike, optional, default self.working_directory. A local directory for saving service's files.</p> <code>None</code> <code>name</code> <code>str</code> <p>string, optional, default service class name (lower case). Name of the subdirectory where the data will be stored.</p> <code>None</code> <code>save_buffer</code> <code>bool</code> <p>boolean, default True. Save also the data in the buffer (a partial node of the tree) along with the rest of the saved data.</p> <code>True</code> <code>override</code> <code>bool</code> <p>bool, optional, default False. False: add a timestamp suffix so each save won\u2019t override the previous ones. True: The existing subdirectory with the provided name is overridden.</p> <code>False</code> <code>allow_pickle</code> <code>bool</code> <p>bool, optional, default True. True: Saves the Coreset tree in pickle format (much faster). False: Saves the Coreset tree in JSON format.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>Save directory path.</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.save_coreset","title":"save_coreset","text":"<pre><code>save_coreset(file_path, level=0, preprocessing_stage='user', with_index=True)\n</code></pre> <p>Get the coreset from the tree and save it to a file. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike. Local file path to store the coreset.</p> required <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>with_index</code> <code>bool</code> <p>boolean, optional, default False. Relevant only when preprocessing_stage=<code>auto</code>. Should the returned data include the index column.</p> <code>True</code>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.seq_dependent_validate","title":"seq_dependent_validate","text":"<pre><code>seq_dependent_validate(level=None, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>The method allows to train and validate on a subset of the Coreset tree, according to the <code>seq_column</code> defined in the <code>DataParams</code> structure passed to the init. This function is only applicable in case the coreset tree was optimized_for <code>training</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree from which the search for the best matching nodes starts. Nodes closer to the leaf level than the specified level, may be selected to better match the provided seq parameters.If None, the search starts from level 0, the head of the tree. If None, the best level will be selected.</p> <code>None</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>The validation score. If return_model=True, the trained model is also returned.</p>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.set_model_cls","title":"set_model_cls","text":"<pre><code>set_model_cls(model_cls)\n</code></pre> <p>Set the model class used to train the model on the coreset, in case a specific model instance wasn't passed to fit or the validation methods.</p> <p>Parameters:</p> Name Type Description Default <code>model_cls</code> <code>Any</code> <p>A Scikit-learn compatible model class.</p> required"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.set_seen_indication","title":"set_seen_indication","text":"<pre><code>set_seen_indication(seen_flag=True, indices=None)\n</code></pre> <p>Set samples as 'seen' or 'unseen'. Not providing an indices list defaults to setting the flag on all samples. This function is only applicable in case the coreset tree was optimized_for 'cleaning'.</p> <p>Parameters:</p> Name Type Description Default <code>seen_flag</code> <code>bool</code> <p>bool, optional, default True. Set 'seen' or 'unseen' flag</p> <code>True</code> <code>indices</code> <code>Iterable</code> <p>array like, optional. Set flag only for the provided list of indices. Defaults to all indices.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.update_dirty","title":"update_dirty","text":"<pre><code>update_dirty(force_resample_all=None, force_sensitivity_recalc=None)\n</code></pre> <p>Calculate the sensitivity and resample the nodes that were marked as dirty, meaning they were affected by any of the methods: remove_samples, update_targets, update_features or filter_out_samples, when they were called with force_do_nothing.</p> <p>Parameters:</p> Name Type Description Default <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.update_features","title":"update_features","text":"<pre><code>update_features(indices, X, feature_names=None, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Update the features for selected samples on the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be updated.</p> required <code>X</code> <code>Iterable</code> <p>array-like. An array of features. Should have the same length as indices.</p> required <code>feature_names</code> <code>Iterable[str]</code> <p>If the quantity of features in X is not equal to the quantity of features in the original coreset, this param should contain list of names of passed features.</p> <code>None</code> <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/pca/#services.coreset_tree.pca.CoresetTreeServicePCA.update_targets","title":"update_targets","text":"<pre><code>update_targets(indices, y, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Update the targets for selected samples on the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be updated.</p> required <code>y</code> <code>Iterable</code> <p>array-like. An array of classes/labels. Should have the same length as indices.</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/svd/","title":"CoresetTreeServiceSVD","text":""},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD","title":"CoresetTreeServiceSVD","text":"<pre><code>CoresetTreeServiceSVD(*, data_manager=None, data_params=None, n_instances=None, max_memory_gb=None, n_classes=None, optimized_for=None, chunk_size=None, chunk_by=None, coreset_size=None, coreset_params=None, sample_all=None, working_directory=None, cache_dir=None, node_train_function=None, node_train_function_params=None, node_metadata_func=None, chunk_sample_ratio=None, model_cls=None)\n</code></pre> <p>             Bases: <code>CoresetTreeServiceUnsupervisedMixin</code>, <code>CoresetTreeService</code></p> <p>Subclass of CoresetTreeService for SVD. A service class for creating a coreset tree and working with it. optimized_for is a required parameter defining the main usage of the service: 'training', 'cleaning' or both, optimized_for=['training', 'cleaning']. The service will decide whether to build an actual Coreset Tree or to build a single Coreset over the entire dataset, based on the triplet: n_instances, n_classes, max_memory_gb and the 'number of features' (deduced from the dataset). The chunk_size and coreset_size will be deduced based on the above triplet too. In case chunk_size and coreset_size are provided, they will override all above mentioned parameters (less recommended).</p> <p>When building the Coreset, samples are selected and weights are assigned to them, therefore it is important to use functions that support the receipt of sample_weight. Sklearn's SVD implementation does not support the receipt of sample_weight, therefore, it is highly recommended to use the built-in fit function of the CoresetTreeServiceSVD class as it was extended to receive sample_weight.</p> <p>Parameters:</p> Name Type Description Default <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. The class used to interact with the provided data and store it locally. By default, only the sampled data is stored in HDF5 files format.</p> <code>None</code> <code>data_params</code> <code>Union[DataParams, dict]</code> <p>DataParams, optional. Data preprocessing information.</p> <code>None</code> <code>n_instances</code> <code>int</code> <p>int. The total number of instances that are going to be processed (can be an estimation). This parameter is required and the only one from the above mentioned quadruplet, which isn't deduced from the data.</p> <code>None</code> <code>max_memory_gb</code> <code>int</code> <p>int, optional. The maximum memory in GB that should be used. When not provided, the server's total memory is used. In any case only 80% of the provided memory or the server's total memory is considered.</p> <code>None</code> <code>optimized_for</code> <code>Union[list, str]</code> <p>str or list Either 'training', 'cleaning' or or both ['training', 'cleaning']. The main usage of the service.</p> <code>None</code> <code>chunk_size</code> <code>Union[dict, int]</code> <p>int, optional. The number of instances to be used when creating a coreset node in the tree. When defined, it will override the parameters of optimized_for, n_instances, n_classes and max_memory_gb. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>coreset_size</code> <code>Union[int, float, dict]</code> <p>int or float, optional. Represents the coreset size of each node in the coreset tree. If provided as a float, it represents the ratio between each chunk and the resulting coreset.In any case the coreset_size is limited to 60% of the chunk_size. The coreset is constructed by sampling data instances from the dataset based on their calculated importance. Since each instance may be sampled more than once, in practice, the actual size of the coreset is mostly smaller than coreset_size.</p> <code>None</code> <code>coreset_params</code> <code>Union[CoresetParams, dict]</code> <p>CoresetParams or dict, optional. Coreset algorithm specific parameters.</p> <code>None</code> <code>node_train_function</code> <code>Callable[[ndarray, ndarray, ndarray], Any]</code> <p>Callable, optional. method for training model at tree node level.</p> <code>None</code> <code>node_train_function_params</code> <code>dict</code> <p>dict, optional. kwargs to be used when calling node_train_function.</p> <code>None</code> <code>node_metadata_func</code> <code>Callable[[Tuple[ndarray], ndarray, Union[list, None]], Union[list, dict, None]]</code> <p>callable, optional. A method for storing user meta data on each node.</p> <code>None</code> <code>working_directory</code> <code>Union[str, PathLike]</code> <p>str, path, optional. Local directory where intermediate data is stored.</p> <code>None</code> <code>cache_dir</code> <code>Union[str, PathLike]</code> <p>str, path, optional. For internal use when loading a saved service.</p> <code>None</code> <code>chunk_sample_ratio</code> <code>float</code> <p>float, optional. Indicates the size of the sample that will be taken and saved from each chunk on top of the Coreset for the validation methods. The values are from the range [0,1]. For example, chunk_sample_ratio=0.5, means that 50% of the data instances from each chunk will be saved.</p> <code>None</code> <code>model_cls</code> <code>Any</code> <p>A Scikit-learn compatible model class, optional. The model class used to train the model on the coreset, in case a specific model instance wasn't passed to fit or the validation methods. The default model class is our WSVD class supporting also weights.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.auto_preprocessing","title":"auto_preprocessing","text":"<pre><code>auto_preprocessing(X=None, sparse_output=False, copy=False)\n</code></pre> <p>Apply auto-preprocessing on the provided (test) data, similarly to the way it is done by the fit or get_coreset methods. Preprocessing includes ohe-hot encoding and handling missing values depends.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features.</p> <code>None</code> <code>sparse_output</code> <code>bool</code> <p>boolean, default False. When set to True, the function will create a sparse matrix after preprocessing.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the following keys: data: A numpy array of the preprocessed data. features: A list of feature names corresponding to the data. sparse: A boolean indicating if the output is in sparse format.</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.build","title":"build","text":"<pre><code>build(X, y=None, indices=None, props=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree from the parameters X, y, indices and props (properties). build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> required <code>y</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of targets. The target will be ignored when the Coreset is built.</p> <code>None</code> <code>indices</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator with indices of X.</p> <code>None</code> <code>props</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.build_from_df","title":"build_from_df","text":"<pre><code>build_from_df(datasets, target_datasets=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree from pandas DataFrame(s). build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator. Data includes features, may include labels and may include indices.</p> required <code>target_datasets</code> <code>Union[Iterator[Union[DataFrame, Series]], DataFrame, Series]</code> <p>pandas DataFrame or a DataFrame iterator, optional. Use when data is split to features and target. Should include only one column.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.build_from_file","title":"build_from_file","text":"<pre><code>build_from_file(file_path, target_file_path=None, *, reader_f=pd.read_csv, reader_kwargs=None, reader_chunk_size_param_name=None, chunk_size=None, chunk_by=None, n_jobs=None, verbose=1)\n</code></pre> <p>Create a coreset tree based on data taken from local storage. build functions may be called only once. To add more data to the coreset tree use one of the partial_build functions. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories. Path(s) to the place where data is stored. Data includes features, may include targets and may include indices.</p> required <code>target_file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories, optional. Use when the dataset files are split to features and target. Each file should include only one column.</p> <code>None</code> <code>reader_f</code> <code>Callable</code> <p>pandas like read method, optional, default pandas read_csv. For example, to read excel files use pandas read_excel.</p> <code>read_csv</code> <code>reader_kwargs</code> <code>dict</code> <p>dict, optional. Keyword arguments used when calling reader_f method.</p> <code>None</code> <code>reader_chunk_size_param_name</code> <code>str</code> <p>str, optional. reader_f input parameter name for reading file in chunks. When not provided we'll try to figure it out our self. Based on the data, we decide on the optimal chunk size to read and use this parameter as input when calling reader_f. Use \"ignore\" to skip the automatic chunk reading logic.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0:  Nodes are created based on input chunks. chunk_size=-1: Force the service to create a single coreset from the entire dataset (if it fits into memory).</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.cross_validate","title":"cross_validate","text":"<pre><code>cross_validate(level=None, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>Method for cross-validation on the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of folds and hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : the score is also displayed;</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>A list of scores, one for each fold. If return_model=True, a list of trained models is also returned (one model for each fold).</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.filter_out_samples","title":"filter_out_samples","text":"<pre><code>filter_out_samples(filter_function, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Remove samples from the coreset tree, based on the provided filter function. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>filter_function</code> <code>Callable[[Iterable, Iterable, Union[Iterable, None], Union[Iterable, None]], Iterable[Any]]</code> <p>function, optional. A function that returns a list of indices to be removed from the tree. The function should accept 4 parameters as input: indices, X, y, props and return a list(iterator) of indices to be removed from the coreset tree. For example, in order to remove all instances with a target equal to 6, use the following function: filter_function = lambda indices, X, y, props : indices[y = 6].</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.fit","title":"fit","text":"<pre><code>fit(level=0, seq_from=None, seq_to=None, model=None, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>Fit a model on the coreset tree. This model will be used when predict and predict_proba are called. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>string/datetime, optional The starting sequence of the training set.</p> <code>None</code> <code>seq_to</code> <code>Any</code> <p>string/datetime, optional The ending sequence of the training set.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>Model hyperparameters kwargs. Input when instantiating default model class.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Fitted estimator.</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.get_cleaning_samples","title":"get_cleaning_samples","text":"<pre><code>get_cleaning_samples(size=None, ignore_indices=None, select_from_indices=None, select_from_function=None, ignore_seen_samples=True)\n</code></pre> <p>Returns indices of samples in descending order of importance. Useful for identifying mislabeled instances and other anomalies in the data. size must be provided. Function must be called after build. This function is only applicable in case the coreset tree was optimized_for 'cleaning'. This function is not for retrieving the coreset (use get_coreset in this case).</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>required, optional. Number of samples to return.</p> <code>None</code> <code>ignore_indices</code> <code>Iterable</code> <p>array-like, optional. An array of indices to ignore when selecting cleaning samples.</p> <code>None</code> <code>select_from_indices</code> <code>Iterable</code> <p>array-like, optional.  An array of indices to consider when selecting cleaning samples.</p> <code>None</code> <code>select_from_function</code> <code>Callable[[Iterable, Iterable, Union[Iterable, None], Union[Iterable, None]], Iterable[Any]]</code> <p>function, optional.  Pass a function in order to limit the selection of the cleaning samples accordingly.  The function should accept 4 parameters as input: indices, X, y, props.  and return a list(iterator) of the desired indices.</p> <code>None</code> <code>ignore_seen_samples</code> <code>bool</code> <p>bool, optional, default True.  Exclude already seen samples and set the seen flag on any indices returned by the function.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Union[ValueError, dict]</code> <p>idx: array-like[int].     Cleaning samples indices. X: array-like[int].     X array. y: array-like[int].     y array. importance: array-like[float].     The importance property. Instances that receive a high Importance in the Coreset computation,     require attention as they usually indicate a labeling error,     anomaly, out-of-distribution problem or other data-related issue.</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.get_coreset","title":"get_coreset","text":"<pre><code>get_coreset(level=0, preprocessing_stage='user', sparse_threshold=0.01, as_df=False, with_index=False, seq_from=None, seq_to=None)\n</code></pre> <p>Get tree's coreset data in one of the preprocessing_stage(s) in the data preprocessing workflow. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Returns the features (X) as a sparse matrix if the data density after preprocessing is below sparse_threshold, otherwise, will return the data as an array (Applicable only for preprocessing_stage=<code>auto</code>).</p> <code>0.01</code> <code>as_df</code> <code>bool</code> <p>boolean, optional, default False. When True, returns the X as a pandas DataFrame.</p> <code>False</code> <code>with_index</code> <code>bool</code> <p>boolean, optional, default False. Relevant only when preprocessing_stage=<code>auto</code>. Should the returned data include the index column.</p> <code>False</code> <code>seq_from</code> <code>Any</code> <p>string or datetime, optional, default None. The start sequence to filter samples by.</p> <code>None</code> <code>seq_to</code> <code>Any</code> <p>string or datetime, optional, default None. The end sequence to filter samples by.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary representing the Coreset: ind: A numpy array of indices. X: A numpy array of the feature matrix. y: A numpy array of the target values. w: A numpy array of sample weights. n_represents: The number of instances represented by the coreset. features_out: A list of the output features, if preprocessing_stage=auto, otherwise None. props: A numpy array of properties, or None if not available.</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.get_coreset_size","title":"get_coreset_size","text":"<pre><code>get_coreset_size(level=0, seq_from=None, seq_to=None)\n</code></pre> <p>Returns the size of the tree's coreset data. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>seq_from</code> <code>Any</code> <p>string or datetime, optional, default None. The start sequence to filter samples by.</p> <code>None</code> <code>seq_to</code> <code>Union[str, datetime]</code> <p>string or datetime, optional, default None. The end sequence to filter samples by.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>coreset size</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.get_hyperparameter_tuning_data","title":"get_hyperparameter_tuning_data","text":"<pre><code>get_hyperparameter_tuning_data(level=None, validation_method='cross validation', preprocessing_stage='user', sparse_threshold=0.01, validation_size=0.2, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, as_df=True)\n</code></pre> <p>A method for retrieving the data for hyperparameter tuning with cross validation, using the coreset tree. The returned data can be used with Scikit-learn\u2019s GridSearchCV, with skopt\u2019s BayesSearchCV and with any other hyperparameter tuning method that can accept a fold iterator object. Note: When using this method with Scikit-learn's <code>GridSearchCV</code> and similar methods, the <code>refit</code> parameter must be set to <code>False</code>. This is because the returned dataset (X, y and w) includes both training and validation data due to the use of a splitter. The returned dataset (X, y and w) is the concatenation of training data for all folds followed by validation data for all folds. By default, GridSearchCV refits the estimator on the entire dataset, not just the training portion, and this behavior cannot be modified and is incorrect. In this case, refit should be handled manually after the cross-validation process, by calling get_coreset with the same parameters that were passed to this function to retrieve the data and then fitting on the returned data using the best hyperparameters found in GridSearchCV. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>None</code> <code>validation_method</code> <code>str</code> <p>str, optional. Indicates which validation method will be used. The possible values are 'cross validation', 'hold-out validation' and 'seq-dependent validation'.</p> <code>'cross validation'</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Returns the features (X) as a sparse matrix if the data density after preprocessing is below sparse_threshold, otherwise, will return the data as an array (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>validation_size</code> <code>float</code> <p>float, optional, default 0.2. The size of the validation set, as a percentage of the training set size for hold-out validation.</p> <code>0.2</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>as_df</code> <code>bool</code> <p>boolean, optional, default False. When True, returns the X as a pandas DataFrame.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Union[ndarray, FoldIterator, Any]]</code> <p>A dictionary with the following keys: ind: The indices of the data. X: The data. y: The labels. w: The weights. splitter: The fold iterator. model_params: The model parameters.</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.get_max_level","title":"get_max_level","text":"<pre><code>get_max_level()\n</code></pre> <p>Return the maximal level of the coreset tree. Level 0 is the head of the tree. Level 1 is the level below the head of the tree, etc.</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.grid_search","title":"grid_search","text":"<pre><code>grid_search(param_grid, level=None, validation_method='cross validation', model=None, scoring=None, refit=True, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, error_score=np.nan, validation_size=0.2, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, n_jobs=None)\n</code></pre> <p>A method for performing hyperparameter selection by grid search, using the coreset tree. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>param_grid</code> <code>Union[Dict[str, List], List[Dict[str, List]]]</code> <p>dict or list of dicts. Dictionary with parameters names (str) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.</p> required <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>validation_method</code> <code>str</code> <p>str, optional. Indicates which validation method will be used. The possible values are 'cross validation', 'hold-out validation' and 'seq-dependent validation'. If 'cross validation' is selected, the process involves progressing through folds. We first train and validate all hyperparameter combinations for each fold, before moving on to the subsequent folds.</p> <code>'cross validation'</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. The model class needs to implement the usual scikit-learn interface.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>refit</code> <code>bool</code> <p>bool, optional. If True, retrain the model on the whole coreset using the best found hyperparameters, and return the model. This model will be used when predict and predict_proba are called.</p> <code>True</code> <code>verbose</code> <code>int</code> <p>int, optional Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of folds and hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each fold and hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>error_score</code> <code>Union[str, float, int]</code> <p>\"raise\" or numeric, optional. Value to assign to the score if an error occurs in model training. If set to \"raise\", the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.</p> <code>nan</code> <code>validation_size</code> <code>float</code> <p>float, optional, default 0.2. The size of the validation set, as a percentage of the training set size for hold-out validation.</p> <code>0.2</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set for seq-dependent validation.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>int, optional. Default: number of CPUs. Number of jobs to run in parallel during grid search.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Dict, DataFrame, BaseEstimator], Tuple[Dict, DataFrame]]</code> <p>A dict with the best hyperparameters setting, among those provided by the user. The keys are the hyperparameters names, while the dicts' values are the hyperparameters values. A Pandas DataFrame holding the score for each hyperparameter combination and fold. For the 'cross validation' method the average across all folds for each hyperparameter combination is included too. If refit=True, the retrained model is also returned.</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.holdout_validate","title":"holdout_validate","text":"<pre><code>holdout_validate(level=None, validation_size=0.2, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>A method for hold-out validation on the coreset tree. The validation set is always the last part of the dataset. This function is only applicable in case the coreset tree was optimized_for <code>training</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree on which the training and validation will be performed. If None, the best level will be selected.</p> <code>None</code> <code>validation_size</code> <code>float</code> <p>float, optional. The percentage of the dataset that will be used for validating the model.</p> <code>0.2</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>The validation score. If return_model=True, the trained model is also returned.</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.is_dirty","title":"is_dirty","text":"<pre><code>is_dirty()\n</code></pre> <p>Returns:</p> Type Description <code>bool</code> <p>Indicates whether the coreset tree has nodes marked as dirty, meaning they were affected by any of the methods: remove_samples, update_targets, update_features or filter_out_samples, when they were called with force_do_nothing.</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(dir_path, name=None, *, data_manager=None, load_buffer=True, working_directory=None)\n</code></pre> <p>Restore a service object from a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>str, path. Local directory where service data is stored.</p> required <code>name</code> <code>str</code> <p>string, optional, default service class name (lower case). The name prefix of the subdirectory to load. When several subdirectories having the same name prefix are found, the last one, ordered by name, is selected. For example when saving with override=False, the chosen subdirectory is the last saved.</p> <code>None</code> <code>data_manager</code> <code>DataManagerT</code> <p>DataManagerBase subclass, optional. When specified, input data manger will be used instead of restoring it from the saved configuration.</p> <code>None</code> <code>load_buffer</code> <code>bool</code> <p>boolean, optional, default True. If set, load saved buffer (a partial node of the tree) from disk and add it to the tree.</p> <code>True</code> <code>working_directory</code> <code>Union[str, PathLike]</code> <p>str, path, optional, default use working_directory from saved configuration. Local directory where intermediate data is stored.</p> <code>None</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>CoresetTreeService object</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.partial_build","title":"partial_build","text":"<pre><code>partial_build(X, y=None, indices=None, props=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree from parameters X, y, indices and props (properties). Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>array like or iterator of arrays like. An array or an iterator of features. Categorical features are automatically one-hot encoded and missing values are automatically handled.</p> required <code>y</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of targets. The target will be ignored when the Coreset is built.</p> <code>None</code> <code>indices</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator with indices of X.</p> <code>None</code> <code>props</code> <code>Union[Iterable[Any], Iterable[Iterable[Any]]]</code> <p>array like or iterator of arrays like, optional. An array or an iterator of properties. Properties, won\u2019t be used to compute the Coreset or train the model, but it is possible to filter_out_samples on them or to pass them in the select_from_function of get_cleaning_samples.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.partial_build_from_df","title":"partial_build_from_df","text":"<pre><code>partial_build_from_df(datasets, target_datasets=None, *, chunk_size=None, chunk_by=None, copy=False, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree based on the pandas DataFrame iterator. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator. Data includes features, may include targets and may include indices.</p> required <code>target_datasets</code> <code>Union[Iterator[DataFrame], DataFrame]</code> <p>pandas DataFrame or a DataFrame iterator, optional. Use when data is split to features and target. Should include only one column.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional, default previous used chunk_size. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function and functions such as update_targets or update_features. True - Data is copied before processing (impacts memory).</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:     self</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.partial_build_from_file","title":"partial_build_from_file","text":"<pre><code>partial_build_from_file(file_path, target_file_path=None, *, reader_f=pd.read_csv, reader_kwargs=None, reader_chunk_size_param_name=None, chunk_size=None, chunk_by=None, n_jobs=None, verbose=1)\n</code></pre> <p>Add new samples to a coreset tree based on data taken from local storage. Categorical features are automatically one-hot encoded and missing values are automatically handled. The target will be ignored when the Coreset is built.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories. Path(s) to the place where data is stored. Data includes features, may include targets and may include indices.</p> required <code>target_file_path</code> <code>Union[Union[str, PathLike], Iterable[Union[str, PathLike]]]</code> <p>file, list of files, directory, list of directories, optional. Use when files are split to features and target. Each file should include only one column.</p> <code>None</code> <code>reader_f</code> <code>Callable</code> <p>pandas like read method, optional, default pandas read_csv. For example, to read excel files use pandas read_excel.</p> <code>read_csv</code> <code>reader_kwargs</code> <code>dict</code> <p>dict, optional. Keyword arguments used when calling reader_f method.</p> <code>None</code> <code>reader_chunk_size_param_name</code> <code>str</code> <p>str, optional. reader_f input parameter name for reading file in chunks. When not provided we'll try to figure it out our self. Based on the data, we decide on the optimal chunk size to read and use this parameter as input when calling reader_f. Use \"ignore\" to skip the automatic chunk reading logic.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>int, optional, default previous used chunk_size. The number of instances used when creating a coreset node in the tree. chunk_size=0: Nodes are created based on input chunks.</p> <code>None</code> <code>chunk_by</code> <code>Union[Callable, str, list]</code> <p>function, label, or list of labels, optional. Split the data according to the provided key. When provided, chunk_size input is ignored.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Default: number of CPUs. Number of jobs to run in parallel during build.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>optional The verbose level for printing build progress, 0 - silent, 1 - (default) print.</p> <code>1</code> <p>Returns:</p> Type Description <code>CoresetTreeService</code> <p>self</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.plot","title":"plot","text":"<pre><code>plot(dir_path=None, selected_trees=None)\n</code></pre> <p>Produce a tree graph plot and save figure as a local png file.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike. Path to save the plot figure in; if not provided, or if isn't valid/doesn't exist, the figure will be saved in the current directory (from which this method is called).</p> <code>None</code> <code>selected_trees</code> <code>dict</code> <p>dict, optional. A dictionary containing the names of the image file(s) to be generated.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Image file path</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.predict","title":"predict","text":"<pre><code>predict(X, sparse_output=False, copy=False)\n</code></pre> <p>Run prediction on the trained model. This function is only applicable in case the coreset tree was optimized_for 'training' and in case fit() or grid_search(refit=True) where called before. The function automatically preprocesses the data according to the preprocessing_stage used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>An array of features.</p> required <code>sparse_output</code> <code>bool</code> <p>boolean, optional, default False. When set to True, the function will create a sparse matrix after preprocessing and pass it to the predict function.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <p>Model prediction results.</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X, sparse_output=False, copy=False)\n</code></pre> <p>Run prediction on the trained model. This function is only applicable in case the coreset tree was optimized_for 'training' and in case fit() or grid_search(refit=True) where called before. The function automatically preprocesses the data according to the preprocessing_stage used to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[Iterable, Iterable[Iterable]]</code> <p>An array of features.</p> required <code>sparse_output</code> <code>bool</code> <p>boolean, optional, default False. When set to True, the function will create a sparse matrix after preprocessing and pass it to the predict_proba function.</p> <code>False</code> <code>copy</code> <code>bool</code> <p>boolean, default False. False (default) - Input data might be updated as result of this function. True - Data is copied before processing (impacts memory).</p> <code>False</code> <p>Returns:</p> Type Description <p>Returns the probability of the sample for each class in the model.</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.print","title":"print","text":"<pre><code>print(selected_tree=None)\n</code></pre> <p>Print the tree's string representation.</p> <p>Parameters:</p> Name Type Description Default <code>selected_tree</code> <code>str</code> <p>string, optional. Which tree to print. Defaults to printing all.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.remove_samples","title":"remove_samples","text":"<pre><code>remove_samples(indices, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Remove samples from the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be removed from the coreset tree.</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.save","title":"save","text":"<pre><code>save(dir_path=None, name=None, save_buffer=True, override=False, allow_pickle=True)\n</code></pre> <p>Save service configuration and relevant data to a local directory. Use this method when the service needs to be restored.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike, optional, default self.working_directory. A local directory for saving service's files.</p> <code>None</code> <code>name</code> <code>str</code> <p>string, optional, default service class name (lower case). Name of the subdirectory where the data will be stored.</p> <code>None</code> <code>save_buffer</code> <code>bool</code> <p>boolean, default True. Save also the data in the buffer (a partial node of the tree) along with the rest of the saved data.</p> <code>True</code> <code>override</code> <code>bool</code> <p>bool, optional, default False. False: add a timestamp suffix so each save won\u2019t override the previous ones. True: The existing subdirectory with the provided name is overridden.</p> <code>False</code> <code>allow_pickle</code> <code>bool</code> <p>bool, optional, default True. True: Saves the Coreset tree in pickle format (much faster). False: Saves the Coreset tree in JSON format.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>Save directory path.</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.save_coreset","title":"save_coreset","text":"<pre><code>save_coreset(file_path, level=0, preprocessing_stage='user', with_index=True)\n</code></pre> <p>Get the coreset from the tree and save it to a file. Use the level parameter to control the level of the tree from which samples will be returned. This function is only applicable in case the coreset tree was optimized_for 'training'.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, PathLike]</code> <p>string or PathLike. Local file path to store the coreset.</p> required <code>level</code> <code>int</code> <p>int, optional, default 0. Defines the depth level of the tree from which the coreset is extracted. Level 0 returns the coreset from the head of the tree with around coreset_size samples. Level 1 returns the coreset from the level below the head of the tree with around twice of the samples compared to level 0, etc. If the passed level is greater than the maximal level of the tree, the maximal available level is used.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>user</code>. The different stages reflect the data preprocessing workflow. - original - Return the data as it was handed to the Coreset\u2019s build function (The data_params.save_orig flag should be set for this option to be available). - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'user'</code> <code>with_index</code> <code>bool</code> <p>boolean, optional, default False. Relevant only when preprocessing_stage=<code>auto</code>. Should the returned data include the index column.</p> <code>True</code>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.seq_dependent_validate","title":"seq_dependent_validate","text":"<pre><code>seq_dependent_validate(level=None, seq_train_from=None, seq_train_to=None, seq_validate_from=None, seq_validate_to=None, model=None, scoring=None, return_model=False, verbose=0, preprocessing_stage='auto', sparse_threshold=0.01, **model_params)\n</code></pre> <p>The method allows to train and validate on a subset of the Coreset tree, according to the <code>seq_column</code> defined in the <code>DataParams</code> structure passed to the init. This function is only applicable in case the coreset tree was optimized_for <code>training</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>int, optional. The level of the tree from which the search for the best matching nodes starts. Nodes closer to the leaf level than the specified level, may be selected to better match the provided seq parameters.If None, the search starts from level 0, the head of the tree. If None, the best level will be selected.</p> <code>None</code> <code>seq_train_from</code> <code>Any</code> <p>Any, optional. The starting sequence of the training set.</p> <code>None</code> <code>seq_train_to</code> <code>Any</code> <p>Any, optional. The ending sequence of the training set.</p> <code>None</code> <code>seq_validate_from</code> <code>Any</code> <p>Any, optional. The starting sequence number of the validation set.</p> <code>None</code> <code>seq_validate_to</code> <code>Any</code> <p>Any, optional. The ending sequence number of the validation set.</p> <code>None</code> <code>model</code> <code>Any</code> <p>A Scikit-learn compatible model instance, optional. When provided, model_params are not relevant. The model class needs to implement the usual scikit-learn interface. Default: instantiate the service model class using input model_params.</p> <code>None</code> <code>scoring</code> <code>Union[str, Callable[[BaseEstimator, ndarray, ndarray], float]]</code> <p>callable or string, optional. If it is a callable object, it must return a scalar score. The signature of the call is (model, X, y), where model is the ML model to be evaluated, X is the data and y is the ground truth labeling. For example, it can be produced using sklearn.metrics.make_scorer. If it is a string, it must be a valid name of a Scikit-learn scoring method If None, the default scorer of the current model is used.</p> <code>None</code> <code>return_model</code> <code>bool</code> <p>bool, optional. If True, the trained model is also returned.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>int, optional. Controls the verbosity: the higher, the more messages.     &gt;=1 : The number of hyperparameter combinations to process at the start and the time it took, best hyperparameters found and their score at the end.     &gt;=2 : The score and time for each hyperparameter combination.</p> <code>0</code> <code>preprocessing_stage</code> <code>Union[str, None]</code> <p>string, optional, default <code>auto</code>. The different stages reflect the data preprocessing workflow. - user - Return the data after any user defined data preprocessing (if defined). - auto - Return the data after applying auto-preprocessing, including one-hot-encoding, converting Boolean fields to numeric, etc.</p> <code>'auto'</code> <code>sparse_threshold</code> <code>float</code> <p>float, optional, default 0.01. Creates a sparse matrix from the features (X), if the data density after preprocessing is below sparse_threshold, otherwise, will create an array. (Applicable only for preprocessing_stage='auto').</p> <code>0.01</code> <code>model_params</code> <p>kwargs, optional. The hyper-parameters of the model. If not provided, the default values are used.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Tuple[List[float], List[BaseEstimator]]]</code> <p>The validation score. If return_model=True, the trained model is also returned.</p>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.set_model_cls","title":"set_model_cls","text":"<pre><code>set_model_cls(model_cls)\n</code></pre> <p>Set the model class used to train the model on the coreset, in case a specific model instance wasn't passed to fit or the validation methods.</p> <p>Parameters:</p> Name Type Description Default <code>model_cls</code> <code>Any</code> <p>A Scikit-learn compatible model class.</p> required"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.set_seen_indication","title":"set_seen_indication","text":"<pre><code>set_seen_indication(seen_flag=True, indices=None)\n</code></pre> <p>Set samples as 'seen' or 'unseen'. Not providing an indices list defaults to setting the flag on all samples. This function is only applicable in case the coreset tree was optimized_for 'cleaning'.</p> <p>Parameters:</p> Name Type Description Default <code>seen_flag</code> <code>bool</code> <p>bool, optional, default True. Set 'seen' or 'unseen' flag</p> <code>True</code> <code>indices</code> <code>Iterable</code> <p>array like, optional. Set flag only for the provided list of indices. Defaults to all indices.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.update_dirty","title":"update_dirty","text":"<pre><code>update_dirty(force_resample_all=None, force_sensitivity_recalc=None)\n</code></pre> <p>Calculate the sensitivity and resample the nodes that were marked as dirty, meaning they were affected by any of the methods: remove_samples, update_targets, update_features or filter_out_samples, when they were called with force_do_nothing.</p> <p>Parameters:</p> Name Type Description Default <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.update_features","title":"update_features","text":"<pre><code>update_features(indices, X, feature_names=None, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Update the features for selected samples on the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be updated.</p> required <code>X</code> <code>Iterable</code> <p>array-like. An array of features. Should have the same length as indices.</p> required <code>feature_names</code> <code>Iterable[str]</code> <p>If the quantity of features in X is not equal to the quantity of features in the original coreset, this param should contain list of names of passed features.</p> <code>None</code> <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"reference/services/coreset_tree/svd/#services.coreset_tree.svd.CoresetTreeServiceSVD.update_targets","title":"update_targets","text":"<pre><code>update_targets(indices, y, force_resample_all=None, force_sensitivity_recalc=None, force_do_nothing=False)\n</code></pre> <p>Update the targets for selected samples on the coreset tree. The coreset tree is automatically updated to accommodate to the changes.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Iterable</code> <p>array-like. An array of indices to be updated.</p> required <code>y</code> <code>Iterable</code> <p>array-like. An array of classes/labels. Should have the same length as indices.</p> required <code>force_resample_all</code> <code>Optional[int]</code> <p>int, optional. Force full resampling of the affected nodes in the coreset tree, starting from level=force_resample_all. None - Do not force_resample_all (default), 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_sensitivity_recalc</code> <code>Optional[int]</code> <p>int, optional. Force the recalculation of the sensitivity and partial resampling of the affected nodes, based on the coreset's quality, starting from level=force_sensitivity_recalc. None - If self.chunk_sample_ratio&lt;1 - one level above leaf node level. If self.chunk_sample_ratio=1 - leaf level 0 - The head of the tree, 1 - The level below the head of the tree, len(tree)-1 = leaf level, -1 - same as leaf level.</p> <code>None</code> <code>force_do_nothing</code> <code>Optional[bool]</code> <p>bool, optional, default False. When set to True, suppresses any update to the coreset tree until update_dirty is called.</p> <code>False</code>"},{"location":"coverage/","title":"Coverage","text":""}]}