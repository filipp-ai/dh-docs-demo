{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Library Functions\n",
    "## Purpose\n",
    "In this example we will demonstrate how to:\n",
    "   - Build a Coreset tree on a train dataset.\n",
    "   - Adding to the tree additional data through partial_build\n",
    "   - Update samples' labels\n",
    "   - Update samples' features\n",
    "   - Remove samples\n",
    "   - Compare the model trained on the Coreset with the model trained on the full dataset\n",
    "\n",
    "In this example we'll be using the well-known Covertype Dataset (https://archive.ics.uci.edu/ml/datasets/covertype).\n",
    "We will split the data to three parts:\n",
    "  - train_1 - 50% of the data\n",
    "  - train_2 - 20% of the data\n",
    "  - test - 30% of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from dataheroes import CoresetTreeServiceLG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Covertype dataset as a pandas data frame.\n",
    "# In the output data frame all columns are features beside the last column.\n",
    "# The last column (Cover_Type) is the target\n",
    "df = fetch_covtype(as_frame=True).frame\n",
    "# add random_date column\n",
    "start_date = datetime.date(year=2021, month=1, day=1)\n",
    "end_date = datetime.date(year=2022, month=12, day=31)\n",
    "df['random_date'] = [start_date + datetime.timedelta(days=random.randint(0, (end_date - start_date).days))\n",
    "                     for _ in range(df.shape[0])]\n",
    "# set random_date column as first in dataframe\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n",
    "# Split dataset: train_1 = 50%, train_2=20%, test=30%\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "train_1, train_2 = train_test_split(train, test_size=20/70, random_state=42)\n",
    "\n",
    "# Prepare the data directory and set the file names.\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "train_1_file_path = data_dir / \"train_1.csv\"\n",
    "train_2_file_path = data_dir / \"train_2.csv\"\n",
    "test_file_path = data_dir / \"test.csv\"\n",
    "\n",
    "# Store datasets as CSV.\n",
    "# We use the data frame index as the unique identifier and define a column for it (index_column)\n",
    "# Defining an index column is optional. In this example we set it to have a reference to the original dataset.\n",
    "train_1.to_csv(train_1_file_path, index_label=\"index_column\")\n",
    "train_2.to_csv(train_2_file_path, index_label=\"index_column\")\n",
    "test.to_csv(test_file_path, index_label=\"index_column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the tree\n",
    "Run `build_from_file` on the first train file.\n",
    "It will include ~290K sample. We pass `n_classes` and `n_instances` to help the tree calculate an optimal Coreset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataheroes.services.tree_services.CoresetTreeServiceLG at 0x175275790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tell the tree how the data is structured.\n",
    "# In this example we have an index column, a target column and a property column. \n",
    "# Property is a column you can filter on, but it isn't taken into account when building \n",
    "# the coreset or training the model. \n",
    "# All other columns are features.\n",
    "data_params = {\n",
    "    'properties': [\n",
    "            {'name': 'random_date'},\n",
    "        ],\n",
    "    'target': {'name': 'Cover_Type'},\n",
    "    'index': {'name': 'index_column'}\n",
    "}\n",
    "# Initialize the service and build the tree.\n",
    "# The tree uses the local file system to store its data.\n",
    "# After this step you will have a new directory .dataheroes_cache\n",
    "service_obj = CoresetTreeServiceLG(data_params=data_params,\n",
    "                                   optimized_for='training',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=406_000\n",
    "                                  )\n",
    "service_obj.build_from_file(train_1_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('output/train_tree.png')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = Path('output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "service_obj.plot(output_dir, \"train_tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get coreset from the tree, train a model based on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore convergence warnings for logistic regression\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "# Get the top level coreset (~2K samples with weights)\n",
    "coreset = service_obj.get_coreset()\n",
    "indices, X, y = coreset['data']\n",
    "w = coreset['w']\n",
    "# Train a logistic regression model on the coreset.\n",
    "coreset_model = LogisticRegression().fit(X, y, sample_weight=w)\n",
    "n_samples_coreset = len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Train a model on the full dataset for comparison\n",
    "We use the same part of the dataset that was used for building the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_1.iloc[:, 1:-1].to_numpy() \n",
    "y = train_1.iloc[:, -1].to_numpy()\n",
    "\n",
    "full_dataset_model = LogisticRegression().fit(X, y)\n",
    "n_samples_full = len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Compare models quality\n",
    "Test both coreset and full model on test dataset and compare their AUC scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coreset AUC score (1,182 samples): 0.8326490662571097\n",
      "Full dataset AUC score (290,505 samples): 0.7903846802843272\n"
     ]
    }
   ],
   "source": [
    "# Read test dataset from file\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "X_test = df.iloc[:, 1:-1].to_numpy()\n",
    "y_test = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "# Evaluate model\n",
    "coreset_score = roc_auc_score(y_test, coreset_model.predict_proba(X_test), multi_class='ovr')\n",
    "full_dataset_score = roc_auc_score(y_test, full_dataset_model.predict_proba(X_test), multi_class='ovr')\n",
    "\n",
    "print(f\"Coreset AUC score ({n_samples_coreset:,} samples): {coreset_score}\")\n",
    "print(f\"Full dataset AUC score ({n_samples_full:,} samples): {full_dataset_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Save the tree for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('output/train_1_coreset_tree')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = Path('output')\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "save_tree_name = 'train_1_coreset_tree'\n",
    "service_obj.save(out_dir, save_tree_name, override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Load the saved tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_obj = CoresetTreeServiceLG.load(out_dir, save_tree_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Partial build\n",
    "Add the second train dataset to the tree using `partial_build_from_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataheroes.services.tree_services.CoresetTreeServiceLG at 0x175262b50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj.partial_build_from_file(train_2_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Get important samples\n",
    "Call `get_important_samples` for a certain class (the biggest) for 100 samples to identify mislabeled samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 7 classes, we choose the biggest one, that is 2\n",
    "biggest_class = 2\n",
    "number_important_samples = 100\n",
    "important_samples = service_obj.get_important_samples(class_size={biggest_class: number_important_samples})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Update targets\n",
    "Call `update_targets` for some returned samples above. To further compare the models' result (tree vs. full dataset) we also update the targets in the same way, for the same samples on full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take first 10 samples for update (0..9)\n",
    "indexes_for_update_targets = np.arange(0,10)\n",
    "# updating target with different values (not equal 2)\n",
    "y_new = [1, 3, 3, 3, 3, 4, 5, 6, 7, 7]\n",
    "# update selected samples on the coreset tree\n",
    "service_obj.update_targets(important_samples['idx'][indexes_for_update_targets], y_new)\n",
    "\n",
    "# Same update on full dataset\n",
    "train.loc[important_samples['idx'][indexes_for_update_targets], 'Cover_Type'] = y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Update features\n",
    "Call `update_features` for some returned samples above (different ones than 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take next 10 samples for update (10..19)\n",
    "indexes_for_update_features = np.arange(10,20)\n",
    "X_new = important_samples['X'][indexes_for_update_features]\n",
    "# modify X, as a simple example - set all binary fields = 1\n",
    "for sample in X_new:\n",
    "    sample[sample == 0] = 1\n",
    "# update selected samples on the coreset tree\n",
    "service_obj.update_features(important_samples['idx'][indexes_for_update_features], X_new)\n",
    "\n",
    "# same features updated on full dataset\n",
    "train.loc[important_samples['idx'][indexes_for_update_features], train.columns[1:-1]] = X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Remove samples\n",
    "Call `remove_samples` for some returned samples above (different ones than 8 and 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take next 10 samples for remove (20..29)\n",
    "indexes_for_remove = np.arange(20,30)\n",
    "# remove selected samples from the coreset tree\n",
    "service_obj.remove_samples(important_samples['idx'][indexes_for_remove])\n",
    "\n",
    "# remove same samples from full dataset\n",
    "train = train.drop(train.loc[important_samples['idx'][indexes_for_remove]].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Filter data from the tree\n",
    "Run `filter_out_samples` to filter out all samples with elevation < 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of Elevation feature\n",
    "elevation_feature_index = [f.name for f in service_obj.data_manager.data_params.features].index('Elevation')\n",
    "# filter function\n",
    "def filter_out_elevation_less_2000(indexes, X, y, props):\n",
    "    return indexes[X[:, elevation_feature_index] < 2000]\n",
    "# filter samples from the tree\n",
    "service_obj.filter_out_samples(filter_out_elevation_less_2000)\n",
    "# Same filter applied to the full dataset\n",
    "train = train[train['Elevation'] >= 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Filter data by property\n",
    "Run `filter_out_samples` to filter out all samples with random_date <= 05-01-2021 (so filtering out the first 5 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter function\n",
    "def filter_out_dates(indexes, X, y, props):\n",
    "    date_arr = np.array([datetime.datetime.strptime(d[0], '%Y-%m-%d').date() for d in props])\n",
    "    return indexes[date_arr <= datetime.date(year=2021, month=1, day=5)]\n",
    "# filter samples from the tree\n",
    "service_obj.filter_out_samples(filter_out_dates)\n",
    "# Same filter applied to the full dataset\n",
    "train = train[train['random_date'] > datetime.date(year=2021, month=1, day=5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Fit on the tree\n",
    "Run `fit` directly on the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_obj.fit()\n",
    "n_samples_coreset = len(service_obj.get_coreset()['w'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save coreset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_coreset_path = os.path.join(output_dir, \"final_coreset\")\n",
    "service_obj.save_coreset(final_coreset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 Train model of entire dataset\n",
    "Train model of entire train dataset to compare AUC score with coreset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.iloc[:, 1:-1].to_numpy()\n",
    "y = train.iloc[:, -1].to_numpy()\n",
    "\n",
    "full_updated_dataset_model = LogisticRegression().fit(X, y)\n",
    "n_samples_full = len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2 Compare models quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Coreset AUC score (1,108 samples): 0.806217250816925\n",
      "Full updated dataset AUC score (402,999 samples): 0.7970536830030398\n"
     ]
    }
   ],
   "source": [
    "final_tree_lg_score = roc_auc_score(y_test, service_obj.predict_proba(X_test), multi_class='ovr')\n",
    "full_updated_dataset_score = roc_auc_score(y_test, full_updated_dataset_model.predict_proba(X_test), multi_class='ovr')\n",
    "\n",
    "print(f\"Updated Coreset AUC score ({n_samples_coreset:,} samples): {final_tree_lg_score}\")\n",
    "print(f\"Full updated dataset AUC score ({n_samples_full:,} samples): {full_updated_dataset_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
